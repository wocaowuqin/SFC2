#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
train_three_phase.py - ä¿®å¤ç‰ˆ
ä¿®å¤é—®é¢˜: é˜¶æ®µ3è®­ç»ƒæ•°æ®è€—å°½å¯¼è‡´ç«‹å³é€€å‡º

å…³é”®ä¿®æ”¹:
1. åœ¨é˜¶æ®µ3å¼€å§‹æ—¶é‡æ–°åˆå§‹åŒ–ç¯å¢ƒä»¥é‡ç½®è¯·æ±‚é˜Ÿåˆ—
2. æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
3. æ·»åŠ æ•°æ®è€—å°½çš„é”™è¯¯å¤„ç†

ä½¿ç”¨æ–¹æ³•:
    python train_three_phase_FIXED.py
"""
import os
import sys
import logging
import random
import pickle
from pathlib import Path
import numpy as np
import torch
import matplotlib

# å¼ºåˆ¶ä½¿ç”¨éäº¤äº’å¼åç«¯
os.environ['MPLBACKEND'] = 'Agg'
matplotlib.use('Agg')

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import hyperparameters as H
from hirl_sfc_env_gnn import SFC_HIRL_Env_GNN
from hirl_gnn_models import GNN_HRL_Controller
from hirl_sfc_agent_gnn import Agent_SFC_GNN
from expert_data_collector import ExpertDataCollector

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(
            H.OUTPUT_DIR / "training_three_phase.log",
            mode='w', encoding='utf-8'
        ) if H.OUTPUT_DIR.exists() else logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ============================================================================
# ä¸‰é˜¶æ®µè®­ç»ƒå™¨ - ä¿®å¤ç‰ˆ
# ============================================================================
class ThreePhaseTrainer:
    """ä¸‰é˜¶æ®µè®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, config: dict):
        """
        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # 1. åˆå§‹åŒ–ç¯å¢ƒ
        logger.info("åˆå§‹åŒ–ç¯å¢ƒ...")
        self.env = SFC_HIRL_Env_GNN(
            input_dir=config['input_dir'],
            topo=config['topology'],
            dc_nodes=config['dc_nodes'],
            capacities=config['capacities'],
            use_gnn=True
        )

        # è·å–çŠ¶æ€ç»´åº¦ (ç”¨äºåˆå§‹åŒ–æ¨¡å‹)
        test_req, test_state = self.env.reset_request()
        if test_req is None:
            raise RuntimeError("No requests available in the dataset!")

        x, edge_index, edge_attr, req_vec = test_state

        # 2. åˆå§‹åŒ–æ¨¡å‹
        logger.info("åˆ›å»ºGNNæ¨¡å‹...")
        self.model = self._create_model(
            node_feat_dim=x.shape[1],
            edge_feat_dim=edge_attr.shape[1],
            request_dim=len(req_vec),
            hidden_dim=config['hidden_dim'],
            num_actions=self.env.NB_LOW_LEVEL_ACTIONS
        )

        # 3. åˆå§‹åŒ–Agent
        logger.info("åˆ›å»º Agent...")
        self.agent = Agent_SFC_GNN(
            model=self.model,
            n_actions=self.env.NB_LOW_LEVEL_ACTIONS,
            lr=config['lr'],
            gamma=config['gamma'],
            device=self.device,
            buffer_size=config['buffer_size'],
            batch_size=config['batch_size'],
            epsilon_start=1.0,
            epsilon_end=0.01,
            epsilon_decay=100000
        )

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # è®­ç»ƒç»Ÿè®¡å®¹å™¨
        self.training_stats = {
            'phase1': {},
            'phase2': {},
            'phase3': {}
        }

    def _create_model(self, node_feat_dim, edge_feat_dim, request_dim,
                      hidden_dim, num_actions):
        """æ™ºèƒ½åˆ›å»ºæ¨¡å‹ - è‡ªåŠ¨é€‚é…å‚æ•°"""
        import inspect
        sig = inspect.signature(GNN_HRL_Controller.__init__)
        valid_params = set(sig.parameters.keys()) - {'self'}

        model_kwargs = {
            'node_feat_dim': node_feat_dim,
            'edge_feat_dim': edge_feat_dim,
            'request_dim': request_dim,
            'hidden_dim': hidden_dim,
            'num_actions': num_actions
        }

        # æ·»åŠ å¯é€‰å‚æ•°
        if 'use_cache' in valid_params:
            model_kwargs['use_cache'] = False
        if 'use_checkpoint' in valid_params:
            model_kwargs['use_checkpoint'] = True

        logger.info(f"æ¨¡å‹åˆå§‹åŒ–å‚æ•°: {list(model_kwargs.keys())}")
        return GNN_HRL_Controller(**model_kwargs)

    def run_phase1_collect_expert_data(self) -> str:
        """
        [é˜¶æ®µ1] æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ®
        Returns: expert_data_path
        """
        logger.info("\n" + "=" * 80)
        logger.info("é˜¶æ®µ1: æ”¶é›†ä¸“å®¶æ¼”ç¤ºæ•°æ® (Expert Data Collection)")
        logger.info("=" * 80)

        expert_data_path = self.output_dir / "expert_data" / "expert_data_final.pkl"

        # å¦‚æœæ•°æ®å·²å­˜åœ¨ï¼Œè¯¢é—®æ˜¯å¦è·³è¿‡
        if expert_data_path.exists():
            logger.info(f"å‘ç°å·²å­˜åœ¨çš„ä¸“å®¶æ•°æ®: {expert_data_path}")
            return str(expert_data_path)

        # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
        collector = ExpertDataCollector(
            env=self.env,
            output_dir=self.output_dir / "expert_data"
        )

        # å¼€å§‹æ”¶é›†
        expert_buffer = collector.collect(
            num_episodes=self.config['phase1_episodes'],
            min_confidence=self.config['min_expert_confidence'],
            save_interval=100
        )

        self.training_stats['phase1'] = collector.stats
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: æ”¶é›†äº† {len(expert_buffer)} æ¡æ•°æ®")
        return str(expert_data_path)

    def run_phase2_imitation_learning(self, expert_data_path: str):
        """
        [é˜¶æ®µ2] æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒ (Behavior Cloning)
        """
        logger.info("\n" + "=" * 80)
        logger.info("é˜¶æ®µ2: æ¨¡ä»¿å­¦ä¹ é¢„è®­ç»ƒ (Imitation Learning)")
        logger.info("=" * 80)

        # åŠ è½½æ•°æ®
        expert_buffer, _ = ExpertDataCollector.load_expert_data(Path(expert_data_path))

        # åˆ’åˆ†æ•°æ®é›†
        random.shuffle(expert_buffer)
        split_idx = int(len(expert_buffer) * 0.9)
        train_data = expert_buffer[:split_idx]
        val_data = expert_buffer[split_idx:]

        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_data)} | éªŒè¯é›†å¤§å°: {len(val_data)}")

        # åˆ‡æ¢æ¨¡å¼
        self.agent.switch_to_imitation_mode()

        num_epochs = self.config['phase2_epochs']
        batch_size = self.config['batch_size']
        best_val_acc = 0.0
        patience = 5
        patience_counter = 0

        for epoch in range(num_epochs):
            # --- è®­ç»ƒ ---
            random.shuffle(train_data)
            epoch_losses = []
            epoch_accs = []

            for i in range(0, len(train_data), batch_size):
                batch = train_data[i:i + batch_size]
                loss, acc = self.agent.supervised_update(batch)
                epoch_losses.append(loss)
                epoch_accs.append(acc)

            avg_loss = np.mean(epoch_losses)
            avg_acc = np.mean(epoch_accs)

            # --- éªŒè¯ ---
            if epoch % 5 == 0 or epoch == num_epochs - 1:
                val_metrics = self.agent.evaluate_imitation(val_data, num_samples=500)
                val_acc = val_metrics['accuracy']

                logger.info(f"Epoch {epoch + 1}/{num_epochs} | "
                            f"Train Loss: {avg_loss:.4f} Acc: {avg_acc:.2f}% | "
                            f"Val Acc: {val_acc:.2f}%")

                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    patience_counter = 0
                    self.agent.save(str(self.output_dir / "best_imitation_model.pth"))
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    logger.info(f"æ—©åœ: éªŒè¯é›†å‡†ç¡®ç‡ {patience} æ¬¡æœªæå‡")
                    break

        # æ¢å¤æœ€ä½³æƒé‡
        if best_val_acc > 0:
            self.agent.load(str(self.output_dir / "best_imitation_model.pth"))
            logger.info(f"âœ… é˜¶æ®µ2å®Œæˆ. åŠ è½½æœ€ä½³æ¨¡å‹ (Val Acc: {best_val_acc:.2f}%)")

        self.training_stats['phase2'] = {'best_val_acc': best_val_acc}

    def run_phase3_rl_finetuning(self):
        """
        [é˜¶æ®µ3] å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (RL Fine-tuning) - ä¿®å¤ç‰ˆ
        """
        logger.info("\n" + "=" * 80)
        logger.info("é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (RL Fine-tuning)")
        logger.info("=" * 80)

        # âœ… å…³é”®ä¿®å¤: é‡æ–°åˆå§‹åŒ–ç¯å¢ƒä»¥é‡ç½®è¯·æ±‚é˜Ÿåˆ—
        logger.info("ğŸ”§ é‡æ–°åˆå§‹åŒ–ç¯å¢ƒä»¥åŠ è½½æ–°æ•°æ®...")
        self.env = SFC_HIRL_Env_GNN(
            input_dir=self.config['input_dir'],
            topo=self.config['topology'],
            dc_nodes=self.config['dc_nodes'],
            capacities=self.config['capacities'],
            use_gnn=True
        )

        # æ£€æŸ¥å¯ç”¨è¯·æ±‚æ•°
        if hasattr(self.env, 'requests'):
            logger.info(f"âœ… ç¯å¢ƒå·²é‡ç½®ï¼Œå¯ç”¨è¯·æ±‚æ•°: {len(self.env.requests)}")
        elif hasattr(self.env, 'data_loader'):
            logger.info(f"âœ… ç¯å¢ƒå·²é‡ç½®ï¼Œä½¿ç”¨æ•°æ®åŠ è½½å™¨")
        else:
            logger.warning("âš ï¸  æ— æ³•ç¡®è®¤è¯·æ±‚æ•°é‡")

        # åˆ‡æ¢å› RL æ¨¡å¼
        self.agent.switch_to_rl_mode(start_epsilon=self.config['phase3_start_epsilon'])
        logger.info(f"åˆ‡æ¢åˆ°RLæ¨¡å¼: epsilon={self.config['phase3_start_epsilon']}")

        num_episodes = self.config['phase3_episodes']
        expert_ratio = self.config['phase3_expert_ratio']

        logger.info(f"å¼€å§‹RLè®­ç»ƒ: {num_episodes} episodes")
        logger.info(f"ä¸“å®¶æ··åˆæ¯”ä¾‹: {expert_ratio}")

        ep_rewards = []
        acceptance_rates = []

        ep_count = 0
        failed_reset_count = 0  # è¿½è¸ªè¿ç»­å¤±è´¥æ¬¡æ•°

        while ep_count < num_episodes:
            req, state = self.env.reset_request()

            # âœ… æ·»åŠ è°ƒè¯•ä¿¡æ¯
            if req is None:
                failed_reset_count += 1
                if failed_reset_count == 1:
                    logger.error(f"âŒ Episode {ep_count}: reset_request() è¿”å› None - æ•°æ®å·²è€—å°½")
                    logger.error(f"   å·²å®Œæˆ episodes: {ep_count}/{num_episodes}")
                    logger.error(f"   æç¤º: æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„è¯·æ±‚")
                if failed_reset_count >= 10:
                    logger.error(f"âŒ è¿ç»­10æ¬¡æ— æ³•è·å–è¯·æ±‚ï¼Œç»ˆæ­¢è®­ç»ƒ")
                    break
                continue

            failed_reset_count = 0  # é‡ç½®è®¡æ•°å™¨

            ep_reward = 0.0
            done = False
            step = 0
            max_steps = 100

            while not done and step < max_steps:
                if not self.env.unadded_dest_indices:
                    break

                # --- ç›®æ ‡é€‰æ‹© ---
                valid_goals = list(self.env.unadded_dest_indices)

                if random.random() < expert_ratio:
                    flat_state = self.env._get_flat_state()
                    candidates = self.env.get_expert_high_level_candidates(flat_state)
                    goal = candidates[0][0] if candidates else random.choice(valid_goals)
                else:
                    goal = random.choice(valid_goals)

                # --- åŠ¨ä½œé€‰æ‹© ---
                valid_actions = self.env.get_valid_low_level_actions()
                if not valid_actions:
                    break

                expert_action = None
                if random.random() < expert_ratio:
                    try:
                        expert_action = self.env.expert_low_level_action(goal)
                    except:
                        pass

                action = self.agent.select_action(
                    state, goal, valid_actions,
                    expert_action=expert_action,
                    beta=expert_ratio
                )

                # --- æ‰§è¡Œ ---
                next_state, reward, sub_done, req_done = self.env.step_low_level(goal, action)

                # --- å­˜å‚¨ä¸æ›´æ–° ---
                next_valid = self.env.get_valid_low_level_actions() if not req_done else None

                try:
                    self.agent.store(state, action, reward, next_state, req_done, goal, next_valid)
                except TypeError:
                    self.agent.store(state, action, reward, next_state, req_done, goal)

                self.agent.update()

                state = next_state
                ep_reward += reward
                step += 1

                if req_done:
                    done = True

            # è®°å½•ä¸æ—¥å¿—
            ep_count += 1
            ep_rewards.append(ep_reward)

            if ep_count % 10 == 0:
                total = max(1, self.env.total_requests_seen)
                acc = self.env.total_requests_accepted / total * 100
                acceptance_rates.append(acc)
                avg_rew = np.mean(ep_rewards[-10:])
                logger.info(
                    f"Episode {ep_count}/{num_episodes} | "
                    f"Reward: {avg_rew:.2f} | "
                    f"Acc: {acc:.2f}% | "
                    f"Eps: {self.agent.get_epsilon():.3f}")

            if ep_count % 100 == 0:
                self.agent.save(str(self.output_dir / f"rl_model_ep{ep_count}.pth"))
                logger.info(f"ğŸ’¾ Checkpoint saved: rl_model_ep{ep_count}.pth")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.agent.save(str(self.output_dir / "final_model.pth"))

        # ç»Ÿè®¡
        final_acc = acceptance_rates[-1] if acceptance_rates else 0.0
        avg_reward = np.mean(ep_rewards) if ep_rewards else 0.0

        self.training_stats['phase3'] = {
            'episodes_completed': ep_count,
            'final_acc': final_acc,
            'avg_reward': avg_reward
        }

        logger.info("=" * 80)
        logger.info("é˜¶æ®µ3è®­ç»ƒå®Œæˆç»Ÿè®¡")
        logger.info("=" * 80)
        logger.info(f"å®ŒæˆEpisodes: {ep_count}/{num_episodes}")
        logger.info(f"å¹³å‡Reward: {avg_reward:.2f}")
        logger.info(f"æœ€ç»ˆæˆåŠŸç‡: {final_acc:.2f}%")
        logger.info("=" * 80)
        logger.info(f"âœ… é˜¶æ®µ3å®Œæˆ. æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜.")

    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        # 1. æ”¶é›†æ•°æ®
        data_path = self.run_phase1_collect_expert_data()
        # 2. æ¨¡ä»¿å­¦ä¹ 
        self.run_phase2_imitation_learning(data_path)
        # 3. RL å¾®è°ƒ
        self.run_phase3_rl_finetuning()

        logger.info("\nğŸ‰ æ‰€æœ‰è®­ç»ƒé˜¶æ®µå®Œæˆï¼")


# ============================================================================
# ä¸»å…¥å£
# ============================================================================
if __name__ == "__main__":
    # é…ç½®
    config = {
        'input_dir': H.INPUT_DIR,
        'topology': H.TOPOLOGY_MATRIX,
        'dc_nodes': H.DC_NODES,
        'capacities': H.CAPACITIES,

        'hidden_dim': 128,
        'lr': 1e-4,
        'gamma': 0.99,
        'buffer_size': 20000,
        'batch_size': 32,

        'phase1_episodes': 500,
        'min_expert_confidence': 0.1,

        'phase2_epochs': 30,

        'phase3_episodes': 1500,
        'phase3_start_epsilon': 0.3,
        'phase3_expert_ratio': 0.2,

        'output_dir': H.OUTPUT_DIR / "three_phase_results"
    }

    trainer = ThreePhaseTrainer(config)
    trainer.run()