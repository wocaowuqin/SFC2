#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
multicast_aware_gat.py
è¯·æ±‚æ„ŸçŸ¥çš„å¤šæ’­å›¾æ³¨æ„åŠ›ç½‘ç»œ

åˆ›æ–°ç‚¹:
1. å¤šç›®æ ‡é›†åˆç¼–ç  (Set Transformer)
2. è¯·æ±‚è°ƒåˆ¶æ³¨æ„åŠ›
3. VNFå…±äº«æ½œåŠ›é¢„æµ‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from typing import List, Optional, Tuple


class SetTransformer(nn.Module):
    """
    å¤šç›®æ ‡é›†åˆç¼–ç å™¨ (æ’åˆ—ä¸å˜æ€§)

    ç†è®ºåŸºç¡€: Deep Sets (Zaheer et al., NeurIPS 2017)
    f({x1, ..., xn}) = Ï(Î£ Ï†(xi))
    """

    def __init__(self, input_dim: int, hidden_dim: int, num_heads: int = 4):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim

        # Ï†: å…ƒç´ çº§ç¼–ç 
        self.element_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # Multi-head Self-Attention (æ•è·ç›®æ ‡é—´å…³ç³»)
        self.self_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=0.1,
            batch_first=True
        )

        # Ï: èšåˆå‡½æ•°
        self.aggregator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, dest_features: torch.Tensor) -> torch.Tensor:
        """
        Args:
            dest_features: [num_dests, input_dim] ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾
        Returns:
            aggregated: [hidden_dim] èšåˆåçš„é›†åˆè¡¨ç¤º
        """
        # 1. å…ƒç´ çº§ç¼–ç 
        encoded = self.element_encoder(dest_features)  # [num_dests, hidden_dim]

        # 2. Self-Attention (æ•è·ç›®æ ‡é—´ç›¸å…³æ€§)
        # ä¾‹å¦‚: ä¸¤ä¸ªç›®æ ‡å¦‚æœè·ç¦»å¾ˆè¿‘,å¯èƒ½å…±äº«VNF
        attn_out, _ = self.self_attn(
            encoded.unsqueeze(0),  # [1, num_dests, hidden_dim]
            encoded.unsqueeze(0),
            encoded.unsqueeze(0)
        )
        attn_out = attn_out.squeeze(0)  # [num_dests, hidden_dim]

        # 3. æ’åˆ—ä¸å˜èšåˆ (æ±‚å’Œ)
        aggregated = torch.sum(attn_out, dim=0)  # [hidden_dim]

        # 4. æœ€ç»ˆæ˜ å°„
        output = self.aggregator(aggregated)

        return output


class RequestModulatedAttention(nn.Module):
    """
    è¯·æ±‚è°ƒåˆ¶çš„å›¾æ³¨æ„åŠ›

    åˆ›æ–°ç‚¹: æ³¨æ„åŠ›æƒé‡ç”±è¯·æ±‚ç‰¹å¾åŠ¨æ€è°ƒåˆ¶
    Î±_ij = attention(h_i, h_j, request_vec)
    """

    def __init__(self, node_dim: int, request_dim: int, hidden_dim: int):
        super().__init__()

        # æ ‡å‡†GATæ³¨æ„åŠ›è®¡ç®—
        self.attn_src = nn.Linear(node_dim, hidden_dim)
        self.attn_dst = nn.Linear(node_dim, hidden_dim)

        # ğŸ”¥ åˆ›æ–°: è¯·æ±‚è°ƒåˆ¶ç½‘ç»œ
        self.request_modulator = nn.Sequential(
            nn.Linear(request_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # æ³¨æ„åŠ›æƒé‡è®¡ç®—
        self.attn_weight = nn.Linear(hidden_dim * 3, 1)

    def forward(self, h_i: torch.Tensor, h_j: torch.Tensor,
                request_vec: torch.Tensor) -> torch.Tensor:
        """
        Args:
            h_i: [batch, node_dim] æºèŠ‚ç‚¹ç‰¹å¾
            h_j: [batch, node_dim] ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾
            request_vec: [batch, request_dim] è¯·æ±‚å‘é‡
        Returns:
            alpha: [batch, 1] æ³¨æ„åŠ›æƒé‡
        """
        # 1. èŠ‚ç‚¹ç‰¹å¾æ˜ å°„
        src_feat = self.attn_src(h_i)  # [batch, hidden_dim]
        dst_feat = self.attn_dst(h_j)  # [batch, hidden_dim]

        # 2. ğŸ”¥ è¯·æ±‚è°ƒåˆ¶
        req_feat = self.request_modulator(request_vec)  # [batch, hidden_dim]

        # 3. æ‹¼æ¥å¹¶è®¡ç®—æƒé‡
        combined = torch.cat([src_feat, dst_feat, req_feat], dim=-1)
        alpha = self.attn_weight(combined)  # [batch, 1]

        return torch.sigmoid(alpha)


class VNFSharingPredictor(nn.Module):
    """
    VNFå…±äº«æ½œåŠ›é¢„æµ‹å™¨

    åŠŸèƒ½: é¢„æµ‹æ¯ä¸ªèŠ‚ç‚¹ä½œä¸ºVNFå…±äº«ç‚¹çš„æ½œåŠ›
    è¾“å…¥: èŠ‚ç‚¹ç‰¹å¾ + ç›®æ ‡é›†åˆç‰¹å¾
    è¾“å‡º: å…±äº«æ½œåŠ›åˆ†æ•° [0, 1]
    """

    def __init__(self, node_dim: int, dest_set_dim: int, hidden_dim: int = 128):
        super().__init__()

        self.predictor = nn.Sequential(
            nn.Linear(node_dim + dest_set_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # è¾“å‡º [0, 1]
        )

    def forward(self, node_feat: torch.Tensor,
                dest_set_feat: torch.Tensor) -> torch.Tensor:
        """
        Args:
            node_feat: [num_nodes, node_dim]
            dest_set_feat: [dest_set_dim] ç›®æ ‡é›†åˆç‰¹å¾
        Returns:
            scores: [num_nodes] æ¯ä¸ªèŠ‚ç‚¹çš„å…±äº«æ½œåŠ›
        """
        # æ‰©å±•dest_set_featä»¥åŒ¹é…èŠ‚ç‚¹æ•°
        dest_expanded = dest_set_feat.unsqueeze(0).expand(
            node_feat.size(0), -1
        )  # [num_nodes, dest_set_dim]

        # æ‹¼æ¥
        combined = torch.cat([node_feat, dest_expanded], dim=-1)

        # é¢„æµ‹
        scores = self.predictor(combined).squeeze(-1)  # [num_nodes]

        return scores


class MulticastAwareGAT(nn.Module):
    """
    å®Œæ•´çš„å¤šæ’­æ„ŸçŸ¥GATç½‘ç»œ

    æ•´åˆæ‰€æœ‰åˆ›æ–°ç»„ä»¶
    """

    def __init__(self, node_feat_dim: int, edge_feat_dim: int,
                 request_dim: int, hidden_dim: int = 128,
                 num_gat_layers: int = 3, num_heads: int = 4):
        super().__init__()

        self.hidden_dim = hidden_dim

        # ===== ç»„ä»¶1: åŸºç¡€ç‰¹å¾ç¼–ç  =====
        self.node_embedding = nn.Linear(node_feat_dim, hidden_dim)
        self.edge_embedding = nn.Linear(edge_feat_dim, hidden_dim)

        # ===== ç»„ä»¶2: å¤šç›®æ ‡é›†åˆç¼–ç å™¨ (åˆ›æ–°) =====
        self.dest_set_encoder = SetTransformer(
            input_dim=hidden_dim,
            hidden_dim=hidden_dim,
            num_heads=num_heads
        )

        # ===== ç»„ä»¶3: è¯·æ±‚è°ƒåˆ¶GATå±‚ (åˆ›æ–°) =====
        self.gat_layers = nn.ModuleList()
        self.request_modulators = nn.ModuleList()

        for _ in range(num_gat_layers):
            # æ ‡å‡†GATå±‚
            self.gat_layers.append(
                GATConv(
                    in_channels=hidden_dim,
                    out_channels=hidden_dim // num_heads,
                    heads=num_heads,
                    edge_dim=hidden_dim,
                    concat=True
                )
            )

            # è¯·æ±‚è°ƒåˆ¶å™¨ (æ¯å±‚ä¸€ä¸ª)
            self.request_modulators.append(
                RequestModulatedAttention(
                    node_dim=hidden_dim,
                    request_dim=request_dim,
                    hidden_dim=hidden_dim
                )
            )

        # Layer Norm
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim) for _ in range(num_gat_layers)
        ])

        # ===== ç»„ä»¶4: VNFå…±äº«æ½œåŠ›é¢„æµ‹ (åˆ›æ–°) =====
        self.sharing_predictor = VNFSharingPredictor(
            node_dim=hidden_dim,
            dest_set_dim=hidden_dim,
            hidden_dim=hidden_dim
        )

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,
                edge_attr: torch.Tensor, request_vec: torch.Tensor,
                dest_indices: List[int],
                batch: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å®Œæ•´å‰å‘ä¼ æ’­

        Args:
            x: [num_nodes, node_feat_dim] èŠ‚ç‚¹ç‰¹å¾
            edge_index: [2, num_edges] è¾¹ç´¢å¼•
            edge_attr: [num_edges, edge_feat_dim] è¾¹ç‰¹å¾
            request_vec: [request_dim] è¯·æ±‚å‘é‡
            dest_indices: List[int] ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
            batch: [num_nodes] batchç´¢å¼• (å¯é€‰)

        Returns:
            node_embeddings: [num_nodes, hidden_dim] èŠ‚ç‚¹åµŒå…¥
            dest_set_embedding: [hidden_dim] ç›®æ ‡é›†åˆåµŒå…¥
            sharing_scores: [num_nodes] VNFå…±äº«æ½œåŠ›åˆ†æ•°
        """
        device = x.device

        # ===== Step 1: åˆå§‹ç‰¹å¾ç¼–ç  =====
        x = self.node_embedding(x)  # [num_nodes, hidden_dim]
        e = self.edge_embedding(edge_attr)  # [num_edges, hidden_dim]

        # ===== Step 2: ç›®æ ‡é›†åˆç¼–ç  (åˆ›æ–°) =====
        dest_features = x[dest_indices]  # [num_dests, hidden_dim]
        dest_set_feat = self.dest_set_encoder(dest_features)  # [hidden_dim]

        # ===== Step 3: è¯·æ±‚è°ƒåˆ¶çš„GATä¼ æ’­ (åˆ›æ–°) =====
        for gat_layer, modulator, norm in zip(
                self.gat_layers, self.request_modulators, self.layer_norms
        ):
            residual = x

            # æ ‡å‡†GATä¼ æ’­
            x_gat = gat_layer(x, edge_index, e)  # [num_nodes, hidden_dim]

            # ğŸ”¥ è¯·æ±‚è°ƒåˆ¶ (åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›)
            # è¿™é‡Œç®€åŒ–å¤„ç†: ä¸ºæ¯ä¸ªèŠ‚ç‚¹è®¡ç®—è°ƒåˆ¶æƒé‡
            # å®é™…å®ç°å¯ä»¥æ›´ç²¾ç»†
            request_expanded = request_vec.unsqueeze(0).expand(x.size(0), -1)
            modulation_weights = modulator(
                x, x, request_expanded
            )  # [num_nodes, 1]

            # åº”ç”¨è°ƒåˆ¶
            x_modulated = x_gat * modulation_weights

            # æ®‹å·®è¿æ¥ + LayerNorm
            x = norm(residual + x_modulated)

        # ===== Step 4: VNFå…±äº«æ½œåŠ›é¢„æµ‹ (åˆ›æ–°) =====
        sharing_scores = self.sharing_predictor(x, dest_set_feat)

        # ===== Step 5: å›¾çº§èšåˆ =====
        if batch is None:
            graph_emb = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long, device=device))
        else:
            graph_emb = global_mean_pool(x, batch)

        return x, dest_set_feat, sharing_scores