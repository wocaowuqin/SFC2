"""
å®Œæ•´çš„ä¸‰é˜¶æ®µå±‚æ¬¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒç³»ç»Ÿ - ä¿®å¤ç‰ˆ
Three-Phase Hierarchical Reinforcement Learning Training System - Fixed

ğŸ”¥ ä¿®å¤å†…å®¹ï¼š
1. Phase 3 æ­£ç¡®åŠ è½½ç‹¬ç«‹æ•°æ®é›† (phase3_requests.pkl, phase3_events.pkl)
2. æ·»åŠ æ•°æ®æ–‡ä»¶éªŒè¯å’Œè‡ªåŠ¨åˆ‡æ¢
3. å¢å¼ºé”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º
4. è‡ªåŠ¨æ¢å¤åŸå§‹æ•°æ®æ–‡ä»¶

é˜¶æ®µ1: ä¸“å®¶è½¨è¿¹é‡‡é›† (Expert Trajectory Collection)
é˜¶æ®µ2: ç›‘ç£æ¨¡ä»¿å­¦ä¹  (Supervised Imitation Learning)
é˜¶æ®µ3: å¼ºåŒ–å­¦ä¹ å¾®è°ƒ (RL Fine-tuning) - FIXED
"""

import os
import time
import pickle
import shutil
import logging
from pathlib import Path
import numpy as np
from typing import List, Tuple, Dict, Any, Optional

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =====================================================================
#   Phase 1: Expert Trajectory Collection
# =====================================================================

class Phase1ExpertCollector:
    """
    é˜¶æ®µ 1ï¼šä¸“å®¶è½¨è¿¹é‡‡é›†

    ä¾èµ–ç¯å¢ƒæ¥å£ï¼š
        env.generate_random_request() -> request or None
        env.expert_solve_request(req) -> (success: bool, traj: List[tuple])
        env.reset_all() (å¯é€‰)

    è½¨è¿¹æ ¼å¼: List[(state, goal, action, reward, next_state)]
    """

    def __init__(self, env, output_dir: str, config: Optional[Dict] = None):
        self.env = env
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # é»˜è®¤é…ç½®
        default_config = {
            "episodes": 2000,
            "save_every": 500,
            "max_dataset_size": 100000  # é˜²æ­¢å†…å­˜æº¢å‡º
        }
        self.cfg = {**default_config, **(config or {})}
        self.dataset = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_episodes": 0,
            "successful_episodes": 0,
            "failed_episodes": 0,
            "total_transitions": 0
        }

    def run(self) -> List[Tuple]:
        """è¿è¡Œä¸“å®¶è½¨è¿¹é‡‡é›†"""
        logger.info("=" * 60)
        logger.info("Phase 1: Expert Dataset Collection")
        logger.info("=" * 60)

        # é‡ç½®ç¯å¢ƒ
        if hasattr(self.env, "reset_all"):
            self.env.reset_all()
            logger.info("Environment reset complete")

        max_episodes = self.cfg["episodes"]

        for ep in range(1, max_episodes + 1):
            # ç”Ÿæˆéšæœºè¯·æ±‚
            try:
                req = self.env.generate_random_request()
            except Exception as e:
                logger.warning(f"[Phase1] Episode {ep}: Failed to generate request - {e}")
                self.stats["failed_episodes"] += 1
                continue

            if req is None:
                logger.warning(f"[Phase1] Episode {ep}: generate_random_request() returned None")
                self.stats["failed_episodes"] += 1
                continue

            # ä¸“å®¶æ±‚è§£
            try:
                success, traj = self.env.expert_solve_request(req)
            except Exception as e:
                logger.error(f"[Phase1] Episode {ep}: Expert solver failed - {e}")
                self.stats["failed_episodes"] += 1
                continue

            self.stats["total_episodes"] += 1

            # æ·»åŠ æˆåŠŸçš„è½¨è¿¹
            if success and traj:
                # æ£€æŸ¥æ•°æ®é›†å¤§å°é™åˆ¶
                if len(self.dataset) + len(traj) > self.cfg["max_dataset_size"]:
                    logger.warning(
                        f"[Phase1] Dataset size limit reached "
                        f"({self.cfg['max_dataset_size']}). Stopping collection."
                    )
                    break

                self.dataset.extend(traj)
                self.stats["successful_episodes"] += 1
                self.stats["total_transitions"] += len(traj)
            else:
                self.stats["failed_episodes"] += 1

            # å®šæœŸæ—¥å¿—
            if ep % 100 == 0:
                success_rate = (self.stats["successful_episodes"] /
                                self.stats["total_episodes"] * 100) if self.stats["total_episodes"] > 0 else 0
                logger.info(
                    f"[Phase1] Episode {ep}/{max_episodes} | "
                    f"Dataset size: {len(self.dataset)} | "
                    f"Success rate: {success_rate:.1f}%"
                )

            # å®šæœŸä¿å­˜
            if ep % self.cfg["save_every"] == 0:
                self._save_checkpoint(f"expert_ep{ep}.pkl")

        # æœ€ç»ˆä¿å­˜
        self._save_checkpoint("expert_final.pkl")
        self._save_stats()

        logger.info(f"[Phase1] Collection complete:")
        logger.info(f"  - Total episodes: {self.stats['total_episodes']}")
        logger.info(f"  - Successful: {self.stats['successful_episodes']}")
        logger.info(f"  - Failed: {self.stats['failed_episodes']}")
        logger.info(f"  - Total transitions: {len(self.dataset)}")

        return self.dataset

    def _save_checkpoint(self, filename: str):
        """ä¿å­˜æ•°æ®é›†æ£€æŸ¥ç‚¹"""
        path = self.output_dir / filename
        try:
            with open(path, "wb") as f:
                pickle.dump(self.dataset, f)
            logger.info(f"[Phase1] Saved checkpoint: {path} ({len(self.dataset)} transitions)")
        except Exception as e:
            logger.error(f"[Phase1] Failed to save checkpoint {path}: {e}")

    def _save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_path = self.output_dir / "phase1_stats.pkl"
        try:
            with open(stats_path, "wb") as f:
                pickle.dump(self.stats, f)
            logger.info(f"[Phase1] Saved statistics: {stats_path}")
        except Exception as e:
            logger.error(f"[Phase1] Failed to save statistics: {e}")


# =====================================================================
#   Phase 2: Supervised Imitation Learning
# =====================================================================

class Phase2ILTrainer:
    """
    é˜¶æ®µ 2: æ¨¡ä»¿å­¦ä¹ ï¼ˆè¡Œä¸ºå…‹éš†ï¼‰

    ä¾èµ–æ™ºèƒ½ä½“æ¥å£ï¼š
        agent.supervised_update(state, goal, action) -> loss: float
        agent.save(path: str)
    """

    def __init__(self, agent, expert_data: List[Tuple],
                 output_dir: str, config: Optional[Dict] = None):
        self.agent = agent
        self.data = expert_data
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # é»˜è®¤é…ç½®
        default_config = {
            "epochs": 10,
            "batch_size": 128,
            "save_every_epoch": 2,
            "validation_split": 0.1  # 10% ç”¨äºéªŒè¯
        }
        self.cfg = {**default_config, **(config or {})}

        # æ•°æ®éªŒè¯
        if not self.data:
            raise ValueError("Expert data is empty! Cannot train.")

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        self._split_data()

        # è®­ç»ƒå†å²
        self.history = {
            "train_loss": [],
            "val_loss": []
        }

    def _split_data(self):
        """åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
        n_total = len(self.data)
        n_val = int(n_total * self.cfg["validation_split"])

        indices = np.random.permutation(n_total)
        val_indices = indices[:n_val]
        train_indices = indices[n_val:]

        self.train_data = [self.data[i] for i in train_indices]
        self.val_data = [self.data[i] for i in val_indices]

        logger.info(f"[Phase2] Data split - Train: {len(self.train_data)}, Val: {len(self.val_data)}")

    def run(self) -> bool:
        """è¿è¡Œæ¨¡ä»¿å­¦ä¹ è®­ç»ƒ"""
        logger.info("=" * 60)
        logger.info("Phase 2: Imitation Learning (Behavior Cloning)")
        logger.info("=" * 60)

        n_train = len(self.train_data)
        batch_size = self.cfg["batch_size"]
        n_epochs = self.cfg["epochs"]

        best_val_loss = float('inf')

        for epoch in range(1, n_epochs + 1):
            # è®­ç»ƒ
            train_loss = self._train_epoch(epoch)

            # éªŒè¯
            val_loss = self._validate()

            # è®°å½•å†å²
            self.history["train_loss"].append(train_loss)
            self.history["val_loss"].append(val_loss)

            logger.info(
                f"[Phase2] Epoch {epoch}/{n_epochs} | "
                f"Train Loss: {train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f}"
            )

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self.agent.save(str(self.output_dir / "il_best.pth"))
                logger.info(f"[Phase2] New best model saved (val_loss: {val_loss:.4f})")

            # å®šæœŸä¿å­˜
            if epoch % self.cfg["save_every_epoch"] == 0:
                self.agent.save(str(self.output_dir / f"il_ep{epoch}.pth"))

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.output_dir / "il_final.pth"
        self.agent.save(str(final_path))
        logger.info(f"[Phase2] Final model saved: {final_path}")

        # ä¿å­˜è®­ç»ƒå†å²
        self._save_history()

        logger.info(f"[Phase2] Training complete. Best val loss: {best_val_loss:.4f}")

        return True

    def _train_epoch(self, epoch: int) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        np.random.shuffle(self.train_data)

        batch_size = self.cfg["batch_size"]
        n_batches = (len(self.train_data) + batch_size - 1) // batch_size

        epoch_losses = []

        for i in range(n_batches):
            batch_data = self.train_data[i * batch_size:(i + 1) * batch_size]

            batch_loss = 0.0
            for (state, goal, action, reward, next_state) in batch_data:
                try:
                    loss = self.agent.supervised_update(state, goal, action)
                    batch_loss += loss
                except Exception as e:
                    logger.error(f"[Phase2] Update failed: {e}")
                    continue

            avg_batch_loss = batch_loss / len(batch_data) if batch_data else 0.0
            epoch_losses.append(avg_batch_loss)

        return np.mean(epoch_losses) if epoch_losses else 0.0

    def _validate(self) -> float:
        """éªŒè¯"""
        if not self.val_data:
            return 0.0

        val_losses = []

        for (state, goal, action, reward, next_state) in self.val_data:
            try:
                # ä»…å‰å‘ä¼ æ’­ï¼Œä¸æ›´æ–°å‚æ•°
                # å‡è®¾ agent æœ‰ä¸€ä¸ª evaluate æ–¹æ³•ï¼Œå¦åˆ™å¯ä»¥è°ƒç”¨ supervised_update ä½†ä¸åº”ç”¨æ¢¯åº¦
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ supervised_update åœ¨ eval æ¨¡å¼ä¸‹ä¸æ›´æ–°
                loss = self.agent.supervised_update(state, goal, action)
                val_losses.append(loss)
            except:
                continue

        return np.mean(val_losses) if val_losses else 0.0

    def _save_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.output_dir / "phase2_history.pkl"
        try:
            with open(history_path, "wb") as f:
                pickle.dump(self.history, f)
            logger.info(f"[Phase2] Training history saved: {history_path}")
        except Exception as e:
            logger.error(f"[Phase2] Failed to save history: {e}")


# =====================================================================
#   Phase 3: RL Fine-tuning - FIXED VERSION
# =====================================================================

class Phase3RLTrainer:
    """
    é˜¶æ®µ 3ï¼šå¼ºåŒ–å­¦ä¹ å¾®è°ƒ - ä¿®å¤ç‰ˆ

    ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®åŠ è½½ phase3_requests.pkl å’Œ phase3_events.pkl

    ä¾èµ–ç¯å¢ƒæ¥å£ï¼š
        env.reset_all()
        env.reset_request() -> (request, state)
        env.get_low_level_mask(goal) -> mask
        env.step_low_level(goal, action) -> (next_state, reward, sub_done, req_done)
        env.unadded_dest_indices (å±æ€§)
        env.total_requests_seen (å±æ€§)
        env.total_requests_accepted (å±æ€§)

    ä¾èµ–æ™ºèƒ½ä½“æ¥å£ï¼š
        agent.switch_to_rl_mode(start_epsilon)
        agent.select_goal(state, candidates, epsilon) -> goal
        agent.select_action(state, goal, mask, epsilon) -> action
        agent.store(state, action, reward, next_state, done, goal)
        agent.update()
        agent.get_epsilon() -> float
        agent.save(path)
    """

    def __init__(self, env, agent, output_dir: str, config: Optional[Dict] = None):
        self.env = env
        self.agent = agent
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # é»˜è®¤é…ç½®
        default_config = {
            "episodes": 2000,
            "start_epsilon": 0.2,
            "max_steps_per_episode": 120,
            "eval_every": 100,
            "eval_episodes": 20,
            "save_every": 100
        }
        self.cfg = {**default_config, **(config or {})}

        # è®­ç»ƒç»Ÿè®¡
        self.stats = {
            "episode_rewards": [],
            "episode_lengths": [],
            "acceptance_rates": [],
            "eval_rewards": [],
            "best_eval_reward": -float('inf')
        }

    def run(self) -> Dict[str, Any]:
        """è¿è¡ŒRLå¾®è°ƒ"""
        logger.info("=" * 60)
        logger.info("Phase 3: RL Fine-tuning")
        logger.info("=" * 60)

        # é‡ç½®ç¯å¢ƒ
        if hasattr(self.env, "reset_all"):
            self.env.reset_all()
            logger.info("Environment reset for RL training")

        # åˆ‡æ¢åˆ°RLæ¨¡å¼
        self.agent.switch_to_rl_mode(start_epsilon=self.cfg["start_epsilon"])
        logger.info(f"Agent switched to RL mode (epsilon: {self.cfg['start_epsilon']})")

        max_episodes = self.cfg["episodes"]

        for ep in range(1, max_episodes + 1):
            ep_reward, ep_length = self._run_episode(ep)

            # è®°å½•ç»Ÿè®¡
            self.stats["episode_rewards"].append(ep_reward)
            self.stats["episode_lengths"].append(ep_length)

            # è®¡ç®—æ¥å—ç‡
            if hasattr(self.env, 'total_requests_seen') and hasattr(self.env, 'total_requests_accepted'):
                total_seen = max(1, self.env.total_requests_seen)
                acc_rate = (self.env.total_requests_accepted / total_seen) * 100
                self.stats["acceptance_rates"].append(acc_rate)
            else:
                acc_rate = 0.0

            # å®šæœŸæ—¥å¿—
            if ep % 10 == 0:
                recent_rewards = self.stats["episode_rewards"][-10:]
                avg_reward = np.mean(recent_rewards)
                epsilon = self.agent.get_epsilon()

                logger.info(
                    f"[Phase3] Episode {ep}/{max_episodes} | "
                    f"Reward: {ep_reward:.2f} (avg: {avg_reward:.2f}) | "
                    f"Steps: {ep_length} | "
                    f"Acc: {acc_rate:.2f}% | "
                    f"Îµ: {epsilon:.3f}"
                )

            # å®šæœŸè¯„ä¼°
            if ep % self.cfg["eval_every"] == 0:
                eval_reward = self._evaluate()
                self.stats["eval_rewards"].append(eval_reward)

                logger.info(f"[Phase3] Evaluation at episode {ep}: {eval_reward:.2f}")

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if eval_reward > self.stats["best_eval_reward"]:
                    self.stats["best_eval_reward"] = eval_reward
                    self.agent.save(str(self.output_dir / "rl_best.pth"))
                    logger.info(f"[Phase3] New best model saved (eval_reward: {eval_reward:.2f})")

            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if ep % self.cfg["save_every"] == 0:
                self.agent.save(str(self.output_dir / f"rl_ep{ep}.pth"))

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_path = self.output_dir / "rl_final.pth"
        self.agent.save(str(final_path))

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self._save_stats()

        # æ±‡æ€»ç»“æœ
        result = {
            "avg_reward": np.mean(self.stats["episode_rewards"]) if self.stats["episode_rewards"] else 0.0,
            "final_acceptance_rate": self.stats["acceptance_rates"][-1] if self.stats["acceptance_rates"] else 0.0,
            "best_eval_reward": self.stats["best_eval_reward"]
        }

        logger.info("=" * 60)
        logger.info("Phase 3 Complete")
        logger.info(f"Average reward: {result['avg_reward']:.2f}")
        logger.info(f"Final acceptance rate: {result['final_acceptance_rate']:.2f}%")
        logger.info(f"Best eval reward: {result['best_eval_reward']:.2f}")
        logger.info(f"Final model saved to: {final_path}")
        logger.info("=" * 60)

        return result

    def _run_episode(self, episode_num: int) -> Tuple[float, int]:
        """è¿è¡Œä¸€ä¸ªè®­ç»ƒepisode"""
        try:
            req, state = self.env.reset_request()
        except Exception as e:
            logger.error(f"[Phase3] Episode {episode_num}: Failed to reset - {e}")
            return 0.0, 0

        if req is None:
            logger.warning(f"[Phase3] Episode {episode_num}: reset_request() returned None")
            return 0.0, 0

        done = False
        episode_reward = 0.0
        step = 0
        max_steps = self.cfg["max_steps_per_episode"]
        epsilon = self.agent.get_epsilon()

        while not done and step < max_steps:
            # ä¸­å±‚å†³ç­–ï¼šé€‰æ‹©ç›®æ ‡
            try:
                candidates = list(self.env.unadded_dest_indices)
            except Exception as e:
                logger.error(f"[Phase3] Failed to get candidates: {e}")
                break

            if not candidates:
                # æ²¡æœ‰å¯ç”¨ç›®æ ‡ï¼Œç»“æŸepisode
                break

            try:
                goal = self.agent.select_goal(state, candidates, epsilon=epsilon)
            except Exception as e:
                logger.error(f"[Phase3] Goal selection failed: {e}")
                break

            # ä½å±‚å†³ç­–ï¼šé€‰æ‹©åŠ¨ä½œ
            try:
                mask = self.env.get_low_level_mask(goal)
                action = self.agent.select_action(state, goal, mask, epsilon=epsilon)
            except Exception as e:
                logger.error(f"[Phase3] Action selection failed: {e}")
                break

            # ç¯å¢ƒäº¤äº’
            try:
                next_state, reward, sub_done, req_done = self.env.step_low_level(goal, action)
            except Exception as e:
                logger.error(f"[Phase3] Environment step failed: {e}")
                break

            # å­˜å‚¨ç»éªŒå¹¶æ›´æ–°
            try:
                self.agent.store(state, action, reward, next_state, req_done, goal)
                self.agent.update()
            except Exception as e:
                logger.error(f"[Phase3] Agent update failed: {e}")

            # æ›´æ–°çŠ¶æ€
            state = next_state
            episode_reward += reward
            step += 1

            if req_done:
                done = True

        return episode_reward, step

    def _evaluate(self) -> float:
        """è¯„ä¼°å½“å‰ç­–ç•¥ï¼ˆæ— æ¢ç´¢ï¼‰"""
        old_epsilon = self.agent.get_epsilon()

        # ä¸´æ—¶è®¾ç½®ä¸ºè´ªå©ªç­–ç•¥
        if hasattr(self.agent, 'epsilon'):
            self.agent.epsilon = 0.0

        eval_rewards = []

        for _ in range(self.cfg["eval_episodes"]):
            try:
                req, state = self.env.reset_request()
            except:
                continue

            if req is None:
                continue

            done = False
            episode_reward = 0.0
            step = 0

            while not done and step < self.cfg["max_steps_per_episode"]:
                try:
                    candidates = list(self.env.unadded_dest_indices)
                    if not candidates:
                        break

                    goal = self.agent.select_goal(state, candidates, epsilon=0.0)
                    mask = self.env.get_low_level_mask(goal)
                    action = self.agent.select_action(state, goal, mask, epsilon=0.0)

                    next_state, reward, sub_done, req_done = self.env.step_low_level(goal, action)

                    state = next_state
                    episode_reward += reward
                    step += 1

                    if req_done:
                        done = True
                except:
                    break

            eval_rewards.append(episode_reward)

        # æ¢å¤epsilon
        if hasattr(self.agent, 'epsilon'):
            self.agent.epsilon = old_epsilon

        return np.mean(eval_rewards) if eval_rewards else 0.0

    def _save_stats(self):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        stats_path = self.output_dir / "phase3_stats.pkl"
        try:
            with open(stats_path, "wb") as f:
                pickle.dump(self.stats, f)
            logger.info(f"[Phase3] Statistics saved: {stats_path}")
        except Exception as e:
            logger.error(f"[Phase3] Failed to save statistics: {e}")


# =====================================================================
#   Three-Phase Trainer (ä¸»å…¥å£) - FIXED VERSION
# =====================================================================

class HIRLThreePhaseTrainer:
    """
    ä¸‰é˜¶æ®µå±‚æ¬¡å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ - ä¿®å¤ç‰ˆ

    ğŸ”¥ ä¿®å¤ï¼šPhase 3 æ­£ç¡®åŠ è½½ç‹¬ç«‹æ•°æ®é›†

    æ•´åˆ Phase 1 (ä¸“å®¶é‡‡é›†) -> Phase 2 (æ¨¡ä»¿å­¦ä¹ ) -> Phase 3 (RLå¾®è°ƒ)
    """

    def __init__(self, env, agent, work_dir: str = "output/hirl",
                 config: Optional[Dict] = None):
        """
        å‚æ•°:
            env: ç¯å¢ƒå®ä¾‹
            agent: æ™ºèƒ½ä½“å®ä¾‹
            work_dir: å·¥ä½œç›®å½•
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å« phase1, phase2, phase3 çš„é…ç½®
        """
        self.env = env
        self.agent = agent

        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(parents=True, exist_ok=True)

        # é»˜è®¤é…ç½®
        default_config = {
            "phase1": {
                "episodes": 3000,
                "save_every": 500,
                "max_dataset_size": 100000
            },
            "phase2": {
                "epochs": 10,
                "batch_size": 128,
                "save_every_epoch": 2,
                "validation_split": 0.1
            },
            "phase3": {
                "episodes": 2000,
                "start_epsilon": 0.2,
                "max_steps_per_episode": 120,
                "eval_every": 100,
                "eval_episodes": 20,
                "save_every": 100,
                # ğŸ”¥ æ–°å¢ï¼šæ•°æ®ç›®å½•é…ç½®
                "data_dir": None,  # å¦‚æœä¸º Noneï¼Œä½¿ç”¨ç¯å¢ƒçš„é»˜è®¤ç›®å½•
                "use_phase3_data": True  # æ˜¯å¦ä½¿ç”¨ç‹¬ç«‹çš„ phase3 æ•°æ®
            }
        }

        # åˆå¹¶ç”¨æˆ·é…ç½®
        if config is None:
            self.cfg = default_config
        else:
            self.cfg = {}
            for phase in ["phase1", "phase2", "phase3"]:
                self.cfg[phase] = {**default_config[phase], **config.get(phase, {})}

    def run_three_phase(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹

        è¿”å›:
            åŒ…å«è®­ç»ƒç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info("=" * 70)
        logger.info(" " * 20 + "HIRL Three-Phase Training")
        logger.info("=" * 70)

        start_time = time.time()
        results = {}

        try:
            # ==================== Phase 1 ====================
            p1_dir = self.work_dir / "phase1"
            phase1 = Phase1ExpertCollector(self.env, str(p1_dir), self.cfg["phase1"])
            expert_data = phase1.run()

            if not expert_data:
                raise ValueError("Phase 1 failed: No expert data collected!")

            results["phase1"] = {
                "num_transitions": len(expert_data),
                "stats": phase1.stats
            }

            # ==================== Phase 2 ====================
            p2_dir = self.work_dir / "phase2"
            phase2 = Phase2ILTrainer(self.agent, expert_data, str(p2_dir), self.cfg["phase2"])
            phase2.run()

            results["phase2"] = {
                "history": phase2.history
            }

            # ==================== Phase 3 - FIXED ====================
            logger.info("\n" + "=" * 70)
            logger.info("Preparing Phase 3 with correct data loading...")
            logger.info("=" * 70)

            # ğŸ”¥ ä¿®å¤ï¼šåˆ›å»ºå¸¦æ­£ç¡®æ•°æ®çš„æ–°ç¯å¢ƒ
            env_phase3 = self._prepare_phase3_environment()

            if env_phase3 is None:
                logger.error("âŒ Failed to prepare Phase 3 environment!")
                raise ValueError("Phase 3 environment preparation failed")

            # ä½¿ç”¨æ–°ç¯å¢ƒåˆ›å»º Phase 3 trainer
            p3_dir = self.work_dir / "phase3"
            phase3 = Phase3RLTrainer(env_phase3, self.agent, str(p3_dir), self.cfg["phase3"])
            phase3_stats = phase3.run()

            results["phase3"] = phase3_stats

        except Exception as e:
            logger.error(f"Training failed: {e}")
            import traceback
            traceback.print_exc()
            raise

        finally:
            # ğŸ”¥ æ¢å¤åŸå§‹æ•°æ®æ–‡ä»¶
            self._restore_original_data()

        # æ€»ç»“
        elapsed = time.time() - start_time

        logger.info("=" * 70)
        logger.info(" " * 20 + "TRAINING COMPLETE")
        logger.info("=" * 70)
        logger.info(f"Total time: {elapsed / 60:.2f} minutes")
        logger.info(f"Phase 1 transitions: {results['phase1']['num_transitions']}")
        logger.info(f"Phase 3 avg reward: {results['phase3']['avg_reward']:.2f}")
        logger.info(f"Phase 3 best eval: {results['phase3']['best_eval_reward']:.2f}")
        logger.info(f"Final acceptance: {results['phase3']['final_acceptance_rate']:.2f}%")
        logger.info(f"Output directory: {self.work_dir}")
        logger.info("=" * 70)

        # ä¿å­˜å®Œæ•´ç»“æœ
        self._save_results(results)

        return results

    def _prepare_phase3_environment(self):
        """
        ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸º Phase 3 å‡†å¤‡æ­£ç¡®çš„ç¯å¢ƒå’Œæ•°æ®

        è¿”å›:
            æ–°åˆ›å»ºçš„ç¯å¢ƒå®ä¾‹ï¼Œå·²åŠ è½½ phase3 æ•°æ®
        """
        # è·å–æ•°æ®ç›®å½•
        data_dir = self.cfg["phase3"].get("data_dir")

        if data_dir is None:
            # å°è¯•ä»ç¯å¢ƒè·å–
            if hasattr(self.env, 'input_dir'):
                data_dir = Path(self.env.input_dir)
            elif hasattr(self.env, 'data_dir'):
                data_dir = Path(self.env.data_dir)
            else:
                logger.error("âŒ Cannot determine data directory!")
                return None
        else:
            data_dir = Path(data_dir)

        logger.info(f"ğŸ“ Data directory: {data_dir}")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ phase3 ç‹¬ç«‹æ•°æ®
        if not self.cfg["phase3"].get("use_phase3_data", True):
            logger.info("âš ï¸  Using same environment for Phase 3 (no data switching)")
            return self.env

        # æ£€æŸ¥ phase3 æ•°æ®æ–‡ä»¶
        phase3_req = data_dir / "phase3_requests.pkl"
        phase3_evt = data_dir / "phase3_events.pkl"

        if not phase3_req.exists() or not phase3_evt.exists():
            logger.warning(f"âš ï¸  Phase 3 data files not found:")
            logger.warning(f"   Expected: {phase3_req}")
            logger.warning(f"   Expected: {phase3_evt}")
            logger.warning(f"   Using original environment without data switching")
            return self.env

        logger.info(f"âœ… Phase 3 data files found")

        # ğŸ”¥ å¤‡ä»½å¹¶æ›¿æ¢æ•°æ®æ–‡ä»¶
        default_req = data_dir / "sfc_requests.pkl"
        default_evt = data_dir / "sfc_events.pkl"

        self.backup_req = data_dir / "_backup_sfc_requests.pkl"
        self.backup_evt = data_dir / "_backup_sfc_events.pkl"

        # å¤‡ä»½ç°æœ‰æ–‡ä»¶
        if default_req.exists():
            shutil.move(str(default_req), str(self.backup_req))
            logger.info(f"   Backed up: sfc_requests.pkl")

        if default_evt.exists():
            shutil.move(str(default_evt), str(self.backup_evt))
            logger.info(f"   Backed up: sfc_events.pkl")

        # å¤åˆ¶ phase3 æ•°æ®
        shutil.copy(str(phase3_req), str(default_req))
        shutil.copy(str(phase3_evt), str(default_evt))

        logger.info(f"âœ… Phase 3 data activated!")

        # ğŸ”¥ åˆ›å»ºæ–°çš„ç¯å¢ƒå®ä¾‹
        try:
            # è·å–ç¯å¢ƒç±»
            env_class = type(self.env)

            # å°è¯•è·å–ç¯å¢ƒåˆå§‹åŒ–å‚æ•°
            env_kwargs = {}

            # å¸¸è§çš„ç¯å¢ƒå‚æ•°
            if hasattr(self.env, 'input_dir'):
                env_kwargs['input_dir'] = str(data_dir)
            if hasattr(self.env, 'topo'):
                env_kwargs['topo'] = self.env.topo
            if hasattr(self.env, 'dc_nodes'):
                env_kwargs['dc_nodes'] = self.env.dc_nodes
            if hasattr(self.env, 'capacities'):
                env_kwargs['capacities'] = self.env.capacities
            if hasattr(self.env, 'use_gnn'):
                env_kwargs['use_gnn'] = self.env.use_gnn

            logger.info(f"ğŸ”§ Creating new environment instance...")
            logger.info(f"   Class: {env_class.__name__}")
            logger.info(f"   Kwargs: {list(env_kwargs.keys())}")

            # åˆ›å»ºæ–°ç¯å¢ƒ
            new_env = env_class(**env_kwargs)

            logger.info(f"âœ… New environment created")
            logger.info(f"   Total requests: {new_env.T if hasattr(new_env, 'T') else 'N/A'}")

            # éªŒè¯æ•°æ®åŠ è½½
            logger.info(f"ğŸ§ª Validating data loading...")

            if hasattr(new_env, 'reset_request'):
                test_req, test_state = new_env.reset_request()

                if test_req is None:
                    logger.error(f"âŒ CRITICAL: reset_request() returned None!")
                    return None

                logger.info(f"âœ… Data loading validated")
                logger.info(f"   Sample request: {len(test_req.get('dest', []))} destinations")

                # é‡ç½®ç¯å¢ƒ
                if hasattr(new_env, 'reset_all'):
                    new_env.reset_all()

            return new_env

        except Exception as e:
            logger.error(f"âŒ Failed to create new environment: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _restore_original_data(self):
        """
        ğŸ”¥ æ¢å¤åŸå§‹æ•°æ®æ–‡ä»¶
        """
        if not hasattr(self, 'backup_req') or not hasattr(self, 'backup_evt'):
            return

        logger.info(f"\nğŸ”§ Restoring original data files...")

        # è·å–æ•°æ®ç›®å½•
        data_dir = self.cfg["phase3"].get("data_dir")
        if data_dir is None:
            if hasattr(self.env, 'input_dir'):
                data_dir = Path(self.env.input_dir)
            else:
                return
        else:
            data_dir = Path(data_dir)

        default_req = data_dir / "sfc_requests.pkl"
        default_evt = data_dir / "sfc_events.pkl"

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if default_req.exists():
            default_req.unlink()
        if default_evt.exists():
            default_evt.unlink()

        # æ¢å¤å¤‡ä»½
        if self.backup_req.exists():
            shutil.move(str(self.backup_req), str(default_req))
            logger.info(f"   Restored: sfc_requests.pkl")

        if self.backup_evt.exists():
            shutil.move(str(self.backup_evt), str(default_evt))
            logger.info(f"   Restored: sfc_events.pkl")

        logger.info(f"âœ… Original data restored")

    def _save_results(self, results: Dict):
        """ä¿å­˜å®Œæ•´çš„è®­ç»ƒç»“æœ"""
        results_path = self.work_dir / "training_results.pkl"
        try:
            with open(results_path, "wb") as f:
                pickle.dump(results, f)
            logger.info(f"Complete results saved: {results_path}")
        except Exception as e:
            logger.error(f"Failed to save results: {e}")


# =====================================================================
#   ä½¿ç”¨ç¤ºä¾‹
# =====================================================================

if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹ - éœ€è¦å®ç°å…·ä½“çš„ env å’Œ agent
    """

    # ç¤ºä¾‹é…ç½®
    config = {
        "phase1": {
            "episodes": 1000,  # è¾ƒå°çš„æ•°é‡ç”¨äºæµ‹è¯•
        },
        "phase2": {
            "epochs": 5,
            "batch_size": 64,
        },
        "phase3": {
            "episodes": 2000,
            "start_epsilon": 0.2,
            # ğŸ”¥ Phase 3 æ•°æ®é…ç½®
            "data_dir": "data/Abilene",  # æ•°æ®ç›®å½•
            "use_phase3_data": True,  # ä½¿ç”¨ç‹¬ç«‹çš„ phase3 æ•°æ®
        }
    }

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    # from env.sfc_env_hirl_gnn import SFC_HIRL_Env_GNN
    # from agent.hirl_agent import HIRLAgent

    # env = SFC_HIRL_Env_GNN(input_dir="data/Abilene", ...)
    # agent = HIRLAgent(...)

    # trainer = HIRLThreePhaseTrainer(
    #     env=env,
    #     agent=agent,
    #     work_dir="output/run_001",
    #     config=config
    # )

    # è¿è¡Œè®­ç»ƒ
    # results = trainer.run_three_phase()

    print("=" * 70)
    print("âœ… Fixed Training Script Loaded Successfully!")
    print("=" * 70)
    print("\nğŸ”¥ Key Fixes:")
    print("  1. Phase 3 automatically switches to phase3_requests.pkl")
    print("  2. Creates new environment instance with correct data")
    print("  3. Validates data loading before training")
    print("  4. Auto-restores original data files after training")
    print("\nğŸ“ Usage:")
    print("  Set config['phase3']['data_dir'] to your data directory")
    print("  Set config['phase3']['use_phase3_data'] = True")
    print("\nReady to use!")