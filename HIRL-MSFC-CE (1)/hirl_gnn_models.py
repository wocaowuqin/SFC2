#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
hirl_gnn_models.py - å¤šæ’­æ„ŸçŸ¥ä¸‰å±‚HRLæ¨¡å‹ (ç”Ÿäº§çº§å®Œæ•´ç‰ˆ)

åˆ›æ–°ç‚¹æ•´åˆ:
1. å¤šæ’­æ„ŸçŸ¥GAT (Multicast-Aware GAT)
   - Set Transformer å¤šç›®æ ‡é›†åˆç¼–ç 
   - è¯·æ±‚è°ƒåˆ¶çš„å›¾æ³¨æ„åŠ›æœºåˆ¶
   - VNFå…±äº«æ½œåŠ›é¢„æµ‹

2. ä¸‰å±‚åˆ†å±‚å¼ºåŒ–å­¦ä¹  (3-Level HRL)
   - High-Level: VNFå…±äº«ç­–ç•¥é€‰æ‹©
   - Mid-Level: ç›®æ ‡èŠ‚ç‚¹é€‰æ‹© (å­æ ‘æ‰©å±•)
   - Low-Level: è·¯å¾„ä¸VNFæ”¾ç½®æ‰§è¡Œ

3. æ¨¡å—åŒ–è®¾è®¡
   - ç‹¬ç«‹å¯æµ‹è¯•çš„ç»„ä»¶
   - çµæ´»çš„é…ç½®æ¥å£
   - å®Œå–„çš„é”™è¯¯å¤„ç†

ç†è®ºåŸºç¡€:
- Set Transformer: Zaheer et al., "Deep Sets", NeurIPS 2017
- Graph Attention: VeliÄkoviÄ‡ et al., "GAT", ICLR 2018
- Hierarchical RL: Kulkarni et al., "h-DQN", NeurIPS 2016

ä½œè€…: Your Name
æ—¥æœŸ: 2025-12-13
ç‰ˆæœ¬: 5.0 (Multicast Edition)
"""

from __future__ import annotations
import logging
import hashlib
import threading
import time
from typing import Optional, List, Union, Tuple, Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint
from torch_geometric.nn import GATConv, global_mean_pool, MessagePassing
from torch_geometric.utils import softmax

# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
logger = logging.getLogger(__name__)


# ============================================================================
# 1. åŸºç¡€ç»„ä»¶: Set Transformer (å¤šç›®æ ‡é›†åˆç¼–ç )
# ============================================================================

class SetTransformer(nn.Module):
    """
    å¤šç›®æ ‡é›†åˆç¼–ç å™¨ - æ’åˆ—ä¸å˜æ€§ (Permutation Invariant)

    ç†è®ºåŸºç¡€: Deep Sets (Zaheer et al., NeurIPS 2017)

    å…³é”®å…¬å¼:
        f({x1, ..., xn}) = Ï(Î£ Ï†(xi))

    å…¶ä¸­:
        - Ï†: å…ƒç´ çº§ç¼–ç å‡½æ•° (Element-wise Encoder)
        - Î£: æ±‚å’Œèšåˆ (ä¿è¯æ’åˆ—ä¸å˜æ€§)
        - Ï: èšåˆåçš„æ˜ å°„å‡½æ•° (Aggregation Function)

    åº”ç”¨åœºæ™¯:
        å¤šæ’­SFCä¸­,ç›®æ ‡èŠ‚ç‚¹é›†åˆ {d1, d2, ..., dk} çš„é¡ºåºä¸åº”å½±å“å†³ç­–
        ä¾‹å¦‚: {d1, d2, d3} å’Œ {d3, d1, d2} åº”äº§ç”Ÿç›¸åŒçš„ç¼–ç 

    åˆ›æ–°ç‚¹:
        1. ä½¿ç”¨Multi-head Self-Attentionæ•è·ç›®æ ‡é—´çš„ç›¸å…³æ€§
        2. ä¾‹å¦‚: ä¸¤ä¸ªè·ç¦»å¾ˆè¿‘çš„ç›®æ ‡å¯èƒ½å…±äº«VNFå®ä¾‹
    """

    def __init__(self, input_dim: int, hidden_dim: int,
                 num_heads: int = 4, dropout: float = 0.1):
        """
        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦ (èŠ‚ç‚¹ç‰¹å¾ç»´åº¦)
            hidden_dim: éšè—å±‚ç»´åº¦
            num_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            dropout: Dropoutæ¯”ä¾‹
        """
        super().__init__()

        if hidden_dim % num_heads != 0:
            raise ValueError(f"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads})")

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads

        # Ï†: å…ƒç´ çº§ç¼–ç ç½‘ç»œ
        self.element_encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim)
        )

        # Multi-head Self-Attention (æ•è·ç›®æ ‡é—´å…³ç³»)
        # ä¾‹å¦‚: ç›®æ ‡d1å’Œd2è·ç¦»å¾ˆè¿‘ â†’ é«˜æ³¨æ„åŠ›æƒé‡ â†’ å¯èƒ½å…±äº«VNF
        self.self_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        self.attn_norm = nn.LayerNorm(hidden_dim)

        # Ï: èšåˆå‡½æ•° (Sum Pooling + MLP)
        self.aggregator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, dest_features: torch.Tensor,
                return_attention: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­

        Args:
            dest_features: [num_dests, input_dim] ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾å¼ é‡
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡ (ç”¨äºå¯è§†åŒ–åˆ†æ)

        Returns:
            aggregated: [hidden_dim] èšåˆåçš„é›†åˆè¡¨ç¤º
            (optional) attention_weights: [num_heads, num_dests, num_dests]
        """
        if dest_features.size(0) == 0:
            # ç©ºé›†åˆæƒ…å†µ: è¿”å›é›¶å‘é‡
            return torch.zeros(self.hidden_dim, device=dest_features.device, dtype=dest_features.dtype)

        # 1. å…ƒç´ çº§ç¼–ç 
        encoded = self.element_encoder(dest_features)  # [num_dests, hidden_dim]

        # 2. Self-Attention (æ•è·ç›®æ ‡é—´ç›¸å…³æ€§)
        # Q, K, V éƒ½æ¥è‡ªåŒä¸€ä¸ªè¾“å…¥ (Self-Attention)
        attn_input = encoded.unsqueeze(0)  # [1, num_dests, hidden_dim]

        attn_out, attn_weights = self.self_attn(
            attn_input, attn_input, attn_input,
            need_weights=return_attention
        )
        attn_out = attn_out.squeeze(0)  # [num_dests, hidden_dim]

        # æ®‹å·®è¿æ¥ + LayerNorm
        attn_out = self.attn_norm(encoded + attn_out)

        # 3. æ’åˆ—ä¸å˜èšåˆ (Sum Pooling)
        # ä¸ºä»€ä¹ˆç”¨Sumè€Œä¸æ˜¯Mean?
        # Sumä¿ç•™äº†é›†åˆå¤§å°ä¿¡æ¯ (å¯¹äºå¤šæ’­å¾ˆé‡è¦)
        aggregated = torch.sum(attn_out, dim=0)  # [hidden_dim]

        # 4. æœ€ç»ˆæ˜ å°„
        output = self.aggregator(aggregated)  # [hidden_dim]

        if return_attention:
            return output, attn_weights
        return output


# ============================================================================
# 2. åŸºç¡€ç»„ä»¶: è¯·æ±‚è°ƒåˆ¶çš„å›¾æ³¨æ„åŠ› (Request-Modulated GAT)
# ============================================================================

class RequestModulatedGATConv(MessagePassing):
    """
    è¯·æ±‚è°ƒåˆ¶çš„å›¾æ³¨æ„åŠ›å·ç§¯å±‚

    åˆ›æ–°ç‚¹ (vs æ ‡å‡†GAT):
        æ ‡å‡†GAT:  Î±_ij = attention(h_i, h_j)
        æœ¬æ–¹æ³•:   Î±_ij = attention(h_i, h_j, request_vec)  â† è¯·æ±‚è°ƒåˆ¶

    ç›´è§‰ç†è§£:
        ä¸åŒçš„è¯·æ±‚æœ‰ä¸åŒçš„éœ€æ±‚ (å¸¦å®½ã€VNFç±»å‹ç­‰)
        æ³¨æ„åŠ›æƒé‡åº”è¯¥æ ¹æ®è¯·æ±‚ç‰¹å¾åŠ¨æ€è°ƒæ•´

    ä¾‹å­:
        - é«˜å¸¦å®½è¯·æ±‚ â†’ æ›´å…³æ³¨é«˜å®¹é‡é“¾è·¯
        - å¤šVNFè¯·æ±‚ â†’ æ›´å…³æ³¨DCèŠ‚ç‚¹
    """

    def __init__(self, in_channels: int, out_channels: int,
                 request_dim: int, heads: int = 4,
                 concat: bool = True, dropout: float = 0.1,
                 edge_dim: Optional[int] = None):
        """
        Args:
            in_channels: è¾“å…¥èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            out_channels: è¾“å‡ºèŠ‚ç‚¹ç‰¹å¾ç»´åº¦ (æ¯ä¸ªå¤´)
            request_dim: è¯·æ±‚å‘é‡ç»´åº¦
            heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            concat: æ˜¯å¦æ‹¼æ¥å¤šå¤´ç»“æœ (True) æˆ–å¹³å‡ (False)
            dropout: Dropoutæ¯”ä¾‹
            edge_dim: è¾¹ç‰¹å¾ç»´åº¦ (å¯é€‰)
        """
        super().__init__(aggr='add', node_dim=0)

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.request_dim = request_dim
        self.heads = heads
        self.concat = concat
        self.dropout = dropout
        self.edge_dim = edge_dim

        # èŠ‚ç‚¹ç‰¹å¾çº¿æ€§å˜æ¢ (ä¸ºæ¯ä¸ªå¤´å•ç‹¬å˜æ¢)
        self.lin_src = nn.Linear(in_channels, heads * out_channels, bias=False)
        self.lin_dst = nn.Linear(in_channels, heads * out_channels, bias=False)

        # ğŸ”¥ åˆ›æ–°: è¯·æ±‚è°ƒåˆ¶ç½‘ç»œ
        self.request_modulator = nn.Sequential(
            nn.Linear(request_dim, heads * out_channels),
            nn.LayerNorm(heads * out_channels),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # è¾¹ç‰¹å¾å˜æ¢ (å¦‚æœæä¾›)
        if edge_dim is not None:
            self.lin_edge = nn.Linear(edge_dim, heads * out_channels, bias=False)
        else:
            self.lin_edge = None

        # æ³¨æ„åŠ›ç³»æ•°è®¡ç®—
        # è¾“å…¥ç»´åº¦: out_channels * 3 (src + dst + request)
        # å¦‚æœæœ‰è¾¹ç‰¹å¾: out_channels * 4
        attn_input_dim = out_channels * 3
        if edge_dim is not None:
            attn_input_dim += out_channels

        self.att = nn.Parameter(torch.Tensor(1, heads, attn_input_dim))

        # Bias
        if concat:
            self.bias = nn.Parameter(torch.Tensor(heads * out_channels))
        else:
            self.bias = nn.Parameter(torch.Tensor(out_channels))

        self.reset_parameters()

    def reset_parameters(self):
        """å‚æ•°åˆå§‹åŒ–"""
        nn.init.xavier_uniform_(self.lin_src.weight)
        nn.init.xavier_uniform_(self.lin_dst.weight)
        if self.lin_edge is not None:
            nn.init.xavier_uniform_(self.lin_edge.weight)
        nn.init.xavier_uniform_(self.att)
        nn.init.zeros_(self.bias)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,
                request_vec: torch.Tensor,
                edge_attr: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [num_nodes, in_channels] èŠ‚ç‚¹ç‰¹å¾
            edge_index: [2, num_edges] è¾¹ç´¢å¼•
            request_vec: [request_dim] è¯·æ±‚å‘é‡
            edge_attr: [num_edges, edge_dim] è¾¹ç‰¹å¾ (å¯é€‰)

        Returns:
            out: [num_nodes, out_channels * heads] æˆ– [num_nodes, out_channels]
        """
        H, C = self.heads, self.out_channels

        # 1. èŠ‚ç‚¹ç‰¹å¾å˜æ¢
        x_src = self.lin_src(x).view(-1, H, C)  # [num_nodes, heads, out_channels]
        x_dst = self.lin_dst(x).view(-1, H, C)

        # 2. ğŸ”¥ è¯·æ±‚è°ƒåˆ¶
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆè¯·æ±‚ç›¸å…³çš„ç‰¹å¾
        request_feat = self.request_modulator(request_vec)  # [heads * out_channels]
        request_feat = request_feat.view(1, H, C)  # [1, heads, out_channels]
        request_feat = request_feat.expand(x.size(0), -1, -1)  # [num_nodes, heads, out_channels]

        # 3. è¾¹ç‰¹å¾å˜æ¢ (å¦‚æœæœ‰)
        edge_feat = None
        if edge_attr is not None and self.lin_edge is not None:
            edge_feat = self.lin_edge(edge_attr).view(-1, H, C)

        # 4. æ¶ˆæ¯ä¼ é€’
        out = self.propagate(
            edge_index, x=(x_src, x_dst),
            request_feat=request_feat,
            edge_feat=edge_feat
        )

        # 5. å¤šå¤´æ‹¼æ¥æˆ–å¹³å‡
        if self.concat:
            out = out.view(-1, H * C)
        else:
            out = out.mean(dim=1)

        # 6. Bias
        out = out + self.bias

        return out

    def message(self, x_i: torch.Tensor, x_j: torch.Tensor,
                request_feat_i: torch.Tensor, request_feat_j: torch.Tensor,
                edge_feat: Optional[torch.Tensor],
                index: torch.Tensor, ptr: Optional[torch.Tensor],
                size_i: Optional[int]) -> torch.Tensor:
        """
        è®¡ç®—æ¶ˆæ¯ (Message Function)

        Args:
            x_i: ç›®æ ‡èŠ‚ç‚¹ç‰¹å¾ [num_edges, heads, out_channels]
            x_j: æºèŠ‚ç‚¹ç‰¹å¾
            request_feat_i, request_feat_j: è¯·æ±‚ç‰¹å¾
            edge_feat: è¾¹ç‰¹å¾
            index: è¾¹ç´¢å¼•
            ptr, size_i: PyGå†…éƒ¨å‚æ•°

        Returns:
            messages: [num_edges, heads, out_channels]
        """
        # æ‹¼æ¥ç‰¹å¾: [src, dst, request]
        combined = torch.cat([x_j, x_i, request_feat_j], dim=-1)

        # å¦‚æœæœ‰è¾¹ç‰¹å¾,ä¹Ÿæ‹¼æ¥ä¸Š
        if edge_feat is not None:
            combined = torch.cat([combined, edge_feat], dim=-1)

        # è®¡ç®—æ³¨æ„åŠ›ç³»æ•°
        # alpha: [num_edges, heads]
        alpha = (combined * self.att).sum(dim=-1)
        alpha = F.leaky_relu(alpha, negative_slope=0.2)

        # Softmaxå½’ä¸€åŒ– (æŒ‰ç›®æ ‡èŠ‚ç‚¹åˆ†ç»„)
        alpha = softmax(alpha, index, ptr, size_i)

        # Dropout
        alpha = F.dropout(alpha, p=self.dropout, training=self.training)

        # åŠ æƒæ¶ˆæ¯
        messages = x_j * alpha.unsqueeze(-1)

        return messages


# ============================================================================
# 3. åŸºç¡€ç»„ä»¶: VNFå…±äº«æ½œåŠ›é¢„æµ‹å™¨
# ============================================================================

class VNFSharingPredictor(nn.Module):
    """
    VNFå…±äº«æ½œåŠ›é¢„æµ‹å™¨

    åŠŸèƒ½:
        é¢„æµ‹æ¯ä¸ªèŠ‚ç‚¹ä½œä¸ºVNFå…±äº«ç‚¹çš„æ½œåŠ›åˆ†æ•°

    è¾“å…¥:
        - èŠ‚ç‚¹ç‰¹å¾ (ä½ç½®ã€èµ„æºã€å½“å‰è´Ÿè½½ç­‰)
        - ç›®æ ‡é›†åˆç‰¹å¾ (ç›®æ ‡èŠ‚ç‚¹çš„èšåˆè¡¨ç¤º)

    è¾“å‡º:
        - å…±äº«æ½œåŠ›åˆ†æ•° âˆˆ [0, 1]

    ç›´è§‰:
        å¥½çš„VNFå…±äº«ç‚¹åº”è¯¥:
        1. é è¿‘å¤šä¸ªç›®æ ‡èŠ‚ç‚¹ (é™ä½ä¼ è¾“å»¶è¿Ÿ)
        2. æœ‰å……è¶³çš„èµ„æº (CPU, Memory)
        3. åœ¨å¤šæ¡è·¯å¾„çš„äº¤æ±‡å¤„ (æé«˜å¤ç”¨ç‡)
    """

    def __init__(self, node_dim: int, dest_set_dim: int,
                 hidden_dim: int = 128, dropout: float = 0.2):
        """
        Args:
            node_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            dest_set_dim: ç›®æ ‡é›†åˆç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            dropout: Dropoutæ¯”ä¾‹
        """
        super().__init__()

        self.node_dim = node_dim
        self.dest_set_dim = dest_set_dim

        # é¢„æµ‹ç½‘ç»œ
        self.predictor = nn.Sequential(
            nn.Linear(node_dim + dest_set_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),

            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # è¾“å‡º [0, 1]
        )

    def forward(self, node_features: torch.Tensor,
                dest_set_feature: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­

        Args:
            node_features: [num_nodes, node_dim] èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
            dest_set_feature: [dest_set_dim] ç›®æ ‡é›†åˆç‰¹å¾å‘é‡

        Returns:
            scores: [num_nodes] æ¯ä¸ªèŠ‚ç‚¹çš„å…±äº«æ½œåŠ›åˆ†æ•°
        """
        num_nodes = node_features.size(0)

        # æ‰©å±•ç›®æ ‡é›†åˆç‰¹å¾ä»¥åŒ¹é…èŠ‚ç‚¹æ•°
        dest_expanded = dest_set_feature.unsqueeze(0).expand(
            num_nodes, -1
        )  # [num_nodes, dest_set_dim]

        # æ‹¼æ¥èŠ‚ç‚¹ç‰¹å¾å’Œç›®æ ‡é›†åˆç‰¹å¾
        combined = torch.cat([node_features, dest_expanded], dim=-1)

        # é¢„æµ‹å…±äº«æ½œåŠ›
        scores = self.predictor(combined).squeeze(-1)  # [num_nodes]

        return scores


# ============================================================================
# 4. æ ¸å¿ƒæ¨¡å‹: å¤šæ’­æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨
# ============================================================================

class MulticastAwareGNN(nn.Module):
    """
    å¤šæ’­æ„ŸçŸ¥å›¾ç¥ç»ç½‘ç»œç¼–ç å™¨

    æ•´åˆæ‰€æœ‰åˆ›æ–°ç»„ä»¶:
    1. Set Transformer (å¤šç›®æ ‡é›†åˆç¼–ç )
    2. Request-Modulated GAT (è¯·æ±‚è°ƒåˆ¶æ³¨æ„åŠ›)
    3. VNF Sharing Predictor (å…±äº«æ½œåŠ›é¢„æµ‹)

    è¾“å…¥:
        - å›¾ç»“æ„: (node_features, edge_index, edge_attr)
        - è¯·æ±‚ä¿¡æ¯: request_vec
        - ç›®æ ‡èŠ‚ç‚¹: dest_indices

    è¾“å‡º:
        - node_embeddings: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
        - dest_set_embedding: ç›®æ ‡é›†åˆåµŒå…¥ [hidden_dim]
        - sharing_scores: VNFå…±äº«æ½œåŠ› [num_nodes]
        - graph_embedding: å›¾çº§åµŒå…¥ [hidden_dim]
    """

    def __init__(self, node_feat_dim: int, edge_feat_dim: int,
                 request_dim: int, hidden_dim: int = 128,
                 num_gat_layers: int = 3, num_heads: int = 4,
                 dropout: float = 0.1, use_checkpoint: bool = False,
                 normalization: str = 'layer'):
        """
        Args:
            node_feat_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            edge_feat_dim: è¾¹ç‰¹å¾ç»´åº¦
            request_dim: è¯·æ±‚å‘é‡ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_gat_layers: GATå±‚æ•°
            num_heads: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            dropout: Dropoutæ¯”ä¾‹
            use_checkpoint: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (èŠ‚çœæ˜¾å­˜)
            normalization: å½’ä¸€åŒ–ç±»å‹ ('layer' or 'batch')
        """
        super().__init__()

        if hidden_dim % num_heads != 0:
            logger.warning(f"hidden_dim ({hidden_dim}) not divisible by num_heads ({num_heads}), adjusting...")
            num_heads = min(num_heads, hidden_dim)

        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.use_checkpoint = use_checkpoint
        self.normalization = normalization

        # ===== ç»„ä»¶1: åˆå§‹ç‰¹å¾ç¼–ç  =====
        self.node_embedding = nn.Linear(node_feat_dim, hidden_dim)
        self.edge_embedding = nn.Linear(edge_feat_dim, hidden_dim)

        # ===== ç»„ä»¶2: Set Transformer (å¤šç›®æ ‡é›†åˆç¼–ç ) =====
        self.dest_set_encoder = SetTransformer(
            input_dim=hidden_dim,
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            dropout=dropout
        )

        # ===== ç»„ä»¶3: Request-Modulated GAT å±‚ =====
        self.gat_layers = nn.ModuleList()
        self.projections = nn.ModuleList()
        self.norm_layers = nn.ModuleList()

        for layer_idx in range(num_gat_layers):
            # Request-Modulated GAT å±‚
            gat = RequestModulatedGATConv(
                in_channels=hidden_dim,
                out_channels=self.head_dim,
                request_dim=request_dim,
                heads=num_heads,
                concat=True,
                dropout=dropout,
                edge_dim=hidden_dim
            )
            self.gat_layers.append(gat)

            # æŠ•å½±å±‚ (å¦‚æœéœ€è¦)
            actual_output_dim = self.head_dim * num_heads
            if actual_output_dim != hidden_dim:
                self.projections.append(nn.Linear(actual_output_dim, hidden_dim))
            else:
                self.projections.append(nn.Identity())

            # å½’ä¸€åŒ–å±‚
            if normalization == 'layer':
                self.norm_layers.append(nn.LayerNorm(hidden_dim))
            elif normalization == 'batch':
                self.norm_layers.append(nn.BatchNorm1d(hidden_dim))
            else:
                self.norm_layers.append(nn.Identity())

        # Dropout
        self.dropout = nn.Dropout(dropout)

        # ===== ç»„ä»¶4: VNFå…±äº«æ½œåŠ›é¢„æµ‹å™¨ =====
        self.sharing_predictor = VNFSharingPredictor(
            node_dim=hidden_dim,
            dest_set_dim=hidden_dim,
            hidden_dim=hidden_dim,
            dropout=dropout
        )

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor,
                edge_attr: Optional[torch.Tensor],
                request_vec: torch.Tensor,
                dest_indices: Optional[List[int]] = None,
                batch: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å®Œæ•´å‰å‘ä¼ æ’­

        Args:
            x: [num_nodes, node_feat_dim] èŠ‚ç‚¹ç‰¹å¾
            edge_index: [2, num_edges] è¾¹ç´¢å¼•
            edge_attr: [num_edges, edge_feat_dim] è¾¹ç‰¹å¾
            request_vec: [request_dim] è¯·æ±‚å‘é‡
            dest_indices: List[int] ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨
            batch: [num_nodes] batchç´¢å¼• (å¯é€‰,ç”¨äºæ‰¹å¤„ç†)

        Returns:
            node_embeddings: [num_nodes, hidden_dim] èŠ‚ç‚¹åµŒå…¥
            dest_set_embedding: [hidden_dim] ç›®æ ‡é›†åˆåµŒå…¥
            sharing_scores: [num_nodes] VNFå…±äº«æ½œåŠ›åˆ†æ•°
            graph_embedding: [hidden_dim] å›¾çº§åµŒå…¥
        """
        device = x.device

        # å¤„ç†edge_atträ¸ºNoneçš„æƒ…å†µ
        if edge_attr is None:
            num_edges = edge_index.size(1)
            edge_attr = torch.zeros(num_edges, self.edge_embedding.in_features,
                                    device=device, dtype=x.dtype)

        # ===== Step 1: åˆå§‹ç‰¹å¾ç¼–ç  =====
        x = self.node_embedding(x)  # [num_nodes, hidden_dim]
        e = self.edge_embedding(edge_attr)  # [num_edges, hidden_dim]

        # ===== Step 2: ç›®æ ‡é›†åˆç¼–ç  (å¦‚æœæä¾›äº†ç›®æ ‡èŠ‚ç‚¹) =====
        dest_set_feat = None
        if dest_indices is not None and len(dest_indices) > 0:
            try:
                dest_features = x[dest_indices]  # [num_dests, hidden_dim]
                dest_set_feat = self.dest_set_encoder(dest_features)  # [hidden_dim]
            except Exception as e:
                logger.warning(f"Set encoding failed: {e}, using zero vector")
                dest_set_feat = torch.zeros(self.hidden_dim, device=device, dtype=x.dtype)
        else:
            # æ²¡æœ‰ç›®æ ‡èŠ‚ç‚¹æ—¶,ä½¿ç”¨é›¶å‘é‡
            dest_set_feat = torch.zeros(self.hidden_dim, device=device, dtype=x.dtype)

        # ===== Step 3: Request-Modulated GAT ä¼ æ’­ =====
        for layer_idx, (gat_layer, proj, norm) in enumerate(
                zip(self.gat_layers, self.projections, self.norm_layers)
        ):
            residual = x

            # æ¢¯åº¦æ£€æŸ¥ç‚¹ (èŠ‚çœæ˜¾å­˜)
            if self.use_checkpoint and self.training:
                x_gat = checkpoint(
                    gat_layer, x, edge_index, request_vec, e,
                    use_reentrant=True
                )
            else:
                x_gat = gat_layer(x, edge_index, request_vec, e)

            # æŠ•å½±
            x_gat = proj(x_gat)

            # æ¿€æ´» + Dropout
            x_gat = F.relu(x_gat)
            x_gat = self.dropout(x_gat)

            # æ®‹å·®è¿æ¥ + å½’ä¸€åŒ–
            x = residual + x_gat
            x = norm(x)

        # ===== Step 4: VNFå…±äº«æ½œåŠ›é¢„æµ‹ =====
        sharing_scores = self.sharing_predictor(x, dest_set_feat)

        # ===== Step 5: å›¾çº§èšåˆ =====
        if batch is None:
            graph_emb = torch.mean(x, dim=0, keepdim=True).squeeze(0)  # [hidden_dim]
        else:
            graph_emb = global_mean_pool(x, batch)  # [batch_size, hidden_dim]
            if graph_emb.dim() == 2 and graph_emb.size(0) == 1:
                graph_emb = graph_emb.squeeze(0)

        return x, dest_set_feat, sharing_scores, graph_emb


# ============================================================================
# 5. æ ¸å¿ƒæ¨¡å‹: ä¸‰å±‚åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨
# ============================================================================

class ThreeLevelHRL_Controller(nn.Module):
    """
    ä¸‰å±‚åˆ†å±‚å¼ºåŒ–å­¦ä¹ æ§åˆ¶å™¨ (å¤šæ’­SFCä¸“ç”¨)

    å±‚æ¬¡ç»“æ„:

    Level 1 (High-Level): VNFå…±äº«ç­–ç•¥é€‰æ‹©
        - è¾“å…¥: å›¾ç‰¹å¾ + è¯·æ±‚ç‰¹å¾
        - è¾“å‡º: å…±äº«ç­–ç•¥ (4ç§)
            0: å®Œå…¨ç‹¬ç«‹éƒ¨ç½² (æ¯ä¸ªç›®æ ‡ç‹¬ç«‹VNF)
            1: éƒ¨åˆ†å…±äº« (ç›¸é‚»ç›®æ ‡å…±äº«)
            2: æœ€å¤§åŒ–å…±äº« (å°½å¯èƒ½å…±äº«)
            3: è‡ªé€‚åº”å…±äº« (åŸºäºèµ„æºçŠ¶æ€åŠ¨æ€å†³ç­–)

    Level 2 (Mid-Level): ç›®æ ‡èŠ‚ç‚¹é€‰æ‹© (å­æ ‘æ‰©å±•)
        - è¾“å…¥: å›¾ç‰¹å¾ + è¯·æ±‚ç‰¹å¾ + å€™é€‰ç›®æ ‡
        - è¾“å‡º: ä¸‹ä¸€ä¸ªè¦è¿æ¥çš„ç›®æ ‡èŠ‚ç‚¹
        - ç‰¹è‰²: åˆ©ç”¨VNFå…±äº«æ½œåŠ›åˆ†æ•°è¾…åŠ©å†³ç­–

    Level 3 (Low-Level): è·¯å¾„ä¸VNFæ”¾ç½®æ‰§è¡Œ
        - è¾“å…¥: å›¾ç‰¹å¾ + è¯·æ±‚ç‰¹å¾ + ç›®æ ‡èŠ‚ç‚¹
        - è¾“å‡º: åŠ¨ä½œQå€¼ (è·¯å¾„é€‰æ‹© + VNFæ”¾ç½®)
        - ç‰¹è‰²: ä¸ç°æœ‰ç¯å¢ƒå…¼å®¹,æ”¯æŒåŠ¨ä½œmask
    """

    def __init__(self, node_feat_dim: int, edge_feat_dim: int,
                 request_dim: int, hidden_dim: int = 128,
                 num_goals: int = 10, num_actions: int = 100,
                 use_cache: bool = False, use_checkpoint: bool = False,
                 max_cache_size: int = 2000):
        """
        Args:
            node_feat_dim: èŠ‚ç‚¹ç‰¹å¾ç»´åº¦
            edge_feat_dim: è¾¹ç‰¹å¾ç»´åº¦
            request_dim: è¯·æ±‚å‘é‡ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            num_goals: æœ€å¤§ç›®æ ‡èŠ‚ç‚¹æ•° (ç”¨äºembedding)
            num_actions: åŠ¨ä½œç©ºé—´å¤§å°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (åŠ é€Ÿæ¨ç†)
            use_checkpoint: æ˜¯å¦ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            max_cache_size: ç¼“å­˜æœ€å¤§å®¹é‡
        """
        super().__init__()

        self.node_feat_dim = node_feat_dim
        self.edge_feat_dim = edge_feat_dim
        self.request_dim = request_dim
        self.hidden_dim = hidden_dim
        self.num_goals = num_goals
        self.num_actions = num_actions

        # ===== æ ¸å¿ƒ: å¤šæ’­æ„ŸçŸ¥GNNç¼–ç å™¨ =====
        self.mgat = MulticastAwareGNN(
            node_feat_dim=node_feat_dim,
            edge_feat_dim=edge_feat_dim,
            request_dim=request_dim,
            hidden_dim=hidden_dim,
            use_checkpoint=use_checkpoint
        )

        # ===== Level 1: High-Level ç­–ç•¥ç½‘ç»œ =====
        # å†³ç­–: é€‰æ‹©VNFå…±äº«ç­–ç•¥
        self.high_policy = nn.Sequential(
            nn.Linear(hidden_dim + request_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 4),  # 4ç§å…±äº«ç­–ç•¥
        )

        # ===== Level 2: Mid-Level ç›®æ ‡é€‰æ‹©ç½‘ç»œ =====
        # å†³ç­–: é€‰æ‹©ä¸‹ä¸€ä¸ªè¦è¿æ¥çš„ç›®æ ‡èŠ‚ç‚¹
        self.mid_context = nn.Sequential(
            nn.Linear(hidden_dim + request_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU()
        )

        self.mid_goal_scorer = nn.Sequential(
            nn.Linear(128 + hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

        # ===== Level 3: Low-Level æ‰§è¡Œç½‘ç»œ =====
        # å†³ç­–: é€‰æ‹©å…·ä½“è·¯å¾„å’ŒVNFæ”¾ç½®
        self.goal_embedding = nn.Embedding(num_goals, 64)

        self.low_q_net = nn.Sequential(
            nn.Linear(hidden_dim + request_dim + 64, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, num_actions)
        )

        # ===== ç¼“å­˜ç³»ç»Ÿ (å¯é€‰,ç”¨äºåŠ é€Ÿæ¨ç†) =====
        self.use_cache = use_cache
        self.max_cache_size = max_cache_size
        if use_cache:
            self.cache = {}
            self.cache_lock = threading.Lock()
            self.cache_hits = 0
            self.cache_misses = 0

        self._init_weights()

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, mean=0, std=0.01)

    def forward_high(self, x: torch.Tensor, edge_index: torch.Tensor,
                     edge_attr: Optional[torch.Tensor],
                     request_vec: torch.Tensor,
                     dest_indices: Optional[List[int]] = None) -> torch.Tensor:
        """
        High-Levelå‰å‘ä¼ æ’­: é€‰æ‹©VNFå…±äº«ç­–ç•¥

        Args:
            x, edge_index, edge_attr: å›¾æ•°æ®
            request_vec: [request_dim] è¯·æ±‚å‘é‡
            dest_indices: ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨

        Returns:
            strategy_logits: [4] å››ç§ç­–ç•¥çš„logits
                - 0: å®Œå…¨ç‹¬ç«‹éƒ¨ç½²
                - 1: éƒ¨åˆ†å…±äº«
                - 2: æœ€å¤§åŒ–å…±äº«
                - 3: è‡ªé€‚åº”å…±äº«
        """
        # GNNç¼–ç 
        _, _, _, graph_emb = self.mgat(
            x, edge_index, edge_attr, request_vec, dest_indices
        )

        # æ‹¼æ¥å›¾ç‰¹å¾å’Œè¯·æ±‚ç‰¹å¾
        combined = torch.cat([graph_emb, request_vec], dim=-1)

        # ç­–ç•¥é¢„æµ‹
        strategy_logits = self.high_policy(combined)

        return strategy_logits

    def forward_mid(self, x: torch.Tensor, edge_index: torch.Tensor,
                    edge_attr: Optional[torch.Tensor],
                    request_vec: torch.Tensor,
                    candidate_goals: List[int],
                    dest_indices: Optional[List[int]] = None) -> torch.Tensor:
        """
        Mid-Levelå‰å‘ä¼ æ’­: é€‰æ‹©ä¸‹ä¸€ä¸ªç›®æ ‡èŠ‚ç‚¹

        ç‰¹è‰²: åˆ©ç”¨VNFå…±äº«æ½œåŠ›åˆ†æ•°è¾…åŠ©å†³ç­–

        Args:
            x, edge_index, edge_attr: å›¾æ•°æ®
            request_vec: [request_dim] è¯·æ±‚å‘é‡
            candidate_goals: å€™é€‰ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨
            dest_indices: æ‰€æœ‰ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨

        Returns:
            scores: [len(candidate_goals)] æ¯ä¸ªå€™é€‰ç›®æ ‡çš„åˆ†æ•°
        """
        if not candidate_goals:
            return torch.tensor([], device=x.device)

        # GNNç¼–ç 
        node_emb, _, sharing_scores, graph_emb = self.mgat(
            x, edge_index, edge_attr, request_vec, dest_indices
        )

        # å…¨å±€ä¸Šä¸‹æ–‡
        combined = torch.cat([graph_emb, request_vec], dim=-1)
        context = self.mid_context(combined)  # [128]

        # ä¸ºæ¯ä¸ªå€™é€‰ç›®æ ‡æ‰“åˆ†
        scores = []
        for goal_idx in candidate_goals:
            if goal_idx >= node_emb.size(0):
                # è¶Šç•Œæ£€æŸ¥
                scores.append(torch.tensor(-1e9, device=x.device))
                continue

            goal_node_emb = node_emb[goal_idx]

            # æ‹¼æ¥ä¸Šä¸‹æ–‡å’ŒèŠ‚ç‚¹ç‰¹å¾
            combined_feat = torch.cat([context, goal_node_emb], dim=-1)
            score = self.mid_goal_scorer(combined_feat)

            # ğŸ”¥ åˆ›æ–°: åŠ æƒVNFå…±äº«æ½œåŠ›
            # å…±äº«æ½œåŠ›é«˜çš„èŠ‚ç‚¹è·å¾—å¥–åŠ±
            sharing_bonus = sharing_scores[goal_idx] * 0.2
            final_score = score + sharing_bonus

            scores.append(final_score)

        return torch.cat(scores, dim=0)  # [num_candidates]

    def forward_low(self, x: torch.Tensor, edge_index: torch.Tensor,
                    edge_attr: Optional[torch.Tensor],
                    batch_vec: Optional[torch.Tensor],
                    request_vec: torch.Tensor,
                    goal_indices: torch.Tensor,
                    action_masks: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Low-Levelå‰å‘ä¼ æ’­: è®¡ç®—åŠ¨ä½œQå€¼

        Args:
            x, edge_index, edge_attr: å›¾æ•°æ®
            batch_vec: batchç´¢å¼• (é€šå¸¸ä¸ºNone,å•å›¾æƒ…å†µ)
            request_vec: [request_dim] æˆ– [batch_size, request_dim]
            goal_indices: [batch_size] ç›®æ ‡èŠ‚ç‚¹ç´¢å¼•
            action_masks: [batch_size, num_actions] åŠ¨ä½œmask (å¯é€‰)

        Returns:
            q_values: [batch_size, num_actions] åŠ¨ä½œQå€¼
        """
        # GNNç¼–ç 
        _, _, _, graph_emb = self.mgat(
            x, edge_index, edge_attr,
            request_vec if request_vec.dim() == 1 else request_vec[0],
            None, batch_vec
        )

        # å¤„ç†batchç»´åº¦
        if graph_emb.dim() == 1:
            graph_emb = graph_emb.unsqueeze(0)
        if request_vec.dim() == 1:
            request_vec = request_vec.unsqueeze(0)

        # Goal embedding
        goal_emb = self.goal_embedding(goal_indices)

        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined = torch.cat([graph_emb, request_vec, goal_emb], dim=-1)

        # Qå€¼è®¡ç®—
        q_values = self.low_q_net(combined)

        # åº”ç”¨åŠ¨ä½œmask (å¦‚æœæä¾›)
        if action_masks is not None:
            if action_masks.device != q_values.device:
                action_masks = action_masks.to(q_values.device)

            huge_neg = torch.full([], -1e9, device=q_values.device, dtype=q_values.dtype)

            # å¤„ç†ä¸åŒç±»å‹çš„mask
            if action_masks.dtype == torch.bool:
                valid_mask = action_masks
            else:
                valid_mask = action_masks > 0.5

            q_values = torch.where(valid_mask, q_values, huge_neg)

        return q_values

    def get_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½® (ç”¨äºä¿å­˜/åŠ è½½)"""
        return {
            'node_feat_dim': self.node_feat_dim,
            'edge_feat_dim': self.edge_feat_dim,
            'request_dim': self.request_dim,
            'hidden_dim': self.hidden_dim,
            'num_goals': self.num_goals,
            'num_actions': self.num_actions,
            'use_cache': self.use_cache,
            'max_cache_size': self.max_cache_size
        }

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.use_cache:
            with self.cache_lock:
                self.cache.clear()
                self.cache_hits = 0
                self.cache_misses = 0

    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        if not self.use_cache:
            return {'enabled': False}

        with self.cache_lock:
            total = self.cache_hits + self.cache_misses
            hit_rate = self.cache_hits / total if total > 0 else 0.0

            return {
                'enabled': True,
                'hits': self.cache_hits,
                'misses': self.cache_misses,
                'hit_rate': hit_rate,
                'size': len(self.cache)
            }

    # Pickleæ”¯æŒ (é˜²æ­¢åºåˆ—åŒ–é”å¯¹è±¡)
    def __getstate__(self):
        state = self.__dict__.copy()
        if 'cache_lock' in state:
            del state['cache_lock']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        if self.use_cache:
            self.cache_lock = threading.Lock()


# ============================================================================
# 6. å‘åå…¼å®¹: GNN_HRL_Controller (åˆ«å)
# ============================================================================

class GNN_HRL_Controller(ThreeLevelHRL_Controller):
    """
    å‘åå…¼å®¹çš„åˆ«å
    ä¿æŒä¸ç°æœ‰ä»£ç çš„å…¼å®¹æ€§
    """
    pass


# ============================================================================
# 7. å·¥å…·å‡½æ•°
# ============================================================================

def create_model(node_feat_dim: int, edge_feat_dim: int, request_dim: int,
                 hidden_dim: int = 128, num_goals: int = 10, num_actions: int = 100,
                 **kwargs) -> ThreeLevelHRL_Controller:
    """
    ä¾¿æ·çš„æ¨¡å‹åˆ›å»ºå‡½æ•°

    è‡ªåŠ¨å¤„ç†å‚æ•°éªŒè¯å’Œé»˜è®¤å€¼
    """
    logger.info("Creating ThreeLevelHRL_Controller...")
    logger.info(f"  Node feat dim: {node_feat_dim}")
    logger.info(f"  Edge feat dim: {edge_feat_dim}")
    logger.info(f"  Request dim: {request_dim}")
    logger.info(f"  Hidden dim: {hidden_dim}")

    model = ThreeLevelHRL_Controller(
        node_feat_dim=node_feat_dim,
        edge_feat_dim=edge_feat_dim,
        request_dim=request_dim,
        hidden_dim=hidden_dim,
        num_goals=num_goals,
        num_actions=num_actions,
        **kwargs
    )

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    logger.info(f"âœ… Model created successfully")
    logger.info(f"  Total parameters: {total_params:,}")
    logger.info(f"  Trainable parameters: {trainable_params:,}")

    return model


def model_summary(model: nn.Module) -> str:
    """
    ç”Ÿæˆæ¨¡å‹æ‘˜è¦
    """
    lines = []
    lines.append("=" * 80)
    lines.append("MODEL SUMMARY")
    lines.append("=" * 80)

    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    lines.append(f"Total Parameters:      {total_params:,}")
    lines.append(f"Trainable Parameters:  {trainable_params:,}")
    lines.append(f"Non-trainable:         {total_params - trainable_params:,}")

    # æ¨¡å—ç»Ÿè®¡
    lines.append("\nModule Breakdown:")
    for name, module in model.named_children():
        params = sum(p.numel() for p in module.parameters())
        lines.append(f"  {name:30s} {params:>15,}")

    lines.append("=" * 80)

    return "\n".join(lines)


# ============================================================================
# æµ‹è¯•ä»£ç 
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print("\n" + "=" * 80)
    print("å¤šæ’­æ„ŸçŸ¥ä¸‰å±‚HRLæ¨¡å‹æµ‹è¯•")
    print("=" * 80 + "\n")

    # åˆ›å»ºæ¨¡å‹
    model = create_model(
        node_feat_dim=14,  # 6 + K_vnf (å‡è®¾K_vnf=8)
        edge_feat_dim=3,
        request_dim=10,
        hidden_dim=128,
        num_goals=10,
        num_actions=50,
        use_checkpoint=False
    )

    # æ‰“å°æ‘˜è¦
    print(model_summary(model))

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    num_nodes = 28
    num_edges = 45

    x = torch.randn(num_nodes, 14, device=device)
    edge_index = torch.randint(0, num_nodes, (2, num_edges), device=device)
    edge_attr = torch.randn(num_edges, 3, device=device)
    request_vec = torch.randn(10, device=device)
    dest_indices = [5, 10, 15]

    print("\n" + "=" * 80)
    print("æµ‹è¯•å„å±‚å‰å‘ä¼ æ’­")
    print("=" * 80 + "\n")

    # æµ‹è¯•High-Level
    print("1. High-Level (VNFå…±äº«ç­–ç•¥é€‰æ‹©):")
    with torch.no_grad():
        strategy_logits = model.forward_high(x, edge_index, edge_attr, request_vec, dest_indices)
        strategy = torch.argmax(strategy_logits).item()
        print(f"   Strategy logits: {strategy_logits.cpu().numpy()}")
        print(f"   Selected strategy: {strategy}")

    # æµ‹è¯•Mid-Level
    print("\n2. Mid-Level (ç›®æ ‡èŠ‚ç‚¹é€‰æ‹©):")
    candidate_goals = [5, 10, 15]
    with torch.no_grad():
        goal_scores = model.forward_mid(x, edge_index, edge_attr, request_vec,
                                        candidate_goals, dest_indices)
        best_goal = candidate_goals[torch.argmax(goal_scores).item()]
        print(f"   Goal scores: {goal_scores.cpu().numpy()}")
        print(f"   Selected goal: {best_goal}")

    # æµ‹è¯•Low-Level
    print("\n3. Low-Level (åŠ¨ä½œQå€¼):")
    goal_idx = torch.tensor([5], device=device)
    action_mask = torch.ones(1, 50, device=device, dtype=torch.bool)
    action_mask[0, 25:] = False  # åªå…è®¸å‰25ä¸ªåŠ¨ä½œ

    with torch.no_grad():
        q_values = model.forward_low(x, edge_index, edge_attr, None,
                                     request_vec, goal_idx, action_mask)
        best_action = torch.argmax(q_values).item()
        print(f"   Q-values shape: {q_values.shape}")
        print(f"   Max Q-value: {q_values.max().item():.4f}")
        print(f"   Selected action: {best_action}")

    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("=" * 80 + "\n")