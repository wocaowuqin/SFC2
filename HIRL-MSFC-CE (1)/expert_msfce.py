#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# expert_msfce.py - BEST OF BOTH WORLDS
"""
èåˆç‰ˆæœ¬ç‰¹æ€§ï¼š
1. âœ… Document 4 çš„ SolverConfig é…ç½®ç±»
2. âœ… Document 4 çš„ Rollback æœºåˆ¶
3. âœ… Document 4 çš„å¢å¼º Recall ç­–ç•¥
4. âœ… Document 5 çš„ç®€æ´ä»£ç ç»“æ„
5. âœ… Document 4 çš„å®Œæ•´èµ„æºéªŒè¯ï¼ˆå«å¸¦å®½ï¼‰
6. âœ… Document 5 çš„æ¸…æ™°æ³¨é‡Šé£æ ¼
"""

from __future__ import annotations
import time
import copy
import logging
import sys
from collections import OrderedDict
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set, Any

import numpy as np
import scipy.io as sio

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [Expert] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)


# ========== é…ç½®ç±»ï¼ˆæ¥è‡ª Document 4ï¼‰==========
@dataclass
class SolverConfig:
    """é›†ä¸­å¼é…ç½®ç®¡ç† - ä¿®å¤ç‰ˆ"""
    alpha: float = 0.3
    beta: float = 0.3
    gamma: float = 0.4
    candidate_set_size: int = 8  # ğŸ”¥ ä»4æ”¹ä¸º8
    lookahead_depth: int = 1  # ğŸ”¥ ä»3æ”¹ä¸º1 (å…³é”®!)
    k_path: int = 5
    max_cache_size: int = 5000
    max_iterations: int = 500  # ğŸ”¥ ä»200æ”¹ä¸º500
    max_time_seconds: float = 60.0  # ğŸ”¥ ä»20æ”¹ä¸º60
    max_candidates: int = 30  # ğŸ”¥ ä»12æ”¹ä¸º30
    otv_link_weight: float = 0.2
    otv_node_weight: float = 0.8
    otv_norm_link: float = 90.0
    otv_norm_node: float = 8.0

    def __post_init__(self):
        """å‚æ•°éªŒè¯"""
        if not (0 <= self.alpha <= 1 and 0 <= self.beta <= 1 and 0 <= self.gamma <= 1):
            raise ValueError("Alpha, beta, gamma must be between 0 and 1")
        if abs(self.alpha + self.beta + self.gamma - 1.0) > 1e-6:
            logger.warning("Score weights do not sum to 1.0")


def parse_mat_request(req_obj) -> Dict:
    """è§£æè¯·æ±‚ï¼ˆå…¼å®¹ Python Dict å’Œ MATLAB æ ¼å¼ï¼‰"""
    # 1. å¦‚æœå·²ç»æ˜¯ Python å­—å…¸ (æ¥è‡ª .pkl)ï¼Œç›´æ¥è¿”å›
    if isinstance(req_obj, dict):
        return req_obj

    # 2. å¦åˆ™å°è¯•è§£æ MATLAB æ ¼å¼ (ä¿ç•™åŸæœ‰é€»è¾‘ä»¥é˜²ä¸‡ä¸€)
    try:
        return {
            'id': int(req_obj['id'][0, 0]),
            'source': int(req_obj['source'][0, 0]),
            'dest': [int(d) for d in req_obj['dest'].flatten()],
            'vnf': [int(v) for v in req_obj['vnf'].flatten()],
            'bw_origin': float(req_obj['bw_origin'][0, 0]),
            'cpu_origin': [float(c) for c in req_obj['cpu_origin'].flatten()],
            'memory_origin': [float(m) for m in req_obj['memory_origin'].flatten()],
            'arrival_time': int(req_obj.get('arrival_time', [[0]])[0, 0]),
            'leave_time': int(req_obj.get('leave_time', [[0]])[0, 0]),
        }
    except:
        # æ—§ç‰ˆå…¼å®¹
        return {
            'id': int(req_obj[0][0][0]),
            'source': int(req_obj[0][1][0]),
            'dest': [int(x) for x in req_obj[0][2].flatten()],
            'vnf': [int(x) for x in req_obj[0][3].flatten()],
            'cpu_origin': [float(x) for x in req_obj[0][4].flatten()],
            'memory_origin': [float(x) for x in req_obj[0][5].flatten()],
            'bw_origin': float(req_obj[0][6][0][0])
        }


# def parse_mat_request(req_obj) -> Dict:
#     """è§£æ MATLAB è¯·æ±‚ï¼ˆå…¼å®¹ä¸¤ç§æ ¼å¼ï¼‰"""
#     try:
#         return {
#             'id': int(req_obj['id'][0, 0]),
#             'source': int(req_obj['source'][0, 0]),
#             'dest': [int(d) for d in req_obj['dest'].flatten()],
#             'vnf': [int(v) for v in req_obj['vnf'].flatten()],
#             'bw_origin': float(req_obj['bw_origin'][0, 0]),
#             'cpu_origin': [float(c) for c in req_obj['cpu_origin'].flatten()],
#             'memory_origin': [float(m) for m in req_obj['memory_origin'].flatten()],
#             'arrival_time': int(req_obj.get('arrival_time', [[0]])[0, 0]),
#             'leave_time': int(req_obj.get('leave_time', [[0]])[0, 0]),
#         }
#     except:
#         return {
#             'id': int(req_obj[0][0][0]),
#             'source': int(req_obj[0][1][0]),
#             'dest': [int(x) for x in req_obj[0][2].flatten()],
#             'vnf': [int(x) for x in req_obj[0][3].flatten()],
#             'cpu_origin': [float(x) for x in req_obj[0][4].flatten()],
#             'memory_origin': [float(x) for x in req_obj[0][5].flatten()],
#             'bw_origin': float(req_obj[0][6][0][0])
#         }
#

class MSFCE_Solver:
    """MSFC-CE ä¸“å®¶ç®—æ³•æ±‚è§£å™¨ï¼ˆèåˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, path_db_file: Path, topology_matrix: np.ndarray,
                 dc_nodes: List[int], capacities: Dict,
                 config: Optional[SolverConfig] = None):

        self.config = config or SolverConfig()

        # åŠ è½½ Path DB
        if not Path(path_db_file).exists():
            raise FileNotFoundError(f"Path DB missing: {path_db_file}")

        try:
            mat = sio.loadmat(path_db_file)
            self.path_db = mat['Paths']
            logger.info(f"Loaded Path DB from {path_db_file}")
        except Exception as e:
            raise RuntimeError(f"Path DB load failed: {e}")

        # ç½‘ç»œæ‹“æ‰‘
        self.node_num = int(topology_matrix.shape[0])
        self.link_num, self.link_map = self._create_link_map(topology_matrix)

        # VNF ç±»å‹å’Œ DC èŠ‚ç‚¹
        self.type_num = 8
        self.DC = set(dc_nodes)
        self.dc_num = len(dc_nodes)

        # èµ„æºå®¹é‡
        self.cap_cpu = float(capacities['cpu'])
        self.cap_mem = float(capacities['memory'])
        self.cap_bw = float(capacities['bandwidth'])

        # K æ¡è·¯å¾„
        self.k_path = int(self.config.k_path)
        self.k_path_count = self.k_path  # å…¼å®¹æ—§ä»£ç 

        # LRU ç¼“å­˜
        self._path_eval_cache = OrderedDict()
        self.MAX_CACHE_SIZE = int(self.config.max_cache_size)

        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            'total_requests': 0,
            'accepted': 0,
            'rejected': 0,
            'failure_reasons': {},
            'cache_hits': 0,
            'cache_misses': 0,
            'processing_times': [],
            'errors': 0,
        }

        # ========== ğŸ” åˆå§‹åŒ–è¯Šæ–­ START ==========
        logger.info("=" * 60)
        logger.info("DIAGNOSTIC: Expert MSFCE Initialization")
        logger.info("=" * 60)

        # 1. åŸºæœ¬é…ç½®
        logger.info(f"âœ“ Node count: {self.node_num}")
        logger.info(f"âœ“ Link count: {self.link_num}")
        logger.info(f"âœ“ Type count: {self.type_num}")
        logger.info(f"âœ“ K-path: {self.k_path}")

        # 2. DCèŠ‚ç‚¹æ£€æŸ¥
        logger.info(f"âœ“ DC count: {len(self.DC)}")
        if len(self.DC) == 0:
            logger.error("âœ— ERROR: DC list is EMPTY! This will cause 100% blocking!")
            logger.error("  Fix: Ensure dc_nodes parameter is passed correctly")
        else:
            dc_sorted = sorted(list(self.DC))
            logger.info(f"âœ“ DC nodes (first 10): {dc_sorted[:10]}")
            logger.info(f"  DC range: [{min(dc_sorted)}, {max(dc_sorted)}]")

        # 3. èµ„æºå®¹é‡
        logger.info(f"âœ“ Capacities: CPU={self.cap_cpu}, MEM={self.cap_mem}, BW={self.cap_bw}")

        # 4. é…ç½®å‚æ•°
        logger.info(f"âœ“ Config: lookahead_depth={self.config.lookahead_depth}, "
                    f"max_candidates={self.config.max_candidates}, "
                    f"candidate_set_size={self.config.candidate_set_size}")

        # 5. è·¯å¾„æ•°æ®åº“æ£€æŸ¥
        if self.path_db is None:
            logger.error("âœ— ERROR: Path DB is None!")
        else:
            logger.info(f"âœ“ Path DB shape: {self.path_db.shape}")

            # æµ‹è¯•è·¯å¾„æŸ¥è¯¢
            if len(self.DC) >= 2:
                dc_list = sorted(list(self.DC))
                test_src, test_dst = dc_list[0], dc_list[1]

                logger.info(f"Testing path query: {test_src} -> {test_dst}")

                try:
                    nodes, dist, links = self._get_path_info(test_src, test_dst, 1)

                    if nodes:
                        logger.info(f"  âœ“ SUCCESS: Found {len(nodes)} nodes")
                        logger.info(f"    Path nodes: {nodes}")

                        # æ£€æŸ¥DCè¦†ç›–
                        dcs_on_path = [n for n in nodes if n in self.DC]
                        logger.info(f"    DCs on path: {dcs_on_path} ({len(dcs_on_path)} DCs)")

                        if len(dcs_on_path) == 0:
                            logger.error("  âœ— ERROR: No DCs on path! Index mismatch?")
                            logger.error(f"    Path nodes range: [{min(nodes)}, {max(nodes)}]")
                            logger.error(f"    DC nodes range: [{min(dc_sorted)}, {max(dc_sorted)}]")
                    else:
                        logger.error("  âœ— ERROR: NO PATH FOUND!")
                        logger.error("    This is likely the root cause of low acceptance rate")
                        logger.error("    Possible issues:")
                        logger.error("      1. Path DB format incompatibility")
                        logger.error("      2. Index confusion (0-based vs 1-based)")
                        logger.error("      3. Missing path data filtering")

                except Exception as e:
                    logger.error(f"  âœ— EXCEPTION during path test: {e}")
                    import traceback
                    traceback.print_exc()

        logger.info("=" * 60)
        # ========== ğŸ” åˆå§‹åŒ–è¯Šæ–­ END ==========

    def _create_link_map(self, topo: np.ndarray) -> Tuple[int, Dict]:
        """æ„å»ºé“¾è·¯æ˜ å°„"""
        link_map = {}
        lid = 1
        for i in range(topo.shape[0]):
            for j in range(i + 1, topo.shape[0]):
                if not np.isinf(topo[i, j]) and topo[i, j] > 0:
                    link_map[(i + 1, j + 1)] = lid
                    link_map[(j + 1, i + 1)] = lid
                    lid += 1
        return lid - 1, link_map

    def _normalize_state(self, state: Dict) -> Dict:
        """æ ‡å‡†åŒ–çŠ¶æ€å­—å…¸"""
        normalized = {}
        normalized['bw'] = state.get('bw', state.get('bandwidth',
                                                     np.full(self.link_num, self.cap_bw)))
        normalized['cpu'] = state.get('cpu', np.full(self.node_num, self.cap_cpu))
        normalized['mem'] = state.get('mem', state.get('memory',
                                                       np.full(self.node_num, self.cap_mem)))
        normalized['hvt'] = state.get('hvt', state.get('hvt_all',
                                                       np.zeros((self.node_num, self.type_num))))
        normalized['bw_ref_count'] = state.get('bw_ref_count',
                                               np.zeros(self.link_num))
        if 'request' in state:
            normalized['request'] = state['request']
        return normalized

    # ========== æ ¸å¿ƒæ–¹æ³•ï¼šè·¯å¾„æŸ¥è¯¢ï¼ˆä¿®å¤ç‰ˆï¼‰==========
    def _get_path_info(self, src: int, dst: int, k: int):
        """
        è·å–è·¯å¾„ä¿¡æ¯ï¼ˆ1-based ç´¢å¼•ï¼‰
        ğŸ”¥ ä¿®å¤ç‰ˆï¼šæ·»åŠ è´Ÿå€¼è¿‡æ»¤å’Œdist_kæˆªæ–­
        """
        if self.path_db is None:
            return [], 0, []

        # è‡ªç¯å¤„ç†
        if src == dst:
            return [src], 0, []

        # ç´¢å¼•èŒƒå›´æ£€æŸ¥
        if src < 1 or src > self.node_num or dst < 1 or dst > self.node_num:
            logger.warning(f"[PATH] Invalid nodes: src={src}, dst={dst}, valid=[1,{self.node_num}]")
            return [], 0, []

        try:
            # è®¿é—®è·¯å¾„æ•°æ® (è½¬ä¸º0-basedç´¢å¼•)
            pinfo = self.path_db[src - 1, dst - 1]

            # æ£€æŸ¥pathså­—æ®µ
            if 'paths' not in pinfo.dtype.names:
                logger.debug(f"[PATH] No 'paths' field for [{src}->{dst}]")
                return [], 0, []

            raw_paths = pinfo['paths']

            if raw_paths.size == 0:
                logger.debug(f"[PATH] Empty paths for [{src}->{dst}]")
                return [], 0, []

            # è·å–ç¬¬kæ¡è·¯å¾„
            idx = k - 1
            path_arr = None

            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
            if raw_paths.dtype == 'O':  # å¯¹è±¡æ•°ç»„
                flat_data = raw_paths.flatten()
                if idx < len(flat_data):
                    path_arr = flat_data[idx]
            elif raw_paths.ndim == 2:  # äºŒç»´æ•°ç»„
                if idx < raw_paths.shape[0]:
                    path_arr = raw_paths[idx]
            elif raw_paths.ndim == 1 and idx == 0:  # ä¸€ç»´æ•°ç»„
                path_arr = raw_paths

            if path_arr is None:
                return [], 0, []

            # ğŸ”¥ ä¿®å¤1: è·å–distanceä¿¡æ¯
            dist_k = 0
            if 'pathsdistance' in pinfo.dtype.names:
                raw_dists = pinfo['pathsdistance'].flatten()
                if idx < len(raw_dists):
                    dist_k = int(raw_dists[idx])

            # ğŸ”¥ ä¿®å¤2: è½¬æ¢ä¸ºåˆ—è¡¨å¹¶è¿‡æ»¤è´Ÿå€¼ (å‚è€ƒcalc_eval1.py)
            path_arr_flat = np.array(path_arr).flatten()

            # å…ˆæˆªå–åˆ°dist_k+1é•¿åº¦
            if dist_k > 0:
                path_segment = path_arr_flat[:dist_k + 1]
            else:
                path_segment = path_arr_flat

            # ğŸ”¥ å…³é”®ä¿®å¤: è¿‡æ»¤è´Ÿå€¼å’Œ0 (MATLABå¡«å……å€¼)
            path_nodes = [int(x) for x in path_segment if int(x) > 0]

            if len(path_nodes) == 0:
                logger.debug(f"[PATH] All nodes filtered for [{src}->{dst}], k={k}")
                return [], 0, []

            # ğŸ”¥ å®Œå…¨å¿½ç•¥ link_ids å­—æ®µï¼Œä»èŠ‚ç‚¹é‡æ–°è®¡ç®—
            # åŸå› ï¼šè·¯å¾„æ•°æ®åº“ä¸­çš„ link_ids å€¼è¶…å‡ºå®é™…é“¾è·¯æ•°é‡
            links = []
            if len(path_nodes) > 1:
                for i in range(len(path_nodes) - 1):
                    u, v = path_nodes[i], path_nodes[i + 1]
                    if (u, v) in self.link_map:
                        links.append(self.link_map[(u, v)])
                    elif (v, u) in self.link_map:
                        # å°è¯•åå‘ï¼ˆåŒå‘é“¾è·¯ï¼‰
                        links.append(self.link_map[(v, u)])
                    else:
                        logger.debug(f"[PATH] No link for edge ({u},{v})")

            return path_nodes, len(path_nodes) - 1 if len(path_nodes) > 1 else 0, links

        except Exception as e:
            logger.warning(f"[PATH] Exception for [{src}->{dst}], k={k}: {e}")
            return [], 0, []

    def _get_max_hops(self, src: int, dst: int) -> int:
        """è·å–æœ€å¤§è·³æ•°"""
        try:
            cell = self.path_db[src - 1, dst - 1]
            return int(cell['pathsdistance'][self.k_path - 1][0])
        except:
            return 10

    # ========== è·¯å¾„è¯„åˆ†ï¼ˆå¸¦ LRU ç¼“å­˜ï¼‰==========
    def _calc_path_eval(self, nodes: List[int], links: List[int],
                        state: Dict, src_node: int, dst_node: int) -> float:
        """
        è®¡ç®—è·¯å¾„è¯„åˆ†ï¼ˆé™æ€éƒ¨åˆ†ç¼“å­˜ + åŠ¨æ€èµ„æºï¼‰
        âœ… Document 4 çš„ç¼“å­˜ç­–ç•¥
        """
        if not nodes:
            return 0.0

        cache_key = (src_node, dst_node, tuple(nodes))

        # ç¼“å­˜æŸ¥è¯¢ï¼ˆLRUï¼‰
        if cache_key in self._path_eval_cache:
            term1, term2 = self._path_eval_cache.pop(cache_key)
            self._path_eval_cache[cache_key] = (term1, term2)
            self.metrics['cache_hits'] += 1
        else:
            # é™æ€éƒ¨åˆ†è®¡ç®—
            max_hops = self._get_max_hops(src_node, dst_node)
            current_hops = len(nodes) - 1
            term1 = 1.0 - (current_hops / max(1, max_hops))

            dc_count = sum(1 for n in nodes if n in self.DC)
            term2 = dc_count / max(1, self.dc_num)

            # å­˜å…¥ç¼“å­˜
            self._path_eval_cache[cache_key] = (term1, term2)
            self.metrics['cache_misses'] += 1

            # LRU æ·˜æ±°
            if len(self._path_eval_cache) > self.MAX_CACHE_SIZE:
                self._path_eval_cache.popitem(last=False)

        # åŠ¨æ€éƒ¨åˆ†ï¼ˆå®æ—¶èµ„æºï¼‰
        sr_val = 0.0
        for n in nodes:
            if n in self.DC:
                idx = n - 1
                if idx < len(state['cpu']):
                    sr_val += (state['cpu'][idx] + state['mem'][idx]) / \
                              (self.cap_cpu + self.cap_mem)
        for lid in links:
            idx = lid - 1
            if idx < len(state['bw']):
                sr_val += state['bw'][idx] / self.cap_bw

        norm_factor = max(1, len(nodes) + len(links))
        term3 = sr_val / norm_factor

        return float(self.config.alpha * term1 +
                     self.config.beta * term2 +
                     self.config.gamma * term3)

    # ========== VNF éƒ¨ç½²ï¼ˆè¯¦ç»†å¤±è´¥åŸå› ï¼‰==========
    def _try_deploy_vnf(self, request: Dict, path_nodes: List[int],
                        state: Dict, existing_hvt: np.ndarray) -> Tuple[bool, np.ndarray, Dict, Dict]:
        """
        å°è¯•éƒ¨ç½² VNF
        âœ… Document 5 çš„è¯¦ç»†å¤±è´¥åŸå› è®°å½•
        Returns: (feasible, hvt, placement, resource_delta_or_reason)
        """
        req_vnfs = request.get('vnf', [])
        hvt = existing_hvt.copy()
        placement = {}
        path_dcs = [n for n in path_nodes if n in self.DC]

        cpu_delta = np.zeros(self.node_num)
        mem_delta = np.zeros(self.node_num)

        # âœ… è¯¦ç»†å¤±è´¥åŸå› ï¼šDC ä¸è¶³
        if len(path_dcs) < len(req_vnfs):
            reason = {
                'type': 'not_enough_dcs',
                'required': len(req_vnfs),
                'available': len(path_dcs),
                'path': path_nodes
            }
            logger.debug(f"Req {request.get('id', '?')}: {reason['type']}")
            return False, existing_hvt, {}, reason

        current_dc_idx = 0
        for v_idx, v_type in enumerate(req_vnfs):
            deployed = False

            # 1. å°è¯•å¤ç”¨
            for node in path_dcs:
                node_idx = node - 1
                if node_idx < hvt.shape[0] and (v_type - 1) < hvt.shape[1]:
                    if hvt[node_idx, v_type - 1] > 0:
                        placement[v_idx] = node
                        deployed = True
                        break
            if deployed:
                continue

            # 2. å°è¯•æ–°éƒ¨ç½²
            start_search = current_dc_idx
            while start_search < len(path_dcs):
                node = path_dcs[start_search]
                node_idx = node - 1
                if node_idx >= len(state['cpu']):
                    start_search += 1
                    continue

                cpu_req = request['cpu_origin'][v_idx]
                mem_req = request['memory_origin'][v_idx]

                curr_cpu = state['cpu'][node_idx] - cpu_delta[node_idx]
                curr_mem = state['mem'][node_idx] - mem_delta[node_idx]

                if curr_cpu >= cpu_req and curr_mem >= mem_req:
                    cpu_delta[node_idx] += cpu_req
                    mem_delta[node_idx] += mem_req
                    hvt[node_idx, v_type - 1] = 1
                    placement[v_idx] = node
                    deployed = True
                    current_dc_idx = start_search + 1
                    break
                else:
                    start_search += 1

            # âœ… è¯¦ç»†å¤±è´¥åŸå› ï¼šèµ„æºä¸è¶³
            if not deployed:
                reason = {
                    'type': 'resource_shortage',
                    'vnf_idx': v_idx,
                    'vnf_type': v_type,
                    'cpu_required': request['cpu_origin'][v_idx],
                    'mem_required': request['memory_origin'][v_idx],
                    'checked_nodes': [path_dcs[i] for i in range(current_dc_idx, len(path_dcs))]
                }
                logger.debug(f"Req {request.get('id', '?')}: VNF {v_idx} resource_shortage")
                return False, existing_hvt, {}, reason

        resource_delta = {'cpu': cpu_delta, 'mem': mem_delta}
        return True, hvt, placement, resource_delta

    def _evaluate_otv(self, request: Dict, tree_links: np.ndarray, hvt: np.ndarray) -> float:
        """è®¡ç®— OTV æˆæœ¬"""
        node_cost = np.sum(hvt)
        link_cost = np.sum(tree_links)
        return self.config.otv_link_weight * (link_cost / self.config.otv_norm_link) + \
            self.config.otv_node_weight * (node_cost / self.config.otv_norm_node)

    # ========== èµ„æºéªŒè¯ï¼ˆå«å¸¦å®½æ£€æŸ¥ï¼‰==========
    def _validate_resource_deduction(self, state: Dict, resource_delta: Dict,
                                     request: Dict, links_used: Optional[List[int]] = None) -> bool:
        """
        âœ… Document 4 çš„å®Œæ•´èµ„æºéªŒè¯ï¼ˆå«å¸¦å®½ï¼‰
        """
        cpu_d = resource_delta.get('cpu', np.zeros(self.node_num))
        mem_d = resource_delta.get('mem', np.zeros(self.node_num))

        # å½¢çŠ¶éªŒè¯
        if cpu_d.shape != (self.node_num,) or mem_d.shape != (self.node_num,):
            logger.error(f"Shape mismatch: cpu{cpu_d.shape}, mem{mem_d.shape}")
            return False

        # éè´Ÿæ€§éªŒè¯
        if np.any(cpu_d < -1e-10) or np.any(mem_d < -1e-10):
            logger.error("Negative delta detected")
            return False

        # å®¹é‡éªŒè¯
        if np.any(state['cpu'] - cpu_d < -1e-8):
            logger.error(f"CPU violation at nodes: {np.where(state['cpu'] - cpu_d < 0)[0]}")
            return False
        if np.any(state['mem'] - mem_d < -1e-8):
            logger.error(f"MEM violation at nodes: {np.where(state['mem'] - mem_d < 0)[0]}")
            return False

        # âœ… å¸¦å®½éªŒè¯
        if links_used:
            bw_required = request.get('bw_origin', 0.0)
            for lid in links_used:
                idx = lid - 1
                if idx < len(state['bw']):
                    if state['bw'][idx] < bw_required - 1e-8:
                        logger.error(f"BW insufficient on link {lid}: {state['bw'][idx]:.4f} < {bw_required:.4f}")
                        return False

        return True

    # ========== åº”ç”¨è·¯å¾„åˆ°æ ‘ï¼ˆå¸¦å›æ»šï¼‰==========
    def _apply_path_to_tree(self, tree_struct, info, request, state,
                            real_deploy=False, resource_delta=None):
        """
        âœ… Document 5 çš„ä¸¥æ ¼æ–­è¨€ + Document 4 çš„å¸¦å®½æ£€æŸ¥
        """
        nodes = info['nodes']
        links_used = []

        # æ›´æ–°é“¾è·¯
        for i in range(len(nodes) - 1):
            u, v = nodes[i], nodes[i + 1]
            if (u, v) in self.link_map:
                lid = self.link_map[(u, v)]
                idx = lid - 1
                links_used.append(lid)
                if idx < len(tree_struct['tree']):
                    if tree_struct['tree'][idx] == 0:
                        tree_struct['tree'][idx] = 1
                        if real_deploy and idx < len(state['bw']):
                            state['bw'][idx] = max(0.0, state['bw'][idx] - request['bw_origin'])

        tree_struct['nodes'].update(nodes)
        tree_struct['paths_map'][nodes[-1]] = nodes

        if 'hvt' in info:
            tree_struct['hvt'] = np.maximum(tree_struct['hvt'], info['hvt'])

        # èµ„æºæ‰£å‡ï¼ˆéªŒè¯ + åº”ç”¨ï¼‰
        if real_deploy and resource_delta:
            ok = self._validate_resource_deduction(state, resource_delta, request, links_used)
            if not ok:
                raise ValueError("Resource deduction validation failed")

            cpu_d = resource_delta['cpu']
            mem_d = resource_delta['mem']
            state['cpu'] = np.maximum(state['cpu'] - cpu_d, 0.0)
            state['mem'] = np.maximum(state['mem'] - mem_d, 0.0)

    def _apply_path_to_tree_with_rollback(self, tree_struct, info, request, state,
                                          real_deploy=False, resource_delta=None) -> bool:
        """
        âœ… Document 4 çš„ Rollback æœºåˆ¶
        """
        original_state = {
            'cpu': state['cpu'].copy(),
            'mem': state['mem'].copy(),
            'bw': state['bw'].copy()
        }
        original_tree = {
            'tree': tree_struct['tree'].copy(),
            'hvt': tree_struct['hvt'].copy(),
            'paths_map': copy.deepcopy(tree_struct['paths_map']),
            'nodes': set(tree_struct['nodes'])
        }

        try:
            self._apply_path_to_tree(tree_struct, info, request, state,
                                     real_deploy, resource_delta)
            return True
        except Exception as e:
            # å›æ»š
            state.update(original_state)
            tree_struct['tree'] = original_tree['tree']
            tree_struct['hvt'] = original_tree['hvt']
            tree_struct['paths_map'] = original_tree['paths_map']
            tree_struct['nodes'] = original_tree['nodes']
            logger.error(f"Rollback: {e}")
            return False

    # ========== æ¥å£æ–¹æ³•ï¼ˆå…¼å®¹æ€§ï¼‰==========
    def _calc_eval(self, request: Dict, d_idx: int, k: int, state: Dict):
        """è¿”å› 8 ä¸ªå€¼ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        state = self._normalize_state(state)
        src = request['source']
        dst = request['dest'][d_idx]
        nodes, dist, links = self._get_path_info(src, dst, k)

        if not nodes:
            return 0.0, [], np.zeros(self.link_num), np.zeros((self.node_num, self.type_num)), False, dst, 0.0, {}

        score = self._calc_path_eval(nodes, links, state, src, dst)
        temp_state = copy.deepcopy(state)
        feasible, new_hvt, placement, _ = self._try_deploy_vnf(
            request, nodes, temp_state, np.zeros((self.node_num, self.type_num)))

        tree_vec = np.zeros(self.link_num)
        if feasible:
            for lid in links:
                if lid - 1 < len(tree_vec):
                    tree_vec[lid - 1] = 1

        cost = self._evaluate_otv(request, tree_vec, new_hvt) if feasible else 0.0
        return score, nodes, tree_vec, new_hvt, feasible, dst, cost, placement

    def _calc_atnp(self, current_tree: Dict, conn_path: List[int], d_idx: int,
                   state: Dict, nodes_on_tree: Set[int]):
        """Stage 2: è¿æ¥æ–°ç›®æ ‡åˆ°æ ‘"""
        state = self._normalize_state(state)
        request = state.get('request')
        if request is None:
            return {'feasible': False}, 0.0, (0, 0), 0.0

        best_eval = -1.0
        best_res = None
        best_action = (0, 0)

        for i_idx, conn_node in enumerate(conn_path):
            for k in range(1, self.k_path + 1):
                # âœ… ä¿®å¤ï¼šåˆ é™¤é‡å¤çš„è¿™ä¸€è¡Œ
                nodes, dist, links = self._get_path_info(conn_node, request['dest'][d_idx], k)

                if not nodes or len(nodes) < 2:
                    continue
                if set(nodes[1:]) & nodes_on_tree:
                    continue

                score = self._calc_path_eval(nodes, links, state, conn_node, request['dest'][d_idx])
                if score > best_eval:
                    temp_state = copy.deepcopy(state)
                    temp_state['request'] = request
                    full_nodes = conn_path[:i_idx + 1] + nodes[1:]
                    existing_hvt = current_tree.get('hvt', np.zeros((self.node_num, self.type_num)))

                    feasible, new_hvt, placement, res_delta = self._try_deploy_vnf(
                        request, full_nodes, temp_state, existing_hvt)

                    if feasible:
                        best_eval = score
                        tree_vec = np.zeros(self.link_num)
                        for lid in links:
                            if lid - 1 < len(tree_vec):
                                tree_vec[lid - 1] = 1
                        best_res = {
                            'tree': tree_vec, 'hvt': new_hvt, 'new_path_full': nodes,
                            'feasible': True, 'placement': placement, 'res_delta': res_delta
                        }
                        best_action = (i_idx, k - 1)

        if best_res:
            cost = self._evaluate_otv(request, best_res['tree'], best_res['hvt'])
            return best_res, best_eval, best_action, cost
        else:
            return {'feasible': False}, 0.0, (0, 0), 0.0

    # ========== èµ„æºé¢„æ£€æŸ¥ ==========
    def _check_resource_feasibility(self, request: Dict, state: Dict) -> bool:
        """å¿«é€Ÿå…¨å±€èµ„æºæ£€æŸ¥"""
        total_cpu_req = sum(request.get('cpu_origin', []))
        total_mem_req = sum(request.get('memory_origin', []))
        total_bw_req = request.get('bw_origin', 0.0) * len(request.get('dest', []))

        available_cpu = np.sum(state['cpu'])
        available_mem = np.sum(state['mem'])
        available_bw = np.sum(state['bw'])

        if total_cpu_req > available_cpu or total_mem_req > available_mem:
            return False
        if total_bw_req > available_bw:
            logger.debug("Total BW requirement high (may still succeed due to sharing)")
        return True

    def _get_adaptive_lookahead_depth(self, num_remaining: int) -> int:
        """åŠ¨æ€è°ƒæ•´ lookahead æ·±åº¦"""
        if num_remaining <= 2:
            return min(num_remaining, self.config.lookahead_depth)
        elif num_remaining <= 5:
            return min(2, self.config.lookahead_depth)
        else:
            return 1

    # ========== æ ‘æ„å»ºä¸»æµç¨‹ ==========
    def _construct_tree(self, request: Dict, network_state: Dict,
                        forced_first_dest_idx: Optional[int] = None) -> Tuple[Optional[Dict], List]:
        """
        æ ¸å¿ƒæ ‘æ„å»ºé€»è¾‘ï¼ˆå«å€™é€‰é›† + Lookaheadï¼‰
        âœ… Document 5 çš„æ¸…æ™°ç»“æ„ + Document 4 çš„ Rollback
        """
        start_time = time.time()
        iteration_count = 0

        current_sim_state = copy.deepcopy(network_state)
        dest_indices = list(range(len(request['dest'])))

        current_tree = {
            'id': request['id'],
            'tree': np.zeros(self.link_num),
            'hvt': np.zeros((self.node_num, self.type_num)),
            'paths_map': {},
            'nodes': {request['source']},
            'added_dest_indices': [],
            'traj': []
        }
        ordered_paths = []

        MAX_CANDIDATES = int(self.config.max_candidates)
        MAX_ITER = int(self.config.max_iterations)
        MAX_TIME = float(self.config.max_time_seconds)

        while len(current_tree['added_dest_indices']) < len(dest_indices):
            iteration_count += 1

            # âœ… è¿­ä»£/è¶…æ—¶é™åˆ¶
            if iteration_count > MAX_ITER:
                logger.warning(f"Req {request['id']}: Max iterations ({MAX_ITER}) reached")
                return None, [d for d in dest_indices if d not in current_tree['added_dest_indices']]

            if time.time() - start_time > MAX_TIME:
                logger.warning(f"Req {request['id']}: Timeout ({MAX_TIME}s) reached")
                return None, [d for d in dest_indices if d not in current_tree['added_dest_indices']]

            unadded = [d for d in dest_indices if d not in current_tree['added_dest_indices']]
            candidates = []

            # A. å€™é€‰é›†ç”Ÿæˆ
            if not ordered_paths:
                # Stage 1: Source -> Dest
                targets = [forced_first_dest_idx] if forced_first_dest_idx is not None else unadded
                for d_idx in targets:
                    if d_idx not in unadded:
                        continue
                    if len(candidates) >= MAX_CANDIDATES:
                        break

                    for k in range(1, self.k_path + 1):
                        score, nodes, t_vec, h_vec, feas, _, cost, pl = self._calc_eval(
                            request, d_idx, k, current_sim_state)

                        if feas:
                            # è·å– res_delta
                            _, _, _, res_delta = self._try_deploy_vnf(
                                request, nodes, current_sim_state,
                                np.zeros((self.node_num, self.type_num)))

                            info = {
                                'nodes': nodes, 'k': k, 'score': score, 'p_idx': 0,
                                'res_delta': res_delta, 'hvt': h_vec, 'tree_vec': t_vec,
                                'placement': pl, 'd_idx': d_idx
                            }
                            candidates.append(info)
            else:
                # Stage 2: Tree -> New Dest
                for p_idx, path in enumerate(ordered_paths):
                    for d_idx in unadded:
                        if len(candidates) >= MAX_CANDIDATES:
                            break

                        res, score, action_in_path, _ = self._calc_atnp(
                            current_tree, path, d_idx, current_sim_state, current_tree['nodes'])

                        if res and res.get('feasible'):
                            k = action_in_path[1] + 1
                            info = {
                                'nodes': res['new_path_full'], 'k': k, 'score': score,
                                'p_idx': p_idx, 'conn_idx_in_path': action_in_path[0],
                                'res_delta': res['res_delta'], 'hvt': res['hvt'],
                                'tree_vec': res['tree'], 'placement': res['placement'],
                                'd_idx': d_idx
                            }
                            candidates.append(info)

            if not candidates:
                return None, unadded

            # B. å€™é€‰é›†æ’åº
            candidates.sort(key=lambda x: x['score'], reverse=True)
            candidate_set = candidates[:int(self.config.candidate_set_size)]

            # C. Lookahead é€‰æ‹©
            best_global_otv = float('inf')
            selected_info = None
            current_lookahead_depth = self._get_adaptive_lookahead_depth(len(unadded) - 1)

            for info in candidate_set:
                d_idx = info['d_idx']

                # æ¨¡æ‹Ÿæ ‘å‰¯æœ¬
                temp_tree_sim = copy.deepcopy(current_tree)
                temp_state_sim = copy.deepcopy(current_sim_state)

                # âœ… ä½¿ç”¨ Rollback æœºåˆ¶
                applied = self._apply_path_to_tree_with_rollback(
                    temp_tree_sim, info, request, temp_state_sim,
                    real_deploy=True, resource_delta=info.get('res_delta'))

                if not applied:
                    continue

                # è´ªå©ª Lookahead æ‰©å±•
                remaining_after = [d for d in unadded if d != d_idx]
                subsequent_count = 0

                while subsequent_count < current_lookahead_depth and remaining_after:
                    next_candidates = []
                    current_sim_paths = list(temp_tree_sim['paths_map'].values()) \
                        if temp_tree_sim['paths_map'] else [[request['source']]]

                    for next_d_idx in remaining_after:
                        for path in current_sim_paths:
                            res, score, _, _ = self._calc_atnp(
                                temp_tree_sim, path, next_d_idx,
                                temp_state_sim, temp_tree_sim['nodes'])

                            if res and res.get('feasible'):
                                next_candidates.append((score, next_d_idx, res))

                    if not next_candidates:
                        break

                    next_candidates.sort(key=lambda x: x[0], reverse=True)
                    best_score, best_next_d, best_res = next_candidates[0]

                    temp_info_next = {
                        'nodes': best_res['new_path_full'],
                        'hvt': best_res['hvt'],
                        'tree_vec': best_res['tree']
                    }

                    applied2 = self._apply_path_to_tree_with_rollback(
                        temp_tree_sim, temp_info_next, request, temp_state_sim,
                        real_deploy=True, resource_delta=best_res['res_delta'])

                    if not applied2:
                        break

                    remaining_after.remove(best_next_d)
                    subsequent_count += 1

                # è®¡ç®— OTV
                otv = self._evaluate_otv(request, temp_tree_sim['tree'], temp_tree_sim['hvt'])
                if otv < best_global_otv:
                    best_global_otv = otv
                    selected_info = info

            # D. æ‰§è¡Œæœ€ä¼˜é€‰æ‹©
            if selected_info:
                d_idx = selected_info['d_idx']

                applied = self._apply_path_to_tree_with_rollback(
                    current_tree, selected_info, request, current_sim_state,
                    real_deploy=True, resource_delta=selected_info.get('res_delta'))

                if not applied:
                    self._record_failure(request.get('id', '?'),
                                         {'type': 'apply_failed', 'info': 'final apply failed'})
                    return None, [d for d in dest_indices if d not in current_tree['added_dest_indices']]

                current_tree['added_dest_indices'].append(d_idx)
                ordered_paths.append(selected_info['nodes'])

                # è®°å½•è½¨è¿¹
                p_idx = selected_info['p_idx']
                k_idx = selected_info['k'] - 1
                placement = selected_info.get('placement', {})
                action_tuple = (p_idx, k_idx, placement)
                cost = self._evaluate_otv(request, current_tree['tree'], current_tree['hvt'])
                current_tree['traj'].append((d_idx, action_tuple, cost))
            else:
                return None, unadded

        return current_tree, current_tree['traj']

    # ========== å¢å¼º Recall ç­–ç•¥ ==========
    def _estimate_destination_resource(self, request: Dict, d_idx: int,
                                       network_state: Dict) -> float:
        """ä¼°ç®—ç›®æ ‡èŠ‚ç‚¹çš„èµ„æºéœ€æ±‚ï¼ˆå¯å‘å¼ï¼‰"""
        cpu = sum(request.get('cpu_origin', []))
        mem = sum(request.get('memory_origin', []))
        bw = request.get('bw_origin', 0.0)
        return float(cpu + mem + bw * 10.0)

    def _enhanced_recall_strategy(self, request: Dict, network_state: Dict,
                                  failed_unadded: List[int]) -> Tuple[Optional[Dict], List]:
        """
        âœ… Document 4 çš„å¢å¼º Recall ç­–ç•¥ï¼ˆæŒ‰èµ„æºéœ€æ±‚æ’åºï¼‰
        """
        if not failed_unadded:
            return None, []

        logger.info(f"Recall for req {request.get('id', '?')} with {len(failed_unadded)} failed dests")

        # æŒ‰èµ„æºéœ€æ±‚æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰
        dest_resources = [(d_idx, self._estimate_destination_resource(request, d_idx, network_state))
                          for d_idx in failed_unadded]
        dest_resources.sort(key=lambda x: x[1])

        # ä¼˜å…ˆå°è¯•èµ„æºéœ€æ±‚å°çš„
        for d_idx, _ in dest_resources:
            recall_tree, recall_traj = self._construct_tree(
                request, network_state, forced_first_dest_idx=d_idx)

            if recall_tree is not None:
                logger.info(f"Recall successful using dest {d_idx} first")
                return recall_tree, recall_traj

        return None, []

    # ========== æ€§èƒ½ç›‘æ§ ==========
    def _record_failure(self, request_id, reason_dict):
        """è®°å½•å¤±è´¥åŸå› """
        reason_type = reason_dict.get('type', 'unknown')
        self.metrics['failure_reasons'][reason_type] = \
            self.metrics['failure_reasons'].get(reason_type, 0) + 1

    def clear_cache(self):
        """æ¸…ç©ºè·¯å¾„è¯„åˆ†ç¼“å­˜"""
        self._path_eval_cache.clear()
        logger.info("Path evaluation cache cleared")

    def export_metrics(self, path: Optional[Path] = None):
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡åˆ° CSV"""
        import csv
        if path is None:
            path = Path('expert_metrics.csv')

        with open(path, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['Metric', 'Value'])
            writer.writerow(['Total Requests', self.metrics['total_requests']])
            writer.writerow(['Accepted', self.metrics['accepted']])
            writer.writerow(['Rejected', self.metrics['rejected']])

            accept_rate = self.metrics['accepted'] / max(1, self.metrics['total_requests'])
            writer.writerow(['Accept Rate', f"{accept_rate:.2%}"])

            writer.writerow([])
            writer.writerow(['Failure Reason', 'Count'])
            for reason, count in self.metrics.get('failure_reasons', {}).items():
                writer.writerow([reason, count])

            if self.metrics.get('processing_times'):
                writer.writerow([])
                writer.writerow(['Avg Processing Time (s)',
                                 np.mean(self.metrics['processing_times'])])

        logger.info(f"Metrics exported to {path}")

    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        report = {
            'total_requests': self.metrics['total_requests'],
            'acceptance_rate': self.metrics['accepted'] / max(1, self.metrics['total_requests']),
            'cache_hit_rate': self.metrics['cache_hits'] /
                              max(1, self.metrics['cache_hits'] + self.metrics['cache_misses']),
            'failure_reasons': self.metrics.get('failure_reasons', {}),
        }

        if self.metrics.get('processing_times'):
            times = self.metrics['processing_times']
            report.update({
                'avg_processing_time': float(np.mean(times)),
                'max_processing_time': float(max(times)),
                'min_processing_time': float(min(times)),
            })

        return report

    def get_detailed_performance_report(self) -> Dict:
        """
        âœ… Document 4 çš„è¯¦ç»†æ€§èƒ½æŠ¥å‘Š
        """
        report = self.get_performance_report()

        # ç¼“å­˜æ•ˆç‡
        cache_eff = {
            'cache_size': len(self._path_eval_cache),
            'cache_max_size': self.MAX_CACHE_SIZE,
            'cache_utilization': len(self._path_eval_cache) / max(1, self.MAX_CACHE_SIZE)
        }
        report['cache_efficiency'] = cache_eff

        # æœ€è¿‘æ€§èƒ½è¶‹åŠ¿
        if self.metrics.get('processing_times'):
            times = self.metrics['processing_times']
            report['recent_performance'] = {
                'last_10_avg': float(np.mean(times[-10:])) if len(times) >= 10 else 0.0,
                'trend': 'improving' if len(times) > 1 and times[-1] < times[0] else 'stable'
            }

        return report

    # ========== ä¸»å…¥å£ ==========
    def solve_request_for_expert(self, request: Dict, network_state: Dict) -> Tuple[Optional[Dict], List]:
        """
        ä¸“å®¶ç®—æ³•ä¸»å…¥å£

        Args:
            request: è¯·æ±‚å­—å…¸ï¼ˆåŒ…å« id, source, dest, vnf, cpu_origin, memory_origin, bw_originï¼‰
            network_state: ç½‘ç»œçŠ¶æ€ï¼ˆåŒ…å« cpu, mem, bw, hvt ç­‰ï¼‰

        Returns:
            (tree_struct, traj) æˆåŠŸæ—¶è¿”å›æ ‘ç»“æ„å’Œè½¨è¿¹
            (None, []) å¤±è´¥æ—¶è¿”å› None
        """
        start_time = time.time()
        self.metrics['total_requests'] += 1

        try:
            network_state = self._normalize_state(network_state)
            network_state['request'] = request

            # 1. å¿«é€Ÿèµ„æºé¢„æ£€æŸ¥
            if not self._check_resource_feasibility(request, network_state):
                logger.warning(f"Req {request.get('id', '?')} skipped: insufficient resources")
                self.metrics['rejected'] += 1
                self._record_failure(request.get('id', '?'), {'type': 'global_resource_shortage'})
                return None, []

            # 2. æ­£å¸¸æ ‘æ„å»º
            res_tree, res_traj = self._construct_tree(request, network_state)

            proc_time = time.time() - start_time
            self.metrics['processing_times'].append(proc_time)

            if res_tree is not None:
                self.metrics['accepted'] += 1
                return res_tree, res_traj

            # 3. è§¦å‘å¢å¼º Recall
            failed_dests = res_traj
            if failed_dests:
                recall_tree, recall_traj = self._enhanced_recall_strategy(
                    request, network_state, failed_dests)

                if recall_tree is not None:
                    self.metrics['accepted'] += 1
                    return recall_tree, recall_traj

            # 4. æœ€ç»ˆå¤±è´¥
            self.metrics['rejected'] += 1
            self._record_failure(request.get('id', '?'), {'type': 'construct_tree_failed'})
            return None, []

        except Exception as e:
            logger.exception(f"Unexpected error in req {request.get('id', '?')}: {e}")
            self.metrics['errors'] += 1
            self.metrics['rejected'] += 1
            return None, []


if __name__ == "__main__":
    logger.info("Expert MSFCE module loaded (Fusion Version)")
    logger.info("Features: SolverConfig + Rollback + Enhanced Recall + Full Validation")