# vnf_metrics_logger.py
"""
VNFéƒ¨ç½²æŒ‡æ ‡æ”¶é›†ä¸CSVå¯¼å‡ºå·¥å…· (æœ€ç»ˆèåˆç‰ˆ)
é›†æˆåŠŸèƒ½ï¼š
1. è¯¦ç»†èµ„æºæ¶ˆè€—ç»Ÿè®¡ï¼ˆå»é‡åçš„å®é™…CPU/MEM/BWæ¶ˆè€—ï¼‰
2. éƒ¨ç½²æˆåŠŸç‡ã€é˜»å¡ç‡ç»Ÿè®¡
3. æ”¯æŒæŒ‰æ—¶é—´æ­¥ (time_step) è®°å½•ä¸èšåˆï¼Œç”¨äºç»˜åˆ¶éšæ—¶é—´å˜åŒ–çš„è¶‹åŠ¿å›¾
"""
import csv
import numpy as np
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from datetime import datetime
import json


class VNFMetricsLogger:
    """
    è®°å½•VNFéƒ¨ç½²è¿‡ç¨‹ä¸­çš„è¯¦ç»†æŒ‡æ ‡
    """

    def __init__(self, network_info: dict):
        """
        Args:
            network_info: {
                "total_nodes": int,
                "total_cpu": float,
                "total_bw": float,
                "total_mem": float,
                # å¯é€‰ï¼šå•èŠ‚ç‚¹/é“¾è·¯å®¹é‡ï¼Œç”¨äºæ›´ç²¾ç¡®è®¡ç®—åˆ©ç”¨ç‡
                "node_cpu_capacity": float,
                "node_mem_capacity": float,
                "link_bw_capacity": float
            }
        """
        self.network_info = network_info

        # ====== å•æ¬¡éƒ¨ç½²æŒ‡æ ‡ (Current Deployment Context) ======
        self.current_deployment = {
            "time_step": 0,  # [æ–°å¢] æ—¶é—´æ­¥
            "request_id": None,
            "start_time": None,
            "vnf_chain": [],
            "destinations": [],

            # èµ„æºæ¶ˆè€—ï¼ˆç´¯è®¡æ‰€æœ‰è·¯å¾„ - ç”¨äºç»Ÿè®¡æ€»æ¶ˆè€—ï¼‰
            "total_cpu_consumed": 0.0,
            "total_bw_consumed": 0.0,
            "total_mem_consumed": 0.0,

            # èµ„æºæ¶ˆè€—ï¼ˆæŒ‰èŠ‚ç‚¹/é“¾è·¯å»é‡ç»Ÿè®¡ - ç”¨äºè®¡ç®—ç²¾ç¡®åˆ©ç”¨ç‡ï¼‰
            "node_cpu_usage": {},  # {node_id: cpu_used}
            "node_mem_usage": {},  # {node_id: mem_used}
            "link_bw_usage": {},  # {(node1, node2): bw_used}

            # å®é™…ä½¿ç”¨çš„èŠ‚ç‚¹å’Œé“¾è·¯é›†åˆ
            "used_nodes": set(),
            "used_links": set(),

            # éƒ¨ç½²ç»“æœ
            "fully_deployed": False,
            "partial_deployed": False,
            "destinations_connected": 0,
            "destinations_failed": 0,

            # å¤±è´¥èŠ‚ç‚¹ä¿¡æ¯
            "failed_nodes": [],
            "failure_reasons": [],

            # è·¯å¾„ä¿¡æ¯
            "total_hops": 0,
            "paths": [],

            # å¤‡ä»½ç­–ç•¥ä½¿ç”¨
            "backup_used": False,
            "backup_levels": [],

            # è€—æ—¶
            "deployment_time": 0.0
        }

        # ====== å…¨å±€ç»Ÿè®¡ (Global Stats) ======
        self.global_stats = {
            "total_requests": 0,
            "fully_accepted": 0,
            "partially_accepted": 0,
            "totally_blocked": 0,

            "total_cpu_consumed": 0.0,
            "total_bw_consumed": 0.0,
            "total_mem_consumed": 0.0,

            "avg_cpu_utilization": [],
            "avg_bw_utilization": [],
            "avg_mem_utilization": [],

            # å¤±è´¥èŠ‚ç‚¹ç»Ÿè®¡
            "failed_nodes_count": defaultdict(int),
            "failure_reasons_count": defaultdict(int),

            # å¤‡ä»½ç­–ç•¥ç»Ÿè®¡
            "backup_usage_count": defaultdict(int),
        }

        # ====== æ—¶åºè®°å½• (History) ======
        self.deployment_history = []
        self.resource_utilization_history = []

    # ================================================================
    # ğŸ“Š è®°å½•å•æ¬¡éƒ¨ç½²
    # ================================================================

    def start_deployment(self, request_id: str, vnf_chain: List[str],
                         destinations: List[int], t: int = 0):  # <--- [æ ¸å¿ƒä¿®æ”¹] å¢åŠ  t å‚æ•°
        """å¼€å§‹è®°å½•ä¸€æ¬¡éƒ¨ç½²ï¼Œä¼ å…¥å½“å‰æ—¶é—´æ­¥ t"""
        self.current_deployment = {
            "time_step": t,  # <--- è®°å½•æ—¶é—´æ­¥
            "request_id": request_id,
            "start_time": datetime.now(),
            "vnf_chain": vnf_chain,
            "destinations": destinations,

            # åˆå§‹åŒ–èµ„æºç»Ÿè®¡å®¹å™¨
            "total_cpu_consumed": 0.0,
            "total_bw_consumed": 0.0,
            "total_mem_consumed": 0.0,
            "node_cpu_usage": {},
            "node_mem_usage": {},
            "link_bw_usage": {},
            "used_nodes": set(),
            "used_links": set(),

            # åˆå§‹åŒ–ç»“æœçŠ¶æ€
            "fully_deployed": False,
            "partial_deployed": False,
            "destinations_connected": 0,
            "destinations_failed": 0,
            "failed_nodes": [],
            "failure_reasons": [],
            "total_hops": 0,
            "paths": [],
            "backup_used": False,
            "backup_levels": [],
            "deployment_time": 0.0
        }

    def record_step(self,
                    step_info: dict,
                    resource_consumed: dict,
                    network_state: dict):
        """
        è®°å½•æ¯ä¸€æ­¥çš„éƒ¨ç½²ä¿¡æ¯ (é›†æˆè¯¦ç»†èµ„æºè®¡ç®—é€»è¾‘)
        """
        # 1. ç´¯è®¡æ€»æ¶ˆè€—ï¼ˆè·¯å¾„çº§ç´¯åŠ ï¼Œå¯èƒ½é‡å¤è®¡ç®—å…±äº«èŠ‚ç‚¹ï¼‰
        self.current_deployment["total_cpu_consumed"] += resource_consumed.get("cpu", 0)
        self.current_deployment["total_bw_consumed"] += resource_consumed.get("bw", 0)
        self.current_deployment["total_mem_consumed"] += resource_consumed.get("mem", 0)

        # 2. è®°å½•èŠ‚ç‚¹çº§èµ„æºä½¿ç”¨ï¼ˆå»é‡é€»è¾‘ï¼‰
        if step_info.get("success") and "vnf_placement" in step_info:
            for node_id, resources in step_info["vnf_placement"].items():
                self.current_deployment["used_nodes"].add(node_id)
                # ç´¯åŠ åŒä¸€èŠ‚ç‚¹çš„èµ„æºï¼ˆå¦‚æœå¤šä¸ªVNFéƒ¨ç½²åœ¨åŒä¸€èŠ‚ç‚¹ï¼‰
                self.current_deployment["node_cpu_usage"][node_id] = \
                    self.current_deployment["node_cpu_usage"].get(node_id, 0) + resources.get("cpu", 0)
                self.current_deployment["node_mem_usage"][node_id] = \
                    self.current_deployment["node_mem_usage"].get(node_id, 0) + resources.get("mem", 0)

        # 3. è®°å½•é“¾è·¯çº§èµ„æºä½¿ç”¨ï¼ˆå»é‡é€»è¾‘ï¼‰
        if step_info.get("success") and "link_usage" in step_info:
            for link, resources in step_info["link_usage"].items():
                self.current_deployment["used_links"].add(link)
                # é“¾è·¯èµ„æºå–æœ€å¤§å€¼ï¼ˆå¤šæ’­å…±äº«å¸¦å®½ç‰¹æ€§ï¼‰
                current_bw = self.current_deployment["link_bw_usage"].get(link, 0)
                self.current_deployment["link_bw_usage"][link] = \
                    max(current_bw, resources.get("bw", 0))

        # 4. è®°å½•è·¯å¾„ä¸ç»“æœ
        if step_info.get("success"):
            self.current_deployment["destinations_connected"] += 1
            self.current_deployment["paths"].append(step_info.get("path", []))
            self.current_deployment["total_hops"] += len(step_info.get("path", [])) - 1
        else:
            self.current_deployment["destinations_failed"] += 1
            self.current_deployment["failed_nodes"].append(step_info.get("destination"))
            self.current_deployment["failure_reasons"].append(
                step_info.get("failure_reason", "unknown")
            )

        # 5. è®°å½•å¤‡ä»½ç­–ç•¥ä½¿ç”¨
        if step_info.get("backup_used"):
            self.current_deployment["backup_used"] = True
            self.current_deployment["backup_levels"].append(
                step_info.get("backup_level", "unknown")
            )

        # 6. è®°å½•èµ„æºåˆ©ç”¨ç‡ï¼ˆä½¿ç”¨å»é‡åçš„å®é™…æ¶ˆè€—ï¼‰
        actual_cpu_used = sum(self.current_deployment["node_cpu_usage"].values())
        actual_mem_used = sum(self.current_deployment["node_mem_usage"].values())
        actual_bw_used = sum(self.current_deployment["link_bw_usage"].values())

        num_used_nodes = len(self.current_deployment["used_nodes"])
        num_used_links = len(self.current_deployment["used_links"])

        # è®¡ç®—åˆ©ç”¨ç‡ï¼ˆåªç»Ÿè®¡ä½¿ç”¨çš„èŠ‚ç‚¹/é“¾è·¯ï¼‰
        if num_used_nodes > 0:
            node_capacity = network_state.get("node_cpu_capacity", 80.0)  # å•èŠ‚ç‚¹å®¹é‡é»˜è®¤å€¼
            cpu_util = actual_cpu_used / (num_used_nodes * node_capacity)
            mem_util = actual_mem_used / (num_used_nodes * network_state.get("node_mem_capacity", 60.0))
        else:
            cpu_util = 0.0
            mem_util = 0.0

        if num_used_links > 0:
            link_capacity = network_state.get("link_bw_capacity", 80.0)
            bw_util = actual_bw_used / (num_used_links * link_capacity)
        else:
            bw_util = 0.0

        self.resource_utilization_history.append({
            "timestamp": datetime.now().isoformat(),
            "request_id": self.current_deployment["request_id"],
            "cpu_utilization": cpu_util,
            "bw_utilization": bw_util,
            "mem_utilization": mem_util,
            "num_used_nodes": num_used_nodes,
            "num_used_links": num_used_links
        })

    def end_deployment(self, network_state: dict):
        """ç»“æŸä¸€æ¬¡éƒ¨ç½²çš„è®°å½•"""
        # è®¡ç®—éƒ¨ç½²æ—¶é—´
        if self.current_deployment["start_time"]:
            elapsed = datetime.now() - self.current_deployment["start_time"]
            self.current_deployment["deployment_time"] = elapsed.total_seconds()

        # åˆ¤æ–­éƒ¨ç½²ç»“æœ
        total_dest = len(self.current_deployment["destinations"])
        connected = self.current_deployment["destinations_connected"]

        if connected == total_dest:
            self.current_deployment["fully_deployed"] = True
            self.global_stats["fully_accepted"] += 1
        elif connected > 0:
            self.current_deployment["partial_deployed"] = True
            self.global_stats["partially_accepted"] += 1
        else:
            self.global_stats["totally_blocked"] += 1

        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self.global_stats["total_requests"] += 1
        self.global_stats["total_cpu_consumed"] += self.current_deployment["total_cpu_consumed"]
        self.global_stats["total_bw_consumed"] += self.current_deployment["total_bw_consumed"]
        self.global_stats["total_mem_consumed"] += self.current_deployment["total_mem_consumed"]

        # è®¡ç®—æœ¬æ¬¡éƒ¨ç½²çš„å®é™…èµ„æºåˆ©ç”¨ç‡ï¼ˆå»é‡åï¼‰
        actual_cpu = sum(self.current_deployment["node_cpu_usage"].values())
        actual_mem = sum(self.current_deployment["node_mem_usage"].values())
        actual_bw = sum(self.current_deployment["link_bw_usage"].values())

        num_nodes = len(self.current_deployment["used_nodes"])
        num_links = len(self.current_deployment["used_links"])

        node_capacity = network_state.get("node_cpu_capacity", 80.0)
        mem_capacity = network_state.get("node_mem_capacity", 60.0)
        link_capacity = network_state.get("link_bw_capacity", 80.0)

        if num_nodes > 0:
            cpu_util = actual_cpu / (num_nodes * node_capacity)
            mem_util = actual_mem / (num_nodes * mem_capacity)
        else:
            cpu_util = 0.0
            mem_util = 0.0

        if num_links > 0:
            bw_util = actual_bw / (num_links * link_capacity)
        else:
            bw_util = 0.0

        self.global_stats["avg_cpu_utilization"].append(cpu_util)
        self.global_stats["avg_bw_utilization"].append(bw_util)
        self.global_stats["avg_mem_utilization"].append(mem_util)

        # è®°å½•å¤±è´¥èŠ‚ç‚¹å’ŒåŸå› 
        for node in self.current_deployment["failed_nodes"]:
            self.global_stats["failed_nodes_count"][node] += 1
        for reason in self.current_deployment["failure_reasons"]:
            self.global_stats["failure_reasons_count"][reason] += 1

        # è®°å½•å¤‡ä»½ç­–ç•¥ä½¿ç”¨
        for level in self.current_deployment["backup_levels"]:
            self.global_stats["backup_usage_count"][level] += 1

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.deployment_history.append(self.current_deployment.copy())

    # ================================================================
    # ğŸ“ˆ è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    # ================================================================

    def compute_statistics(self) -> dict:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        total = self.global_stats["total_requests"]
        if total == 0:
            return {}

        stats = {
            # æ¥å—ç‡
            "full_acceptance_rate": self.global_stats["fully_accepted"] / total,
            "partial_acceptance_rate": self.global_stats["partially_accepted"] / total,
            "blocking_rate": self.global_stats["totally_blocked"] / total,

            # å¹³å‡èµ„æºæ¶ˆè€—ï¼ˆæ¯æ¬¡éƒ¨ç½²ï¼‰
            "avg_cpu_per_deployment": self.global_stats["total_cpu_consumed"] / total,
            "avg_bw_per_deployment": self.global_stats["total_bw_consumed"] / total,
            "avg_mem_per_deployment": self.global_stats["total_mem_consumed"] / total,

            # å¹³å‡èµ„æºåˆ©ç”¨ç‡
            "avg_cpu_utilization": np.mean(self.global_stats["avg_cpu_utilization"]),
            "avg_bw_utilization": np.mean(self.global_stats["avg_bw_utilization"]),
            "avg_mem_utilization": np.mean(self.global_stats["avg_mem_utilization"]),

            # èµ„æºåˆ©ç”¨ç‡æ ‡å‡†å·®
            "std_cpu_utilization": np.std(self.global_stats["avg_cpu_utilization"]),
            "std_bw_utilization": np.std(self.global_stats["avg_bw_utilization"]),
            "std_mem_utilization": np.std(self.global_stats["avg_mem_utilization"]),

            # å¤±è´¥èŠ‚ç‚¹åˆ†æ
            "top_failed_nodes": sorted(
                self.global_stats["failed_nodes_count"].items(),
                key=lambda x: x[1], reverse=True
            )[:10],

            "failure_reasons": dict(self.global_stats["failure_reasons_count"]),

            # å¤‡ä»½ç­–ç•¥ä½¿ç”¨ç»Ÿè®¡
            "backup_usage": dict(self.global_stats["backup_usage_count"]),

            # å¹³å‡è·³æ•°
            "avg_hops": np.mean([
                d["total_hops"] / max(d["destinations_connected"], 1)
                for d in self.deployment_history
            ]),

            # å¹³å‡éƒ¨ç½²æ—¶é—´
            "avg_deployment_time": np.mean([
                d["deployment_time"] for d in self.deployment_history
            ])
        }

        return stats

    # ================================================================
    # ğŸ’¾ CSVå¯¼å‡ºåŠŸèƒ½
    # ================================================================

    def export_deployment_details(self, filename: str = "deployment_details.csv"):
        """å¯¼å‡ºæ¯æ¬¡éƒ¨ç½²çš„è¯¦ç»†ä¿¡æ¯ (åŒ…å«æ—¶é—´æ­¥)"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            # [æ ¸å¿ƒä¿®æ”¹] å¢åŠ  time_step åˆ—
            fieldnames = [
                'time_step', 'request_id', 'vnf_chain', 'total_destinations',
                'destinations_connected', 'destinations_failed',
                'total_cpu_consumed', 'total_bw_consumed', 'total_mem_consumed',
                'actual_cpu_used', 'actual_bw_used', 'actual_mem_used',
                'num_used_nodes', 'num_used_links',
                'avg_cpu_util_per_node', 'avg_bw_util_per_link', 'avg_mem_util_per_node',
                'total_hops', 'avg_hops_per_dest',
                'fully_deployed', 'partial_deployed',
                'backup_used', 'backup_levels',
                'failed_nodes', 'failure_reasons',
                'deployment_time'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()

            for dep in self.deployment_history:
                avg_hops = dep["total_hops"] / max(dep["destinations_connected"], 1)

                # è®¡ç®—å»é‡åçš„å®é™…èµ„æºä½¿ç”¨ (ç”¨äºå¯¼å‡º)
                actual_cpu = sum(dep["node_cpu_usage"].values())
                actual_mem = sum(dep["node_mem_usage"].values())
                actual_bw = sum(dep["link_bw_usage"].values())

                num_nodes = len(dep["used_nodes"])
                num_links = len(dep["used_links"])

                node_capacity = 80.0  # é»˜è®¤å€¼ï¼Œæˆ–è€…ä» self.network_info è¯»å–
                mem_capacity = 60.0
                link_capacity = 80.0

                cpu_util = (actual_cpu / (num_nodes * node_capacity)) if num_nodes > 0 else 0.0
                mem_util = (actual_mem / (num_nodes * mem_capacity)) if num_nodes > 0 else 0.0
                bw_util = (actual_bw / (num_links * link_capacity)) if num_links > 0 else 0.0

                writer.writerow({
                    'time_step': dep.get('time_step', 0),  # [ä¿®æ”¹] å†™å…¥æ—¶é—´æ­¥
                    'request_id': dep['request_id'],
                    'vnf_chain': '->'.join(dep['vnf_chain']),
                    'total_destinations': len(dep['destinations']),
                    'destinations_connected': dep['destinations_connected'],
                    'destinations_failed': dep['destinations_failed'],
                    'total_cpu_consumed': f"{dep['total_cpu_consumed']:.4f}",
                    'total_bw_consumed': f"{dep['total_bw_consumed']:.4f}",
                    'total_mem_consumed': f"{dep['total_mem_consumed']:.4f}",
                    'actual_cpu_used': f"{actual_cpu:.4f}",
                    'actual_bw_used': f"{actual_bw:.4f}",
                    'actual_mem_used': f"{actual_mem:.4f}",
                    'num_used_nodes': num_nodes,
                    'num_used_links': num_links,
                    'avg_cpu_util_per_node': f"{cpu_util:.4f}",
                    'avg_bw_util_per_link': f"{bw_util:.4f}",
                    'avg_mem_util_per_node': f"{mem_util:.4f}",
                    'total_hops': dep['total_hops'],
                    'avg_hops_per_dest': f"{avg_hops:.2f}",
                    'fully_deployed': dep['fully_deployed'],
                    'partial_deployed': dep['partial_deployed'],
                    'backup_used': dep['backup_used'],
                    'backup_levels': ','.join(dep['backup_levels']),
                    'failed_nodes': ','.join(map(str, dep['failed_nodes'])),
                    'failure_reasons': ','.join(dep['failure_reasons']),
                    'deployment_time': f"{dep['deployment_time']:.4f}"
                })

        print(f"âœ… Deployment details exported to {filename}")

    def export_metrics_by_time_interval(self, filename: str = "metrics_by_time.csv", interval: int = 50):
        """
        [æ–°å¢] æŒ‰æ—¶é—´é—´éš”èšåˆæ•°æ® (ç”¨äºç»˜åˆ¶æŸ±çŠ¶å›¾)
        èšåˆé¡¹ï¼šè¯·æ±‚æ•°é‡ã€æ¥å—ç‡ã€CPUæ€»æ¶ˆè€—ã€å¸¦å®½æ€»æ¶ˆè€—
        """
        if not self.deployment_history:
            return

        # æ‰¾å‡ºæœ€å¤§æ—¶é—´æ­¥
        max_t = max(d.get('time_step', 0) for d in self.deployment_history)

        # å‡†å¤‡åˆ†æ¡¶
        stats = defaultdict(lambda: {'total_req': 0, 'full_acc': 0, 'cpu': 0.0, 'bw': 0.0})

        for dep in self.deployment_history:
            t = dep.get('time_step', 0)
            # è®¡ç®—æ¡¶ç´¢å¼•ï¼šä¾‹å¦‚ 1-50 -> 50, 51-100 -> 100
            if t == 0:
                bin_idx = interval
            else:
                bin_idx = ((t - 1) // interval + 1) * interval

            stats[bin_idx]['total_req'] += 1
            if dep['fully_deployed']:
                stats[bin_idx]['full_acc'] += 1

            # ä½¿ç”¨ total_cpu_consumed è¿›è¡Œèšåˆ (æ€»æ¶ˆè€—)
            stats[bin_idx]['cpu'] += dep['total_cpu_consumed']
            stats[bin_idx]['bw'] += dep['total_bw_consumed']

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(
                ['Time_Interval', 'Request_Count', 'Acceptance_Rate', 'Total_CPU_Consumed', 'Total_BW_Consumed'])

            for b in sorted(stats.keys()):
                d = stats[b]
                acc_rate = d['full_acc'] / d['total_req'] if d['total_req'] > 0 else 0
                writer.writerow([
                    b,
                    d['total_req'],
                    f"{acc_rate:.2%}",
                    f"{d['cpu']:.4f}",
                    f"{d['bw']:.4f}"
                ])

        print(f"âœ… Aggregated time metrics exported to {filename}")

    def export_resource_utilization(self, filename: str = "resource_utilization.csv"):
        """å¯¼å‡ºèµ„æºåˆ©ç”¨ç‡æ—¶åºæ•°æ®"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'timestamp', 'request_id',
                'cpu_utilization', 'bw_utilization', 'mem_utilization',
                'num_used_nodes', 'num_used_links'
            ])

            writer.writeheader()

            for record in self.resource_utilization_history:
                writer.writerow({
                    'timestamp': record['timestamp'],
                    'request_id': record['request_id'],
                    'cpu_utilization': f"{record['cpu_utilization']:.4f}",
                    'bw_utilization': f"{record['bw_utilization']:.4f}",
                    'mem_utilization': f"{record['mem_utilization']:.4f}",
                    'num_used_nodes': record.get('num_used_nodes', 0),
                    'num_used_links': record.get('num_used_links', 0)
                })

        print(f"âœ… Resource utilization exported to {filename}")

    def export_summary_statistics(self, filename: str = "summary_statistics.csv"):
        """å¯¼å‡ºæ±‡æ€»ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.compute_statistics()

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Metric', 'Value'])

            # æ¥å—ç‡å’Œé˜»å¡ç‡
            writer.writerow(['Total Requests', self.global_stats['total_requests']])
            writer.writerow(['Full Acceptance Rate', f"{stats['full_acceptance_rate']:.2%}"])
            writer.writerow(['Partial Acceptance Rate', f"{stats['partial_acceptance_rate']:.2%}"])
            writer.writerow(['Blocking Rate', f"{stats['blocking_rate']:.2%}"])
            writer.writerow([''])

            # èµ„æºæ¶ˆè€—
            writer.writerow(['Avg CPU per Deployment', f"{stats['avg_cpu_per_deployment']:.4f}"])
            writer.writerow(['Avg BW per Deployment', f"{stats['avg_bw_per_deployment']:.4f}"])
            writer.writerow(['Avg MEM per Deployment', f"{stats['avg_mem_per_deployment']:.4f}"])
            writer.writerow([''])

            # èµ„æºåˆ©ç”¨ç‡
            writer.writerow(['Avg CPU Utilization', f"{stats['avg_cpu_utilization']:.2%}"])
            writer.writerow(['Avg BW Utilization', f"{stats['avg_bw_utilization']:.2%}"])
            writer.writerow(['Avg MEM Utilization', f"{stats['avg_mem_utilization']:.2%}"])
            writer.writerow(['Std CPU Utilization', f"{stats['std_cpu_utilization']:.4f}"])
            writer.writerow(['Std BW Utilization', f"{stats['std_bw_utilization']:.4f}"])
            writer.writerow(['Std MEM Utilization', f"{stats['std_mem_utilization']:.4f}"])
            writer.writerow([''])

            # å…¶ä»–æŒ‡æ ‡
            writer.writerow(['Avg Hops per Destination', f"{stats['avg_hops']:.2f}"])
            writer.writerow(['Avg Deployment Time (s)', f"{stats['avg_deployment_time']:.4f}"])
            writer.writerow([''])

            # å¤±è´¥èŠ‚ç‚¹TOP 10
            writer.writerow(['Top Failed Nodes', ''])
            for node, count in stats['top_failed_nodes']:
                writer.writerow([f'  Node {node}', count])
            writer.writerow([''])

            # å¤±è´¥åŸå› ç»Ÿè®¡
            writer.writerow(['Failure Reasons', ''])
            for reason, count in stats['failure_reasons'].items():
                writer.writerow([f'  {reason}', count])
            writer.writerow([''])

            # å¤‡ä»½ç­–ç•¥ä½¿ç”¨
            writer.writerow(['Backup Policy Usage', ''])
            for level, count in stats['backup_usage'].items():
                writer.writerow([f'  {level}', count])

        print(f"âœ… Summary statistics exported to {filename}")

    def export_failed_nodes_analysis(self, filename: str = "failed_nodes_analysis.csv"):
        """å¯¼å‡ºå¤±è´¥èŠ‚ç‚¹çš„è¯¦ç»†åˆ†æ"""
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'node_id', 'failure_count', 'failure_rate',
                'main_failure_reasons'
            ])

            writer.writeheader()

            # åˆ†ææ¯ä¸ªå¤±è´¥èŠ‚ç‚¹
            node_reasons = defaultdict(lambda: defaultdict(int))
            for dep in self.deployment_history:
                for node, reason in zip(dep['failed_nodes'], dep['failure_reasons']):
                    node_reasons[node][reason] += 1

            total_attempts = len(self.deployment_history)

            for node, count in self.global_stats["failed_nodes_count"].items():
                reasons = node_reasons[node]
                main_reasons = sorted(reasons.items(), key=lambda x: x[1], reverse=True)

                writer.writerow({
                    'node_id': node,
                    'failure_count': count,
                    'failure_rate': f"{count / total_attempts:.2%}",
                    'main_failure_reasons': '; '.join([
                        f"{r}({c})" for r, c in main_reasons[:3]
                    ])
                })

        print(f"âœ… Failed nodes analysis exported to {filename}")

    def export_all(self, prefix: str = "vnf_metrics"):
        """ä¸€é”®å¯¼å‡ºæ‰€æœ‰CSVæ–‡ä»¶"""
        self.export_deployment_details(f"{prefix}_deployment_details.csv")
        self.export_resource_utilization(f"{prefix}_resource_utilization.csv")
        self.export_summary_statistics(f"{prefix}_summary_statistics.csv")
        self.export_failed_nodes_analysis(f"{prefix}_failed_nodes_analysis.csv")
        # [ä¿®æ”¹] å¯¼å‡ºæŒ‰æ—¶é—´é—´éš”èšåˆçš„æ•°æ®
        self.export_metrics_by_time_interval(f"{prefix}_metrics_by_time_interval.csv", interval=50)

        print(f"\nğŸ‰ All metrics exported with prefix: {prefix}")

    # ================================================================
    # ğŸ“Š å®æ—¶ç›‘æ§æ¥å£
    # ================================================================

    def get_realtime_stats(self) -> dict:
        """è·å–å®æ—¶ç»Ÿè®¡ï¼ˆç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§ï¼‰"""
        if len(self.deployment_history) == 0:
            return {}

        recent_window = min(100, len(self.deployment_history))
        recent_deps = self.deployment_history[-recent_window:]

        fully_deployed = sum(1 for d in recent_deps if d['fully_deployed'])
        partially_deployed = sum(1 for d in recent_deps if d['partial_deployed'])

        return {
            "recent_full_acceptance": fully_deployed / recent_window,
            "recent_partial_acceptance": partially_deployed / recent_window,
            "recent_blocking": 1 - (fully_deployed + partially_deployed) / recent_window,
            "recent_avg_cpu_util": np.mean(self.global_stats["avg_cpu_utilization"][-recent_window:]),
            "recent_backup_usage": sum(1 for d in recent_deps if d['backup_used']) / recent_window
        }

    def print_summary(self):
        """æ‰“å°æ±‡æ€»ä¿¡æ¯åˆ°æ§åˆ¶å°"""
        stats = self.compute_statistics()

        print("\n" + "=" * 60)
        print("ğŸ“Š VNF DEPLOYMENT METRICS SUMMARY")
        print("=" * 60)
        print(f"Total Requests: {self.global_stats['total_requests']}")
        print(f"Full Acceptance Rate: {stats['full_acceptance_rate']:.2%}")
        print(f"Partial Acceptance Rate: {stats['partial_acceptance_rate']:.2%}")
        print(f"Blocking Rate: {stats['blocking_rate']:.2%}")
        print("-" * 60)
        print(f"Avg CPU Utilization: {stats['avg_cpu_utilization']:.2%}")
        print(f"Avg BW Utilization: {stats['avg_bw_utilization']:.2%}")
        print(f"Avg MEM Utilization: {stats['avg_mem_utilization']:.2%}")
        print("-" * 60)
        print(f"Avg Hops per Destination: {stats['avg_hops']:.2f}")
        print(f"Avg Deployment Time: {stats['avg_deployment_time']:.4f}s")
        print("=" * 60 + "\n")


if __name__ == "__main__":
    # ç®€å•çš„æµ‹è¯•æ¡©ï¼ŒéªŒè¯ä»£ç æ˜¯å¦å¯è¿è¡Œ
    print("VNFMetricsLogger is ready.")