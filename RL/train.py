# -*- coding: utf-8 -*-
# @File     : train_hierarchical.py
# @Author   : AI Assistant
# @Desc     : Training script for Hierarchical RL in SFC Mapping (å®Œæ•´ä¿®å¤ç‰ˆ)

import sys
import time
import pickle
import logging
from pathlib import Path
from collections import deque
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import xml.etree.ElementTree as ET
import networkx as nx

# å¯¼å…¥ä¿®å¤åçš„æ¨¡å—
from config import Config
from env import SFCEnv
from log import MyLog


# ==================== åˆ†å±‚DQN Agentå®ç° ====================
class GoalNetwork(nn.Module):
    """Meta-Controllerï¼šé€‰æ‹©æŠ½è±¡ç›®æ ‡ï¼ˆGoalï¼‰"""
    def __init__(self, state_dim, goal_dim, hidden_dim=256):
        super(GoalNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, goal_dim)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        goal = torch.tanh(self.fc3(x))  # é™åˆ¶goalåœ¨[-1, 1]
        return goal


class SubPolicyNetwork(nn.Module):
    """Sub-Controllerï¼šæ ¹æ®goalé€‰æ‹©å…·ä½“åŠ¨ä½œ"""
    def __init__(self, state_dim, goal_dim, action_dim, hidden_dim=256):
        super(SubPolicyNetwork, self).__init__()
        # æ‹¼æ¥stateå’Œgoal
        input_dim = state_dim + goal_dim
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, state, goal):
        # æ‹¼æ¥stateå’Œgoal
        x = torch.cat([state, goal], dim=-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        q_values = self.fc3(x)
        return q_values


class HierarchicalDQNAgent:
    """
    åˆ†å±‚DQNä»£ç†
    - Meta-Controllerï¼šé€‰æ‹©ç›®æ ‡ï¼ˆgoalï¼‰
    - Sub-Controllerï¼šæ ¹æ®goalé€‰æ‹©åŠ¨ä½œ
    """
    def __init__(self, config, num_nodes):
        self.config = config
        self.num_nodes = num_nodes
        self.device = config.DEVICE
        
        # çŠ¶æ€ç»´åº¦ï¼ˆç®€åŒ–ä¸ºèŠ‚ç‚¹æ•°ï¼‰
        self.state_dim = num_nodes
        self.goal_dim = 32  # goalçš„ç»´åº¦
        self.action_dim = num_nodes
        
        # Meta-Controllerï¼ˆé€‰æ‹©goalï¼‰
        self.goal_net = GoalNetwork(self.state_dim, self.goal_dim).to(self.device)
        self.goal_target_net = GoalNetwork(self.state_dim, self.goal_dim).to(self.device)
        self.goal_target_net.load_state_dict(self.goal_net.state_dict())
        self.goal_optimizer = optim.Adam(self.goal_net.parameters(), lr=config.LR)
        
        # Sub-Controllerï¼ˆæ ¹æ®goalé€‰æ‹©åŠ¨ä½œï¼‰
        self.sub_net = SubPolicyNetwork(self.state_dim, self.goal_dim, self.action_dim).to(self.device)
        self.sub_target_net = SubPolicyNetwork(self.state_dim, self.goal_dim, self.action_dim).to(self.device)
        self.sub_target_net.load_state_dict(self.sub_net.state_dict())
        self.sub_optimizer = optim.Adam(self.sub_net.parameters(), lr=config.LR)
        
        # ç»éªŒå›æ”¾ç¼“å†²
        self.meta_buffer = deque(maxlen=10000)
        self.sub_buffer = deque(maxlen=50000)
        
        # è®­ç»ƒè®¡æ•°
        self.meta_step = 0
        self.sub_step = 0
        
        # è¶…å‚æ•°
        self.gamma = config.DISCOUNT
        self.batch_size = config.BATCH_SIZE
        self.tau = config.TAU
        
        self.is_training = True
        
    def extract_state_features(self, state_tensor):
        """
        ä»SFCEnvçš„çŠ¶æ€å¼ é‡ä¸­æå–ç‰¹å¾å‘é‡
        è¾“å…¥: (1, 7, N, N)
        è¾“å‡º: (state_dim,) å³ (N,)
        """
        # ç®€åŒ–ï¼šå¯¹æ¯ä¸ªèŠ‚ç‚¹è®¡ç®—å¹³å‡ç‰¹å¾
        # state_tensor: (1, 7, N, N)
        if state_tensor.dim() == 4:
            state_tensor = state_tensor.squeeze(0)  # (7, N, N)
        
        # æå–å¯¹è§’çº¿å…ƒç´ ï¼ˆèŠ‚ç‚¹è‡ªèº«ç‰¹å¾ï¼‰
        # é€šé“0: CPU, é€šé“3: éƒ¨ç½²çŠ¶æ€, é€šé“6: è¿›åº¦
        cpu_features = state_tensor[0].diag()  # (N,)
        deploy_features = state_tensor[3].diag()
        progress_features = state_tensor[6].diag()
        
        # ç»„åˆç‰¹å¾ï¼ˆå¯ä»¥æ›´å¤æ‚ï¼‰
        node_features = (cpu_features + deploy_features + progress_features) / 3.0
        
        return node_features.to(self.device)
    
    def select_goal(self, state, epsilon=0.1):
        """Meta-Controlleré€‰æ‹©goal"""
        if np.random.rand() < epsilon and self.is_training:
            # éšæœºæ¢ç´¢
            goal = torch.randn(self.goal_dim).to(self.device)
        else:
            # ç­–ç•¥é€‰æ‹©
            with torch.no_grad():
                state_features = self.extract_state_features(state).unsqueeze(0)  # (1, N)
                goal = self.goal_net(state_features).squeeze(0)  # (goal_dim,)
        
        return goal
    
    def select_action(self, state, goal, epsilon, valid_actions):
        """Sub-Controlleræ ¹æ®goalé€‰æ‹©åŠ¨ä½œ"""
        if np.random.rand() < epsilon and self.is_training:
            # ä»æœ‰æ•ˆåŠ¨ä½œä¸­éšæœºé€‰æ‹©
            return np.random.choice(valid_actions)
        else:
            with torch.no_grad():
                state_features = self.extract_state_features(state).unsqueeze(0)  # (1, N)
                goal_tensor = goal.unsqueeze(0)  # (1, goal_dim)
                q_values = self.sub_net(state_features, goal_tensor).squeeze(0)  # (action_dim,)
                
                # åªè€ƒè™‘æœ‰æ•ˆåŠ¨ä½œ
                valid_q_values = {a: q_values[a].item() for a in valid_actions}
                action = max(valid_q_values, key=valid_q_values.get)
        
        return action
    
    def compute_intrinsic_reward(self, goal, action, next_state):
        """
        è®¡ç®—å†…åœ¨å¥–åŠ±ï¼ˆgoalè¾¾æˆåº¦ï¼‰
        ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡next_stateä¸goalçš„æ¥è¿‘ç¨‹åº¦
        """
        with torch.no_grad():
            next_features = self.extract_state_features(next_state)  # (N,)
            
            # å°†goalæŠ•å½±åˆ°çŠ¶æ€ç©ºé—´ï¼ˆç®€åŒ–æ–¹æ³•ï¼‰
            goal_projection = goal[:self.state_dim]  # å–å‰Nç»´
            
            # ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = F.cosine_similarity(
                next_features.unsqueeze(0),
                goal_projection.unsqueeze(0),
                dim=1
            ).item()
            
            intrinsic_reward = similarity * 0.5  # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
        
        return intrinsic_reward
    
    def push_meta(self, state, goal, reward, next_state, done):
        """å­˜å‚¨Meta-level transition"""
        state_features = self.extract_state_features(state)
        next_features = self.extract_state_features(next_state) if next_state is not None else None
        
        self.meta_buffer.append({
            'state': state_features.cpu(),
            'goal': goal.cpu(),
            'reward': reward,
            'next_state': next_features.cpu() if next_features is not None else None,
            'done': done
        })
    
    def push_sub(self, state, goal, action, reward, next_state, done):
        """å­˜å‚¨Sub-level transition"""
        state_features = self.extract_state_features(state)
        next_features = self.extract_state_features(next_state) if next_state is not None else None
        
        self.sub_buffer.append({
            'state': state_features.cpu(),
            'goal': goal.cpu(),
            'action': action,
            'reward': reward,
            'next_state': next_features.cpu() if next_features is not None else None,
            'done': done
        })
    
    def train_meta(self):
        """è®­ç»ƒMeta-Controller"""
        if len(self.meta_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·batch
        indices = np.random.choice(len(self.meta_buffer), self.batch_size, replace=False)
        batch = [self.meta_buffer[i] for i in indices]
        
        states = torch.stack([b['state'] for b in batch]).to(self.device)
        goals = torch.stack([b['goal'] for b in batch]).to(self.device)
        rewards = torch.tensor([b['reward'] for b in batch], dtype=torch.float32).to(self.device)
        next_states = torch.stack([b['next_state'] for b in batch if b['next_state'] is not None]).to(self.device)
        dones = torch.tensor([b['done'] for b in batch], dtype=torch.float32).to(self.device)
        
        # å¤„ç†ç»ˆæ­¢çŠ¶æ€
        non_final_mask = torch.tensor([b['next_state'] is not None for b in batch], dtype=torch.bool)
        
        # å½“å‰goalçš„Qå€¼ï¼ˆç®€åŒ–ï¼šä½¿ç”¨goalä¸stateçš„ç›¸ä¼¼åº¦ä½œä¸ºQå€¼ï¼‰
        current_q = torch.sum(states * goals[:, :self.state_dim], dim=1)
        
        # ç›®æ ‡Qå€¼
        next_q = torch.zeros(self.batch_size).to(self.device)
        if non_final_mask.sum() > 0:
            with torch.no_grad():
                next_goals = self.goal_target_net(next_states)
                next_q[non_final_mask] = torch.sum(next_states * next_goals[:, :self.state_dim], dim=1)
        
        target_q = rewards + (1 - dones) * self.gamma * next_q
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q, target_q)
        
        # ä¼˜åŒ–
        self.goal_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.goal_net.parameters(), 1.0)
        self.goal_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update(self.goal_net, self.goal_target_net)
        
        self.meta_step += 1
        return loss.item()
    
    def train_sub(self):
        """è®­ç»ƒSub-Controller"""
        if len(self.sub_buffer) < self.batch_size:
            return None
        
        # é‡‡æ ·batch
        indices = np.random.choice(len(self.sub_buffer), self.batch_size, replace=False)
        batch = [self.sub_buffer[i] for i in indices]
        
        states = torch.stack([b['state'] for b in batch]).to(self.device)
        goals = torch.stack([b['goal'] for b in batch]).to(self.device)
        actions = torch.tensor([b['action'] for b in batch], dtype=torch.long).to(self.device)
        rewards = torch.tensor([b['reward'] for b in batch], dtype=torch.float32).to(self.device)
        next_states = torch.stack([b['next_state'] for b in batch if b['next_state'] is not None]).to(self.device)
        dones = torch.tensor([b['done'] for b in batch], dtype=torch.float32).to(self.device)
        
        non_final_mask = torch.tensor([b['next_state'] is not None for b in batch], dtype=torch.bool)
        
        # å½“å‰Qå€¼
        current_q = self.sub_net(states, goals).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        # ç›®æ ‡Qå€¼ï¼ˆDDQNï¼‰
        next_q = torch.zeros(self.batch_size).to(self.device)
        if non_final_mask.sum() > 0:
            with torch.no_grad():
                # ä½¿ç”¨å½“å‰ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
                next_actions = self.sub_net(next_states, goals[non_final_mask]).argmax(1)
                # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°
                next_q[non_final_mask] = self.sub_target_net(
                    next_states, goals[non_final_mask]
                ).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        
        target_q = rewards + (1 - dones) * self.gamma * next_q
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q, target_q)
        
        # ä¼˜åŒ–
        self.sub_optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.sub_net.parameters(), 1.0)
        self.sub_optimizer.step()
        
        # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self._soft_update(self.sub_net, self.sub_target_net)
        
        self.sub_step += 1
        return loss.item()
    
    def _soft_update(self, source_net, target_net):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        for target_param, source_param in zip(target_net.parameters(), source_net.parameters()):
            target_param.data.copy_(
                self.tau * source_param.data + (1.0 - self.tau) * target_param.data
            )
    
    def save(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'goal_net': self.goal_net.state_dict(),
            'goal_target_net': self.goal_target_net.state_dict(),
            'sub_net': self.sub_net.state_dict(),
            'sub_target_net': self.sub_target_net.state_dict(),
            'goal_optimizer': self.goal_optimizer.state_dict(),
            'sub_optimizer': self.sub_optimizer.state_dict(),
            'meta_step': self.meta_step,
            'sub_step': self.sub_step,
        }, path)
    
    def load(self, path):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)
        self.goal_net.load_state_dict(checkpoint['goal_net'])
        self.goal_target_net.load_state_dict(checkpoint['goal_target_net'])
        self.sub_net.load_state_dict(checkpoint['sub_net'])
        self.sub_target_net.load_state_dict(checkpoint['sub_target_net'])
        self.goal_optimizer.load_state_dict(checkpoint['goal_optimizer'])
        self.sub_optimizer.load_state_dict(checkpoint['sub_optimizer'])
        self.meta_step = checkpoint.get('meta_step', 0)
        self.sub_step = checkpoint.get('sub_step', 0)
    
    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.is_training = False
        self.goal_net.eval()
        self.sub_net.eval()
    
    def train_mode(self):
        """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.is_training = True
        self.goal_net.train()
        self.sub_net.train()


# ==================== è®­ç»ƒå™¨ç±» ====================
class HierarchicalSFCTrainer:
    """åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, config: Config, env: SFCEnv, name: str = "HRL_SFC"):
        self.config = config
        self.env = env
        self.name = name

        # æ—¥å¿—ç³»ç»Ÿ
        self.mylog = MyLog(Path(__file__), filesave=True, consoleprint=True, name=name)
        self.logger = self.mylog.logger

        # åˆ›å»ºåˆ†å±‚ä»£ç†
        self.agent = HierarchicalDQNAgent(config, num_nodes=len(env.nodes))

        # TensorBoard
        self.writer = SummaryWriter(f"./runs/HRL_{name}")

        # è®­ç»ƒå‚æ•°
        self.meta_epsilon_start = 1.0
        self.meta_epsilon_end = 0.1
        self.meta_epsilon_decay = config.E_GREEDY_EPISODES // 2

        self.sub_epsilon_start = 1.0
        self.sub_epsilon_end = 0.01
        self.sub_epsilon_decay = config.E_GREEDY_EPISODES

        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards = []
        self.meta_losses = []
        self.sub_losses = []

        # GoalæŒç»­æ—¶é—´
        self.goal_horizon = 3

    def get_epsilon(self, episode: int, is_meta: bool = False) -> float:
        """è®¡ç®—epsilon"""
        if is_meta:
            start, end, decay = self.meta_epsilon_start, self.meta_epsilon_end, self.meta_epsilon_decay
        else:
            start, end, decay = self.sub_epsilon_start, self.sub_epsilon_end, self.sub_epsilon_decay

        if episode >= decay:
            return end

        epsilon = end + (start - end) * np.exp(-episode / decay)
        return epsilon

    def train_episode(self, episode: int, pkl_path: str = None):
        """è®­ç»ƒå•ä¸ªepisode"""
        # é‡ç½®ç¯å¢ƒ
        if pkl_path is not None:
            self.env.read_pickle_and_modify(pkl_path)

        state, _ = self.env.reset()
        episode_reward = 0
        step_count = 0

        # Episodeçº§åˆ«çš„goal
        current_goal = None
        goal_start_vnf = 0

        # Meta-levelçŠ¶æ€
        meta_state = state.clone()
        meta_cumulative_reward = 0

        # è®­ç»ƒå¾ªç¯
        while True:
            # Meta-levelå†³ç­–
            if current_goal is None or (self.env.current_vnf_index - goal_start_vnf) >= self.goal_horizon:
                # ä¿å­˜ä¸Šä¸€ä¸ªmeta transition
                if current_goal is not None:
                    self.agent.push_meta(
                        meta_state,
                        current_goal,
                        meta_cumulative_reward,
                        state.clone(),
                        False
                    )

                # è®­ç»ƒMeta-Controller
                meta_loss = self.agent.train_meta()
                if meta_loss is not None:
                    self.meta_losses.append(meta_loss)
                    self.writer.add_scalar('Loss/Meta', meta_loss, self.agent.meta_step)

                # é€‰æ‹©æ–°goal
                meta_epsilon = self.get_epsilon(episode, is_meta=True)
                current_goal = self.agent.select_goal(state, epsilon=meta_epsilon)

                # é‡ç½®metaçŠ¶æ€
                meta_state = state.clone()
                meta_cumulative_reward = 0
                goal_start_vnf = self.env.current_vnf_index

            # Sub-levelå†³ç­–
            sub_epsilon = self.get_epsilon(episode, is_meta=False)
            valid_actions = self.env.get_valid_actions()

            if not valid_actions:
                self.logger.warning(f"Episode {episode}: No valid actions at step {step_count}")
                break

            action = self.agent.select_action(state, current_goal, sub_epsilon, valid_actions)

            # ç¯å¢ƒäº¤äº’
            next_state, extrinsic_reward, done, info = self.env.step(action)

            # è®¡ç®—å†…åœ¨å¥–åŠ±
            intrinsic_reward = self.agent.compute_intrinsic_reward(current_goal, action, next_state)

            # ä¿å­˜sub transition
            self.agent.push_sub(
                state.clone(),
                current_goal.clone(),
                action,
                intrinsic_reward,
                next_state.clone() if not done else None,
                done
            )

            # è®­ç»ƒSub-Controller
            sub_loss = self.agent.train_sub()
            if sub_loss is not None:
                self.sub_losses.append(sub_loss)
                self.writer.add_scalar('Loss/Sub', sub_loss, self.agent.sub_step)

            # æ›´æ–°ç»Ÿè®¡
            episode_reward += extrinsic_reward
            meta_cumulative_reward += extrinsic_reward
            step_count += 1

            # æ—¥å¿—
            if step_count % 5 == 0:
                self.logger.info(
                    f"[Ep {episode}][Step {step_count}] "
                    f"VNF {self.env.current_vnf_index}/{len(self.env.vnfs)} | "
                    f"Reward: {extrinsic_reward:.4f} | "
                    f"Intrinsic: {intrinsic_reward:.4f} | "
                    f"Node: {action} | "
                    f"Tree: {len(self.env.tree_nodes)} nodes"
                )

            # æ£€æŸ¥ç»ˆæ­¢
            if done:
                # ä¿å­˜æœ€åçš„meta transition
                self.agent.push_meta(
                    meta_state,
                    current_goal,
                    meta_cumulative_reward,
                    next_state.clone(),
                    True
                )

                meta_loss = self.agent.train_meta()
                if meta_loss is not None:
                    self.meta_losses.append(meta_loss)

                self.logger.info(
                    f"[Episode {episode} Done] "
                    f"Reward: {episode_reward:.4f} | "
                    f"Steps: {step_count} | "
                    f"Status: {info.get('termination', 'unknown')}"
                )
                break

            state = next_state

        return episode_reward, step_count

    def train(self, num_episodes: int = 2000):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        start_time = time.time()

        # è·å–pklæ–‡ä»¶åˆ—è¡¨
        pkl_files = list(self._get_pkl_files())
        if not pkl_files:
            self.logger.warning("æœªæ‰¾åˆ°pklæ–‡ä»¶ï¼Œä½¿ç”¨é™æ€ç¯å¢ƒ")
            pkl_files = [None] * num_episodes

        for episode in range(num_episodes):
            pkl_path = pkl_files[episode % len(pkl_files)]

            # è®­ç»ƒä¸€ä¸ªepisode
            episode_reward, steps = self.train_episode(episode, pkl_path)

            # è®°å½•
            self.episode_rewards.append(episode_reward)
            self.writer.add_scalar('Reward/Episode', episode_reward, episode)
            self.writer.add_scalar('Steps/Episode', steps, episode)
            self.writer.add_scalar('Epsilon/Meta', self.get_epsilon(episode, True), episode)
            self.writer.add_scalar('Epsilon/Sub', self.get_epsilon(episode, False), episode)

            # å®šæœŸä¿å­˜
            if episode % 100 == 0 and episode > 0:
                self.save_checkpoint(episode)
                self._log_statistics(episode)

            # å®šæœŸè¯„ä¼°
            if episode % 500 == 0 and episode > 0:
                self.evaluate(num_eval_episodes=10)

        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint('final')
        self._log_statistics(num_episodes)

        elapsed_time = time.time() - start_time
        self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {elapsed_time / 3600:.2f} å°æ—¶")

    def evaluate(self, num_eval_episodes: int = 10):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        self.logger.info(f"å¼€å§‹è¯„ä¼° {num_eval_episodes} episodes...")

        eval_rewards = []
        eval_success = []
        eval_qos = {'bw': [], 'delay': [], 'loss': [], 'length': []}

        self.agent.eval()

        for ep in range(num_eval_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            current_goal = None
            goal_start_vnf = 0

            while True:
                # Meta-levelå†³ç­–ï¼ˆè´ªå©ªï¼‰
                if current_goal is None or (self.env.current_vnf_index - goal_start_vnf) >= self.goal_horizon:
                    current_goal = self.agent.select_goal(state, epsilon=0.0)
                    goal_start_vnf = self.env.current_vnf_index

                # Sub-levelå†³ç­–ï¼ˆè´ªå©ªï¼‰
                valid_actions = self.env.get_valid_actions()
                if not valid_actions:
                    break

                action = self.agent.select_action(state, current_goal, epsilon=0.0, valid_actions=valid_actions)
                next_state, reward, done, info = self.env.step(action)

                episode_reward += reward

                if done:
                    if info.get('termination') == 'sfc_completed':
                        eval_success.append(1)
                        bw, delay, loss, length = self.env.get_sfc_qos_params()
                        eval_qos['bw'].append(bw)
                        eval_qos['delay'].append(delay)
                        eval_qos['loss'].append(loss)
                        eval_qos['length'].append(length)
                    else:
                        eval_success.append(0)
                        eval_qos['bw'].append(0.0)
                        eval_qos['delay'].append(1.0)
                        eval_qos['loss'].append(1.0)
                        eval_qos['length'].append(0)
                    break

                state = next_state

            eval_rewards.append(episode_reward)

        self.agent.train_mode()

        # ç»Ÿè®¡ç»“æœ
        avg_reward = np.mean(eval_rewards)
        success_rate = np.mean(eval_success)
        avg_bw = np.mean(eval_qos['bw'])
        avg_delay = np.mean(eval_qos['delay'])
        avg_loss = np.mean(eval_qos['loss'])
        avg_length = np.mean(eval_qos['length'])

        self.logger.info("=" * 60)
        self.logger.info(f"è¯„ä¼°ç»“æœ ({num_eval_episodes} episodes):")
        self.logger.info(f"  å¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        self.logger.info(f"  æˆåŠŸç‡: {success_rate * 100:.2f}%")
        self.logger.info(f"  å¹³å‡å¸¦å®½: {avg_bw:.4f}")
        self.logger.info(f"  å¹³å‡å»¶è¿Ÿ: {avg_delay:.4f}")
        self.logger.info(f"  å¹³å‡ä¸¢åŒ…: {avg_loss:.6f}")
        self.logger.info(f"  å¹³å‡é•¿åº¦: {avg_length:.2f}")
        self.logger.info("=" * 60)

        return avg_reward, success_rate

    def save_checkpoint(self, episode):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path('./checkpoints') / self.name
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        checkpoint_path = checkpoint_dir / f'hrl_episode_{episode}.pt'
        self.agent.save(str(checkpoint_path))

        stats = {
            'episode_rewards': self.episode_rewards,
            'meta_losses': self.meta_losses,
            'sub_losses': self.sub_losses,
        }
        stats_path = checkpoint_dir / f'stats_episode_{episode}.pkl'
        with open(stats_path, 'wb') as f:
            pickle.dump(stats, f)

        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.agent.load(checkpoint_path)

        stats_path = Path(checkpoint_path).parent / f"stats_{Path(checkpoint_path).stem}.pkl"
        if stats_path.exists():
            with open(stats_path, 'rb') as f:
                stats = pickle.load(f)
                self.episode_rewards = stats.get('episode_rewards', [])
                self.meta_losses = stats.get('meta_losses', [])
                self.sub_losses = stats.get('sub_losses', [])
            self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²åŠ è½½: {stats_path}")

    def _log_statistics(self, episode: int):
        """è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.episode_rewards) == 0:
            return

        recent_rewards = self.episode_rewards[-100:] if len(self.episode_rewards) >= 100 else self.episode_rewards
        avg_reward = np.mean(recent_rewards)

        self.logger.info("=" * 60)
        self.logger.info(f"è®­ç»ƒç»Ÿè®¡ (Episode {episode}):")
        self.logger.info(f"  è¿‘100å›åˆå¹³å‡å¥–åŠ±: {avg_reward:.4f}")
        self.logger.info(f"  æ€»å›åˆæ•°: {len(self.episode_rewards)}")
        self.logger.info(f"  Metaç¼“å†²åŒºå¤§å°: {len(self.agent.meta_buffer)}")
        self.logger.info(f"  Subç¼“å†²åŒºå¤§å°: {len(self.agent.sub_buffer)}")
        self.logger.info(f"  Metaè®­ç»ƒæ­¥æ•°: {self.agent.meta_step}")
        self.logger.info(f"  Subè®­ç»ƒæ­¥æ•°: {self.agent.sub_step}")
        if self.meta_losses:
            self.logger.info(f"  è¿‘æœŸMetaæŸå¤±: {np.mean(self.meta_losses[-100:]):.6f}")
        if self.sub_losses:
            self.logger.info(f"  è¿‘æœŸSubæŸå¤±: {np.mean(self.sub_losses[-100:]):.6f}")
        self.logger.info("=" * 60)

    def _get_pkl_files(self):
        """è·å–pklæ–‡ä»¶åˆ—è¡¨"""
        pkl_dir = Path(self.config.pkl_weight_path)
        if not pkl_dir.exists():
            return []

        pkl_files = list(pkl_dir.glob('*.pkl'))
        return sorted(pkl_files)[:self.config.PKL_CUT_NUM]


# ==================== ä¸»å‡½æ•° ====================
def parse_xml_topology(xml_path):
    """è§£æXMLæ‹“æ‰‘æ–‡ä»¶"""
    if not Path(xml_path).exists():
        # åˆ›å»ºé»˜è®¤æ‹“æ‰‘
        print(f"è­¦å‘Š: '{xml_path}' ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ‹“æ‰‘")
        graph = nx.Graph()
        for i in range(1, 6):
            graph.add_node(i, cpu=100.0, cpu_total=100.0)
        
        links = [(1, 2), (2, 3), (3, 4), (4, 5), (1, 5), (2, 4)]
        for u, v in links:
            graph.add_edge(u, v, bandwidth=1000.0, delay=10.0, loss=0.01)
        
        return graph
    
    tree = ET.parse(xml_path)
    root = tree.getroot()
    topo_element = root.find("topology")
    graph = nx.Graph()

    for child in topo_element.iter():
        if child.tag == 'node':
            node_id = int(child.get('id'))
            cpu_total = float(child.get('cpu_total', 100.0))
            graph.add_node(node_id, cpu=cpu_total, cpu_total=cpu_total)
        elif child.tag == 'link':
            from_node = int(child.find('from').get('node'))
            to_node = int(child.find('to').get('node'))
            graph.add_edge(from_node, to_node,
                         bandwidth=float(child.get('bw', 1000.0)),
                         delay=float(child.get('delay', 10.0)),
                         loss=float(child.get('loss', 0.01)))

    return graph


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Hierarchical RL for SFC Mapping")
    parser.add_argument('--mode', type=str, default='train', 
                       choices=['train', 'eval', 'test'],
                       help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--episodes', type=int, default=2000, 
                       help='è®­ç»ƒå›åˆæ•°')
    parser.add_argument('--checkpoint', type=str, default=None, 
                       help='æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--name', type=str, default='HRL_SFC', 
                       help='å®éªŒåç§°')
    parser.add_argument('--goal_horizon', type=int, default=3, 
                       help='GoalæŒç»­æ—¶é—´ï¼ˆæ¯ä¸ªgoalç”¨äºå¤šå°‘ä¸ªVNFï¼‰')
    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("\n" + "=" * 70)
    print("  åˆ†å±‚å¼ºåŒ–å­¦ä¹  (HRL) SFCæ˜ å°„è®­ç»ƒç³»ç»Ÿ")
    print("=" * 70)
    print(f"  æ¨¡å¼: {args.mode}")
    print(f"  å®éªŒåç§°: {args.name}")
    print(f"  Goal Horizon: {args.goal_horizon}")
    if args.checkpoint:
        print(f"  åŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint}")
    print("=" * 70 + "\n")

    # åˆå§‹åŒ–é…ç½®
    config = Config()

    # è§£ææ‹“æ‰‘
    graph = parse_xml_topology(config.xml_topology_path)
    print(f"âœ“ æ‹“æ‰‘åŠ è½½å®Œæˆ: {len(graph.nodes)} èŠ‚ç‚¹, {len(graph.edges)} è¾¹")

    # åˆå§‹åŒ–ç¯å¢ƒ
    vnfs = getattr(config, 'vnfs', [{'cpu': 10}, {'cpu': 5}, {'cpu': 7}])
    env = SFCEnv(graph, vnfs, config)
    print(f"âœ“ ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: {len(vnfs)} VNFs")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HierarchicalSFCTrainer(config, env, name=args.name)
    trainer.goal_horizon = args.goal_horizon
    print(f"âœ“ è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ\n")

    # åŠ è½½æ£€æŸ¥ç‚¹
    if args.checkpoint:
        try:
            trainer.load_checkpoint(args.checkpoint)
            print(f"âœ“ æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {args.checkpoint}\n")
        except Exception as e:
            print(f"âœ— æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}\n")
            return

    # æ‰§è¡Œæ“ä½œ
    if args.mode == 'train':
        print(f"ğŸš€ å¼€å§‹åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒ ({args.episodes} episodes)...\n")
        try:
            trainer.train(num_episodes=args.episodes)
            print("\nâœ“ è®­ç»ƒå®Œæˆ!")
        except KeyboardInterrupt:
            print("\n\nâš  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            trainer.save_checkpoint('interrupted')
            print("âœ“ ä¸­æ–­çŠ¶æ€å·²ä¿å­˜")
        except Exception as e:
            print(f"\nâœ— è®­ç»ƒå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    elif args.mode == 'eval':
        print("ğŸ“Š å¼€å§‹è¯„ä¼°...\n")
        try:
            avg_reward, success_rate = trainer.evaluate(num_eval_episodes=50)
            print(f"\nâœ“ è¯„ä¼°å®Œæˆ!")
            print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {avg_reward:.4f}")
            print(f"  æœ€ç»ˆæˆåŠŸç‡: {success_rate * 100:.2f}%")
        except Exception as e:
            print(f"\nâœ— è¯„ä¼°å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    elif args.mode == 'test':
        print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...\n")
        print("æµ‹è¯•ä¸åŒGoal Horizonçš„å½±å“:\n")
        
        results = []
        for horizon in [1, 2, 3, 5, 10]:
            print(f"--- æµ‹è¯• Goal Horizon = {horizon} ---")
            trainer.goal_horizon = horizon
            try:
                avg_reward, success_rate = trainer.evaluate(num_eval_episodes=20)
                results.append({
                    'horizon': horizon,
                    'reward': avg_reward,
                    'success_rate': success_rate
                })
                print(f"  å¥–åŠ±: {avg_reward:.4f}")
                print(f"  æˆåŠŸç‡: {success_rate * 100:.2f}%\n")
            except Exception as e:
                print(f"  âœ— æµ‹è¯•å¤±è´¥: {e}\n")
        
        # è¾“å‡ºæœ€ä½³é…ç½®
        if results:
            best_by_reward = max(results, key=lambda x: x['reward'])
            best_by_success = max(results, key=lambda x: x['success_rate'])
            
            print("=" * 70)
            print("æµ‹è¯•æ€»ç»“:")
            print(f"  æœ€ä½³å¥–åŠ±é…ç½®: Goal Horizon = {best_by_reward['horizon']} "
                  f"(Reward = {best_by_reward['reward']:.4f})")
            print(f"  æœ€ä½³æˆåŠŸç‡é…ç½®: Goal Horizon = {best_by_success['horizon']} "
                  f"(Success Rate = {best_by_success['success_rate'] * 100:.2f}%)")
            print("=" * 70)

    print("\n" + "=" * 70)
    print("  ç¨‹åºç»“æŸ")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
