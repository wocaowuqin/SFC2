# -*- coding: utf-8 -*-
# @File     : train_hierarchical.py
# @Author   : AI Assistant
# @Desc     : Training script for Hierarchical RL in SFC Mapping
# @Fixes    : Corrected class structure and logical flow

import sys
import time
import pickle
import logging
from pathlib import Path
from collections import deque
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter


# -------------------------------------------
# âš ï¸ å‡è®¾æ‚¨çš„åŸæœ‰æ¨¡å—å·²æ­£ç¡®å¯¼å…¥
from config import Config
from env import SFCEnv
from rl import HierarchicalDQNAgent
from log import MyLog
# -------------------------------------------

# Placeholder classes for running the script
# (Replace with your actual imports)
class Config:
    E_GREEDY_EPISODES = 1000
    xml_topology_path = "topology.xml"  # å‡è®¾çš„æ‹“æ‰‘æ–‡ä»¶
    pkl_weight_path = "./pkl_files"
    PKL_CUT_NUM = 100
    vnfs = [{'cpu': 10}, {'cpu': 5}, {'cpu': 7}]


class SFCEnv:
    def __init__(self, graph, vnfs, config):
        self.nodes = graph.nodes()
        self.vnfs = vnfs
        self.config = config
        self.current_vnf_index = 0
        self.tree_nodes = []
        print("Mock SFCEnv initialized.")

    def reset(self):
        self.current_vnf_index = 0
        self.tree_nodes = []
        return torch.rand(10), {}  # è¿”å›ä¸€ä¸ªè™šæ‹ŸçŠ¶æ€

    def read_pickle_and_modify(self, pkl_path):
        pass  # æ¨¡æ‹Ÿ

    def get_valid_actions(self):
        return [1, 2, 3]  # æ¨¡æ‹Ÿ

    def step(self, action):
        self.current_vnf_index += 1
        self.tree_nodes.append(action)
        done = self.current_vnf_index >= len(self.vnfs)
        info = {'termination': 'sfc_completed' if done else 'in_progress'}
        reward = np.random.rand()
        next_state = torch.rand(10)
        return next_state, reward, done, info

    def get_sfc_qos_params(self):
        return 0.9, 0.5, 0.01, 5  # bw, delay, loss, length


class HierarchicalDQNAgent:
    def __init__(self, config, num_nodes):
        self.meta_step = 0
        self.sub_step = 0
        self.replay_buffer = self  # Mock replay buffer
        self.meta_buffer = deque(maxlen=1000)
        self.sub_buffer = deque(maxlen=10000)
        print("Mock HierarchicalDQNAgent initialized.")

    def select_goal(self, state, epsilon):
        return torch.rand(5)  # æ¨¡æ‹Ÿ

    def select_action(self, state, goal, epsilon, valid_actions):
        return np.random.choice(valid_actions)  # æ¨¡æ‹Ÿ

    def compute_intrinsic_reward(self, goal, action, next_state):
        return np.random.rand() * 0.1  # æ¨¡æ‹Ÿ

    def push_meta(self, s, g, r, ns, d):
        self.meta_buffer.append((s, g, r, ns, d))

    def push_sub(self, s, g, a, r, ns, d):
        self.sub_buffer.append((s, g, a, r, ns, d))

    def train_meta(self):
        if len(self.meta_buffer) < 100: return None
        self.meta_step += 1
        return np.random.rand() * 0.1  # æ¨¡æ‹Ÿ

    def train_sub(self):
        if len(self.sub_buffer) < 100: return None
        self.sub_step += 1
        return np.random.rand() * 0.5  # æ¨¡æ‹Ÿ

    def save(self, path):
        pass

    def load(self, path):
        pass

    def eval(self):
        pass

    def train_mode(self):
        pass


class MyLog:
    def __init__(self, *args, **kwargs):
        self.logger = logging.getLogger(kwargs.get("name", "HRL_SFC"))
        print("Mock MyLog initialized.")


# (End of placeholder classes)


class HierarchicalSFCTrainer:
    """
    åˆ†å±‚å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ï¼ˆé€‚é…SFCæ˜ å°„ï¼‰
    """

    def __init__(self, config: Config, env: SFCEnv, name: str = "HRL_SFC"):
        self.config = config
        self.env = env
        self.name = name

        # æ—¥å¿—ç³»ç»Ÿ
        self.mylog = MyLog(Path(__file__), filesave=True, consoleprint=True, name=name)
        self.logger = self.mylog.logger

        # åˆ›å»ºåˆ†å±‚ä»£ç†
        self.agent = HierarchicalDQNAgent(config, num_nodes=len(env.nodes))

        # TensorBoard
        self.writer = SummaryWriter(f"./runs/HRL_{name}")

        # è®­ç»ƒå‚æ•°
        self.meta_epsilon_start = 1.0
        self.meta_epsilon_end = 0.1
        self.meta_epsilon_decay = config.E_GREEDY_EPISODES // 2

        self.sub_epsilon_start = 1.0
        self.sub_epsilon_end = 0.01
        self.sub_epsilon_decay = config.E_GREEDY_EPISODES

        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards = []
        self.meta_losses = []
        self.sub_losses = []

        # GoalæŒç»­æ—¶é—´ï¼ˆæ¯ä¸ªgoalç”¨äºå¤šå°‘æ­¥ï¼‰
        self.goal_horizon = 3  # æ¯3ä¸ªVNFé‡æ–°é€‰æ‹©goal

    def get_epsilon(self, episode: int, is_meta: bool = False) -> float:
        """è®¡ç®—epsilonï¼ˆepsilon-greedyç­–ç•¥ï¼‰"""
        if is_meta:
            start, end, decay = self.meta_epsilon_start, self.meta_epsilon_end, self.meta_epsilon_decay
        else:
            start, end, decay = self.sub_epsilon_start, self.sub_epsilon_end, self.sub_epsilon_decay

        if episode >= decay:
            return end

        epsilon = end + (start - end) * np.exp(-episode / decay)
        return epsilon

    def train_episode(self, episode: int, pkl_path: str = None):
        """
        è®­ç»ƒå•ä¸ªepisodeï¼ˆåˆ†å±‚å†³ç­–ï¼‰
        (FIXED: è¿™æ˜¯ `train_episode` çš„æ­£ç¡®é€»è¾‘)
        """
        # é‡ç½®ç¯å¢ƒ
        if pkl_path is not None:
            self.env.read_pickle_and_modify(pkl_path)

        state, _ = self.env.reset()
        episode_reward = 0
        step_count = 0

        # Episodeçº§åˆ«çš„goalï¼ˆåˆå§‹åŒ–ï¼‰
        current_goal = None
        goal_start_vnf = 0  # å½“å‰goalä»å“ªä¸ªVNFå¼€å§‹

        # ç”¨äºmeta-level transitionçš„ç¼“å­˜
        meta_state = state.clone()
        meta_cumulative_reward = 0

        # è®­ç»ƒå¾ªç¯
        while True:
            # ============== Meta-levelå†³ç­– ==============
            if current_goal is None or (self.env.current_vnf_index - goal_start_vnf) >= self.goal_horizon:
                # 1. ä¿å­˜ä¸Šä¸€ä¸ªmeta transitionï¼ˆå¦‚æœæœ‰ï¼‰
                if current_goal is not None:
                    self.agent.replay_buffer.push_meta(
                        meta_state,
                        current_goal,
                        meta_cumulative_reward,
                        state.clone(),
                        False  # is_done=False
                    )

                # 2. è®­ç»ƒMeta-Controller
                meta_loss = self.agent.train_meta()
                if meta_loss is not None:
                    self.meta_losses.append(meta_loss)
                    self.writer.add_scalar('Loss/Meta', meta_loss, self.agent.meta_step)

                # 3. é€‰æ‹©æ–°goal (epsilon-greedy)
                meta_epsilon = self.get_epsilon(episode, is_meta=True)
                current_goal = self.agent.select_goal(state, epsilon=meta_epsilon)

                # 4. é‡ç½®metaçŠ¶æ€
                meta_state = state.clone()
                meta_cumulative_reward = 0
                goal_start_vnf = self.env.current_vnf_index

            # ============== Sub-levelå†³ç­– ==============
            sub_epsilon = self.get_epsilon(episode, is_meta=False)
            valid_actions = self.env.get_valid_actions()

            if not valid_actions:
                self.logger.warning(f"Episode {episode}: No valid actions at step {step_count}")
                break

            action = self.agent.select_action(state, current_goal, sub_epsilon, valid_actions)

            # ç¯å¢ƒäº¤äº’
            next_state, extrinsic_reward, done, info = self.env.step(action)

            # è®¡ç®—å†…åœ¨å¥–åŠ±
            intrinsic_reward = self.agent.compute_intrinsic_reward(current_goal, action, next_state)

            # ç»„åˆå¥–åŠ±ï¼ˆå¯è°ƒæ•´æƒé‡ï¼‰
            # combined_reward = 0.5 * extrinsic_reward + 0.5 * intrinsic_reward

            # ä¿å­˜sub transition (Sub-controllerä½¿ç”¨å†…åœ¨å¥–åŠ±)
            self.agent.replay_buffer.push_sub(
                state.clone(),
                current_goal.clone(),
                action,
                intrinsic_reward,
                next_state.clone() if not done else None,
                done
            )

            # è®­ç»ƒSub-Controller
            sub_loss = self.agent.train_sub()
            if sub_loss is not None:
                self.sub_losses.append(sub_loss)
                self.writer.add_scalar('Loss/Sub', sub_loss, self.agent.sub_step)

            # æ›´æ–°ç»Ÿè®¡
            episode_reward += extrinsic_reward
            meta_cumulative_reward += extrinsic_reward  # Meta-controllerä½¿ç”¨ç´¯ç§¯çš„å¤–åœ¨å¥–åŠ±
            step_count += 1

            # æ—¥å¿—è®°å½•
            if step_count % 5 == 0:
                self.logger.info(
                    f"[Ep {episode}][Step {step_count}] "
                    f"VNF {self.env.current_vnf_index}/{len(self.env.vnfs)} | "
                    f"Reward: {extrinsic_reward:.4f} | "
                    f"Intrinsic: {intrinsic_reward:.4f} | "
                    f"Node: {action} | "
                    f"Tree: {len(self.env.tree_nodes)} nodes"
                )

            # æ£€æŸ¥ç»ˆæ­¢
            if done:
                # ä¿å­˜æœ€åçš„meta transition (is_done=True)
                self.agent.replay_buffer.push_meta(
                    meta_state,
                    current_goal,
                    meta_cumulative_reward,
                    next_state.clone(),
                    True
                )

                # æœ€åè®­ç»ƒä¸€æ¬¡
                meta_loss = self.agent.train_meta()
                if meta_loss is not None:
                    self.meta_losses.append(meta_loss)

                self.logger.info(
                    f"[Episode {episode} Done] "
                    f"Reward: {episode_reward:.4f} | "
                    f"Steps: {step_count} | "
                    f"Status: {info.get('termination', 'unknown')}"
                )
                break

            state = next_state

        return episode_reward, step_count

    def train(self, num_episodes: int = 2000):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        (FIXED: å·²ç§»å…¥class)
        """
        start_time = time.time()

        # è·å–pklæ–‡ä»¶åˆ—è¡¨
        pkl_files = list(self._get_pkl_files())
        if not pkl_files:
            self.logger.warning("æœªæ‰¾åˆ°pklæ–‡ä»¶ï¼Œä½¿ç”¨é™æ€ç¯å¢ƒ")
            pkl_files = [None] * num_episodes

        for episode in range(num_episodes):
            # é€‰æ‹©pklæ–‡ä»¶ï¼ˆå¾ªç¯ä½¿ç”¨ï¼‰
            pkl_path = pkl_files[episode % len(pkl_files)]

            # è®­ç»ƒä¸€ä¸ªepisode
            episode_reward, steps = self.train_episode(episode, pkl_path)

            # è®°å½•
            self.episode_rewards.append(episode_reward)
            self.writer.add_scalar('Reward/Episode', episode_reward, episode)
            self.writer.add_scalar('Steps/Episode', steps, episode)
            self.writer.add_scalar('Epsilon/Meta', self.get_epsilon(episode, True), episode)
            self.writer.add_scalar('Epsilon/Sub', self.get_epsilon(episode, False), episode)

            # å®šæœŸä¿å­˜
            if episode % 100 == 0 and episode > 0:
                self.save_checkpoint(episode)
                self._log_statistics(episode)

            # å®šæœŸè¯„ä¼°
            if episode % 500 == 0 and episode > 0:
                self.evaluate(num_eval_episodes=10)

        # æœ€ç»ˆä¿å­˜
        self.save_checkpoint('final')
        self._log_statistics(num_episodes)

        elapsed_time = time.time() - start_time
        self.logger.info(f"Training completed in {elapsed_time / 3600:.2f} hours")

    def evaluate(self, num_eval_episodes: int = 10):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆæ— æ¢ç´¢ï¼‰
        (FIXED: å·²è¡¥å…¨å¹¶ç§»å…¥class)
        """
        self.logger.info(f"Starting evaluation for {num_eval_episodes} episodes...")

        eval_rewards = []
        eval_success = []
        eval_qos = {'bw': [], 'delay': [], 'loss': [], 'length': []}

        self.agent.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

        for ep in range(num_eval_episodes):
            state, _ = self.env.reset()
            episode_reward = 0
            current_goal = None
            goal_start_vnf = 0

            while True:
                # Meta-levelå†³ç­–ï¼ˆè´ªå©ªç­–ç•¥ï¼‰
                if current_goal is None or (self.env.current_vnf_index - goal_start_vnf) >= self.goal_horizon:
                    current_goal = self.agent.select_goal(state, epsilon=0.0)
                    goal_start_vnf = self.env.current_vnf_index

                # Sub-levelå†³ç­–ï¼ˆè´ªå©ªç­–ç•¥ï¼‰
                valid_actions = self.env.get_valid_actions()
                if not valid_actions:
                    break  # å¤±è´¥

                action = self.agent.select_action(state, current_goal, epsilon=0.0, valid_actions=valid_actions)
                next_state, reward, done, info = self.env.step(action)

                episode_reward += reward

                if done:
                    if info.get('termination') == 'sfc_completed':
                        eval_success.append(1)
                        bw, delay, loss, length = self.env.get_sfc_qos_params()
                        eval_qos['bw'].append(bw)
                        eval_qos['delay'].append(delay)
                        eval_qos['loss'].append(loss)
                        eval_qos['length'].append(length)
                    else:
                        eval_success.append(0)
                        eval_qos['bw'].append(0.0)
                        eval_qos['delay'].append(1.0)
                        eval_qos['loss'].append(1.0)
                        eval_qos['length'].append(0)
                    break

                state = next_state

            eval_rewards.append(episode_reward)

        self.agent.train_mode()  # æ¢å¤è®­ç»ƒæ¨¡å¼

        # ç»Ÿè®¡ç»“æœ
        avg_reward = np.mean(eval_rewards)
        success_rate = np.mean(eval_success)
        avg_bw = np.mean(eval_qos['bw'])
        avg_delay = np.mean(eval_qos['delay'])
        avg_loss = np.mean(eval_qos['loss'])
        avg_length = np.mean(eval_qos['length'])

        self.logger.info("=" * 60)
        self.logger.info(f"Evaluation Results ({num_eval_episodes} episodes):")
        self.logger.info(f"  Average Reward: {avg_reward:.4f}")
        self.logger.info(f"  Success Rate: {success_rate * 100:.2f}%")
        self.logger.info(f"  Average BW: {avg_bw:.4f}")
        self.logger.info(f"  Average Delay: {avg_delay:.4f}")
        self.logger.info(f"  Average Loss: {avg_loss:.6f}")
        self.logger.info(f"  Average Length: {avg_length:.2f}")
        self.logger.info("=" * 60)

        return avg_reward, success_rate

    def save_checkpoint(self, episode):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_dir = Path('./checkpoints') / self.name
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        checkpoint_path = checkpoint_dir / f'hrl_episode_{episode}.pt'
        self.agent.save(str(checkpoint_path))

        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats = {
            'episode_rewards': self.episode_rewards,
            'meta_losses': self.meta_losses,
            'sub_losses': self.sub_losses,
        }
        stats_path = checkpoint_dir / f'stats_episode_{episode}.pkl'
        with open(stats_path, 'wb') as f:
            pickle.dump(stats, f)

        self.logger.info(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        self.agent.load(checkpoint_path)

        # å°è¯•åŠ è½½ç»Ÿè®¡ä¿¡æ¯
        stats_path = Path(checkpoint_path).parent / f"stats_{Path(checkpoint_path).stem}.pkl"
        if stats_path.exists():
            with open(stats_path, 'rb') as f:
                stats = pickle.load(f)
                self.episode_rewards = stats.get('episode_rewards', [])
                self.meta_losses = stats.get('meta_losses', [])
                self.sub_losses = stats.get('sub_losses', [])
            self.logger.info(f"Statistics loaded from {stats_path}")

    def _log_statistics(self, episode: int):
        """è®°å½•è®­ç»ƒç»Ÿè®¡"""
        if len(self.episode_rewards) == 0:
            return

        recent_rewards = self.episode_rewards[-100:] if len(self.episode_rewards) >= 100 else self.episode_rewards
        avg_reward = np.mean(recent_rewards)

        self.logger.info("=" * 60)
        self.logger.info(f"Training Statistics (Episode {episode}):")
        self.logger.info(f"  Recent 100 Avg Reward: {avg_reward:.4f}")
        self.logger.info(f"  Total Episodes: {len(self.episode_rewards)}")
        self.logger.info(f"  Meta Buffer Size: {len(self.agent.replay_buffer.meta_buffer)}")
        self.logger.info(f"  Sub Buffer Size: {len(self.agent.replay_buffer.sub_buffer)}")
        self.logger.info(f"  Meta Training Steps: {self.agent.meta_step}")
        self.logger.info(f"  Sub Training Steps: {self.agent.sub_step}")
        if self.meta_losses:
            self.logger.info(f"  Recent Meta Loss: {np.mean(self.meta_losses[-100:]):.6f}")
        if self.sub_losses:
            self.logger.info(f"  Recent Sub Loss: {np.mean(self.sub_losses[-100:]):.6f}")
        self.logger.info("=" * 60)

    def _get_pkl_files(self):
        """è·å–pklæ–‡ä»¶åˆ—è¡¨"""
        pkl_dir = Path(self.config.pkl_weight_path)
        if not pkl_dir.exists():
            return []

        pkl_files = list(pkl_dir.glob('*.pkl'))
        return sorted(pkl_files)[:self.config.PKL_CUT_NUM]


# ==================== é«˜çº§å˜ä½“ï¼šOptionsæ¡†æ¶ ====================
class OptionBasedHRL:
    """
    åŸºäºOptionsçš„åˆ†å±‚RLï¼ˆSutton et al., 1999ï¼‰
    Optionså®šä¹‰ä¸ºæ—¶é—´ä¸Šçš„æŠ½è±¡åŠ¨ä½œï¼ˆç­–ç•¥ç‰‡æ®µï¼‰
    """

    def __init__(self, config, num_nodes: int):
        self.config = config
        self.num_nodes = num_nodes

        # å®šä¹‰Optionsï¼ˆæ¯ä¸ªOptionæ˜¯ä¸€ä¸ªå­ç­–ç•¥ï¼‰
        self.options = self._define_options()
        self.num_options = len(self.options)

        # Optioné€‰æ‹©ç½‘ç»œ
        self.option_selector = self._build_option_selector()

        # æ¯ä¸ªOptionçš„Qç½‘ç»œ
        self.option_q_networks = {
            opt_id: self._build_option_q_network()
            for opt_id in range(self.num_options)
        }

    def _define_options(self):
        """
        å®šä¹‰Optionsï¼ˆç­–ç•¥é›†åˆï¼‰
        æ¯ä¸ªOptionä»£è¡¨ä¸€ç§éƒ¨ç½²ç­–ç•¥
        """
        options = [
            {'id': 0, 'name': 'greedy_resource', 'desc': 'ä¼˜å…ˆé€‰æ‹©èµ„æºå……è¶³çš„èŠ‚ç‚¹'},
            {'id': 1, 'name': 'greedy_qos', 'desc': 'ä¼˜å…ˆé€‰æ‹©QoSæœ€ä¼˜çš„èŠ‚ç‚¹'},
            {'id': 2, 'name': 'balanced', 'desc': 'å¹³è¡¡èµ„æºå’ŒQoS'},
            {'id': 3, 'name': 'exploration', 'desc': 'æ¢ç´¢æ€§éƒ¨ç½²'},
        ]
        return options

    def _build_option_selector(self):
        """æ„å»ºOptioné€‰æ‹©å™¨"""
        # ç®€åŒ–ç¤ºä¾‹
        return None

    def _build_option_q_network(self):
        """æ„å»ºOptionçº§åˆ«çš„Qç½‘ç»œ"""
        # ç®€åŒ–ç¤ºä¾‹
        return None


# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="Hierarchical RL for SFC Mapping")
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'eval', 'test'])
    parser.add_argument('--episodes', type=int, default=2000, help='Number of training episodes')
    parser.add_argument('--checkpoint', type=str, default=None, help='Checkpoint path to load')
    parser.add_argument('--name', type=str, default='HRL_SFC', help='Experiment name')
    parser.add_argument('--goal_horizon', type=int, default=3, help='Goal horizon (VNFs per goal)')
    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆå§‹åŒ–é…ç½®
    # from config import Config
    config = Config()

    # è§£ææ‹“æ‰‘
    import xml.etree.ElementTree as ET
    import networkx as nx

    # --- Mock Topology Generation (if XML not found) ---
    if not Path(config.xml_topology_path).exists():
        print(f"Warning: '{config.xml_topology_path}' not found. Creating a mock graph.")
        graph = nx.Graph()
        graph.add_node(1, cpu=100.0, cpu_total=100.0)
        graph.add_node(2, cpu=100.0, cpu_total=100.0)
        graph.add_node(3, cpu=100.0, cpu_total=100.0)
        graph.add_edge(1, 2, bandwidth=1000.0, delay=10.0, loss=0.01)
        graph.add_edge(2, 3, bandwidth=1000.0, delay=10.0, loss=0.01)
    else:
        tree = ET.parse(config.xml_topology_path)
        root = tree.getroot()
        topo_element = root.find("topology")
        graph = nx.Graph()

        for child in topo_element.iter():
            if child.tag == 'node':
                node_id = int(child.get('id'))
                cpu_total = float(child.get('cpu_total', 100.0))
                graph.add_node(node_id, cpu=cpu_total, cpu_total=cpu_total)
            elif child.tag == 'link':
                from_node = int(child.find('from').get('node'))
                to_node = int(child.find('to').get('node'))
                graph.add_edge(from_node, to_node,
                               bandwidth=float(child.get('bw', 1000.0)),
                               delay=float(child.get('delay', 10.0)),
                               loss=float(child.get('loss', 0.01)))

    # åˆå§‹åŒ–ç¯å¢ƒ
    # from env import SFCEnv
    vnfs = getattr(config, 'vnfs', [{'cpu': 10}, {'cpu': 5}, {'cpu': 7}])
    env = SFCEnv(graph, vnfs, config)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = HierarchicalSFCTrainer(config, env, name=args.name)
    trainer.goal_horizon = args.goal_horizon

    # åŠ è½½æ£€æŸ¥ç‚¹
    if args.checkpoint:
        trainer.load_checkpoint(args.checkpoint)
        print(f"âœ… Checkpoint loaded: {args.checkpoint}")

    # æ‰§è¡Œæ“ä½œ
    if args.mode == 'train':
        print(f"ğŸš€ Starting Hierarchical RL training for {args.episodes} episodes...")
        trainer.train(num_episodes=args.episodes)
        print("âœ… Training completed!")

    elif args.mode == 'eval':
        print("ğŸ“Š Starting evaluation...")
        trainer.evaluate(num_eval_episodes=50)
        print("âœ… Evaluation completed!")

    elif args.mode == 'test':
        print("ğŸ§ª Running test mode...")
        # æµ‹è¯•ä¸åŒgoal_horizonçš„å½±å“
        for horizon in [1, 2, 3, 5]:
            print(f"\n--- Testing with goal_horizon={horizon} ---")
            trainer.goal_horizon = horizon
            avg_reward, success_rate = trainer.evaluate(num_eval_episodes=20)
            print(f"Goal Horizon {horizon}: Reward={avg_reward:.4f}, Success={success_rate * 100:.2f}%")


if __name__ == "__main__":
    main()